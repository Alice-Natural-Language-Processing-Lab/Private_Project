{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 뉴스 기사 및 댓글에 대한 감정 분석\n",
    "## * Doc2Vec \n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 2월 1일까지 63일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import doc2vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Database_Handler as dh\n",
    "import Basic_Module as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "ct = Twitter()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naver : (15120, 11)\n",
      "Daum : (9372, 11)\n"
     ]
    }
   ],
   "source": [
    "#Naver\n",
    "naverData = pickle.load(open('./data/pre_data/stastics/for_statistics_Naver_from_mongodb.pickled','rb'))\n",
    "naverData = pd.DataFrame.from_dict(naverData, orient = 'index')\n",
    "naverData.reset_index(inplace = True)\n",
    "naverData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "#Daum\n",
    "daumData = pickle.load(open('./data/pre_data/stastics/for_statistics_daum_from_mongodb.pickled','rb'))\n",
    "daumData = pd.DataFrame.from_dict(daumData, orient = 'index')\n",
    "daumData.reset_index(inplace = True)\n",
    "daumData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "\n",
    "print ('Naver : {}'.format(naverData.shape))\n",
    "print ('Daum : {}'.format(daumData.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extNaverData = naverData.loc[:, ['id', 'title', 'date', 'press', 'rank']].copy()\n",
    "extNaverData['site'] = pd.Series(['Naver']*extNaverData.shape[0])\n",
    "extDaumData = daumData.loc[:, ['id', 'title', 'date', 'press', 'rank']].copy()\n",
    "extDaumData['site'] = pd.Series(['daum']*extDaumData.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "    classifierPath = '/Volumes/disk1/data/pre_data/classifier/'\n",
    "    #newsPath = '/Volumes/data/pre_data/news_sentiment/'\n",
    "    newsPath = './data/pre_data/news_sentiment/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "    classifierPath = 'd:/data/pre_data/classifier/'\n",
    "    newsPath = './data/pre_data/news_sentiment/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaggedDocument For Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName = 'ct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_doc2vec_sentiment_analysis.pickled'):\n",
    "    taggedDaumData = pickle.load(open('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_doc2vec_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedDaumData = bm.MakeTaggedDataDAUM(daumData, TaggedDocument, ct, stopwords, 'daum')\n",
    "    pickle.dump(taggedDaumData, open('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_doc2vec_sentiment_analysis.pickled', 'wb'))\n",
    "\n",
    "    \n",
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_doc2vec_sentiment_analysis.pickled'):\n",
    "    taggedNaverData = pickle.load(open('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_doc2vec_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedNaverData = bm.MakeTaggedDataDAUM(naverData, TaggedDocument, ct, stopwords, 'naver')\n",
    "    pickle.dump(taggedNaverData, open('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_doc2vec_sentiment_analysis.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-ct.model')\n",
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+taggerName\n",
    "model2 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-ct.model')\n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+taggerName\n",
    "model3 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-ct.model')\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+taggerName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tagged Document to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음\n",
    "daum_news_by_m1_name = newsPath+'daum_news_vector_'+name1\n",
    "daum_news_by_m2_name = newsPath+'daum_news_vector_'+name2\n",
    "daum_news_by_m3_name = newsPath+'daum_news_vector_'+name3\n",
    "if not os.path.isfile(daum_news_by_m1_name):\n",
    "    daum_news_by_m1 = bm.Get_Infer_Vector(taggedDaumData, model1)\n",
    "    pickle.dump(daum_news_by_m1, open(daum_news_by_m1_name, 'wb'))\n",
    "    del daum_news_by_m1\n",
    "\n",
    "if not os.path.isfile(daum_news_by_m2_name):\n",
    "    daum_news_by_m2 = bm.Get_Infer_Vector(taggedDaumData, model2)\n",
    "    pickle.dump(daum_news_by_m2, open(daum_news_by_m2_name, 'wb'))\n",
    "    del daum_news_by_m2\n",
    "    \n",
    "if not os.path.isfile(daum_news_by_m3_name):\n",
    "    daum_news_by_m3 = bm.Get_Infer_Vector(taggedDaumData, model3)\n",
    "    pickle.dump(daum_news_by_m3, open(daum_news_by_m3_name, 'wb'))\n",
    "    del daum_news_by_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버\n",
    "naver_news_by_m1_name = newsPath+'daum_news_vector_'+name1\n",
    "naver_news_by_m2_name = newsPath+'daum_news_vector_'+name2\n",
    "naver_news_by_m3_name = newsPath+'daum_news_vector_'+name3\n",
    "if not os.path.isfile(naver_news_by_m1_name):\n",
    "    naver_news_by_m1 = bm.Get_Infer_Vector(taggedDaumData, model1)\n",
    "    pickle.dump(naver_news_by_m1, open(naver_news_by_m1_name, 'wb'))\n",
    "    del naver_news_by_m1\n",
    "\n",
    "if not os.path.isfile(naver_news_by_m2_name):\n",
    "    naver_news_by_m2 = bm.Get_Infer_Vector(taggedDaumData, model2)\n",
    "    pickle.dump(naver_news_by_m2, open(naver_news_by_m2_name, 'wb'))\n",
    "    del naver_news_by_m2\n",
    "    \n",
    "if not os.path.isfile(naver_news_by_m3_name):\n",
    "    naver_news_by_m3 = bm.Get_Infer_Vector(taggedDaumData, model3)\n",
    "    pickle.dump(naver_news_by_m3, open(naver_news_by_m3_name, 'wb'))\n",
    "    del naver_news_by_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del taggedDaumData\n",
    "del taggedNaverData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "RandomForestClassifier\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "XGBoost\n",
      "SVC\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 813884.99it/s]\n",
      "9372it [00:00, 741763.54it/s]\n",
      "9372it [00:00, 662571.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.3 s, sys: 574 ms, total: 31.9 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daum_news_by_m1 = pickle.load(open(daum_news_by_m1_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_news_by_m1, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+name1,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 726679.80it/s]\n",
      "9372it [00:00, 874097.02it/s]\n",
      "9372it [00:00, 863687.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.4 s, sys: 586 ms, total: 32 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naver_news_by_m1 = pickle.load(open(naver_news_by_m1_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_news_by_m1, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+name1,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daum_news_by_m1\n",
    "del naver_news_by_m1\n",
    "del daum_news_by_m1_name\n",
    "del naver_news_by_m1_name\n",
    "del name1\n",
    "del model1\n",
    "del loadClassifierDict\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 515934.07it/s]\n",
      "9372it [00:00, 840008.06it/s]\n",
      "9372it [00:00, 668520.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 s, sys: 458 ms, total: 14.2 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daum_news_by_m2 = pickle.load(open(daum_news_by_m2_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_news_by_m2, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+name2,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 464212.11it/s]\n",
      "9372it [00:00, 882416.71it/s]\n",
      "9372it [00:00, 709152.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 402 ms, total: 14.5 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naver_news_by_m2 = pickle.load(open(naver_news_by_m2_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_news_by_m2, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+name2,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daum_news_by_m2\n",
    "del naver_news_by_m2\n",
    "del daum_news_by_m2_name\n",
    "del naver_news_by_m2_name\n",
    "del name2\n",
    "del model2\n",
    "del loadClassifierDict\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 473688.22it/s]\n",
      "9372it [00:00, 828587.45it/s]\n",
      "9372it [00:00, 822243.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.3 s, sys: 873 ms, total: 36.1 s\n",
      "Wall time: 36.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daum_news_by_m3 = pickle.load(open(daum_news_by_m3_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_news_by_m3, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+name3,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 714164.04it/s]\n",
      "9372it [00:00, 740575.69it/s]\n",
      "9372it [00:00, 725231.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.5 s, sys: 604 ms, total: 36.1 s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naver_news_by_m3 = pickle.load(open(naver_news_by_m3_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_news_by_m3, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+name3,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daum_news_by_m3\n",
    "del naver_news_by_m3\n",
    "del daum_news_by_m3_name\n",
    "del naver_news_by_m3_name\n",
    "del name3\n",
    "del model3\n",
    "del loadClassifierDict\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName = 'mecab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_doc2vec_sentiment_analysis.pickled'):\n",
    "    taggedDaumData = pickle.load(open('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_doc2vec_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedDaumData = bm.MakeTaggedDataDAUM(daumData, TaggedDocument, mecab, stopwords, 'daum')\n",
    "    pickle.dump(taggedDaumData, open('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_doc2vec_sentiment_analysis.pickled', 'wb'))\n",
    "\n",
    "    \n",
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_doc2vec_sentiment_analysis.pickled'):\n",
    "    taggedNaverData = pickle.load(open('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_doc2vec_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedNaverData = bm.MakeTaggedDataDAUM(naverData, TaggedDocument, mecab, stopwords, 'naver')\n",
    "    pickle.dump(taggedNaverData, open('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_doc2vec_sentiment_analysis.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-mecab.model')\n",
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+taggerName\n",
    "model2 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-mecab.model')\n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+taggerName\n",
    "model3 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-mecab.model')\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+taggerName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tagged Document to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2811/9372 [1:25:01<3:18:27,  1.81s/it]"
     ]
    }
   ],
   "source": [
    "# 다음\n",
    "daum_news_by_m1_name = newsPath+'daum_news_vector_'+name1\n",
    "daum_news_by_m2_name = newsPath+'daum_news_vector_'+name2\n",
    "daum_news_by_m3_name = newsPath+'daum_news_vector_'+name3\n",
    "if not os.path.isfile(daum_news_by_m1_name):\n",
    "    daum_news_by_m1 = bm.Get_Infer_Vector(taggedDaumData, model1)\n",
    "    pickle.dump(daum_news_by_m1, open(daum_news_by_m1_name, 'wb'))\n",
    "    del daum_news_by_m1\n",
    "\n",
    "if not os.path.isfile(daum_news_by_m2_name):\n",
    "    daum_news_by_m2 = bm.Get_Infer_Vector(taggedDaumData, model2)\n",
    "    pickle.dump(daum_news_by_m2, open(daum_news_by_m2_name, 'wb'))\n",
    "    del daum_news_by_m2\n",
    "    \n",
    "if not os.path.isfile(daum_news_by_m3_name):\n",
    "    daum_news_by_m3 = bm.Get_Infer_Vector(taggedDaumData, model3)\n",
    "    pickle.dump(daum_news_by_m3, open(daum_news_by_m3_name, 'wb'))\n",
    "    del daum_news_by_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버\n",
    "naver_news_by_m1_name = newsPath+'daum_news_vector_'+name1\n",
    "naver_news_by_m2_name = newsPath+'daum_news_vector_'+name2\n",
    "naver_news_by_m3_name = newsPath+'daum_news_vector_'+name3\n",
    "if not os.path.isfile(naver_news_by_m1_name):\n",
    "    naver_news_by_m1 = bm.Get_Infer_Vector(taggedDaumData, model1)\n",
    "    pickle.dump(naver_news_by_m1, open(naver_news_by_m1_name, 'wb'))\n",
    "    del naver_news_by_m1\n",
    "\n",
    "if not os.path.isfile(naver_news_by_m2_name):\n",
    "    naver_news_by_m2 = bm.Get_Infer_Vector(taggedDaumData, model2)\n",
    "    pickle.dump(naver_news_by_m2, open(naver_news_by_m2_name, 'wb'))\n",
    "    del naver_news_by_m2\n",
    "    \n",
    "if not os.path.isfile(naver_news_by_m3_name):\n",
    "    naver_news_by_m3 = bm.Get_Infer_Vector(taggedDaumData, model3)\n",
    "    pickle.dump(naver_news_by_m3, open(naver_news_by_m3_name, 'wb'))\n",
    "    del naver_news_by_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del taggedDaumData\n",
    "del taggedNaverData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "daum_news_by_m1 = pickle.load(open(daum_news_by_m1_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_news_by_m1, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+name1,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "naver_news_by_m1 = pickle.load(open(naver_news_by_m1_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_news_by_m1, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+name1,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daum_news_by_m1\n",
    "del naver_news_by_m1\n",
    "del daum_news_by_m1_name\n",
    "del naver_news_by_m1_name\n",
    "del name1\n",
    "del model1\n",
    "del loadClassifierDict\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "daum_news_by_m2 = pickle.load(open(daum_news_by_m2_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_news_by_m2, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+name2,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "naver_news_by_m2 = pickle.load(open(naver_news_by_m2_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_news_by_m2, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+name2,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daum_news_by_m2\n",
    "del naver_news_by_m2\n",
    "del daum_news_by_m2_name\n",
    "del naver_news_by_m2_name\n",
    "del name2\n",
    "del model2\n",
    "del loadClassifierDict\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "daum_news_by_m3 = pickle.load(open(daum_news_by_m3_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_news_by_m3, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+name3,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "naver_news_by_m3 = pickle.load(open(naver_news_by_m3_name, 'rb'))\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_news_by_m3, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+name3,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daum_news_by_m3\n",
    "del naver_news_by_m3\n",
    "del daum_news_by_m3_name\n",
    "del naver_news_by_m3_name\n",
    "del name3\n",
    "del model3\n",
    "del loadClassifierDict\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
