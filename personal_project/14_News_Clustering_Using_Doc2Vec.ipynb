{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "import sys \n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import cpu_count\n",
    "from collections import namedtuple\n",
    "import html\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel, ldaseqmodel, LdaMulticore, lda_dispatcher, doc2vec\n",
    "from gensim.models.wrappers import LdaMallet, DtmModel\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.matutils import hellinger\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = Twitter()\n",
    "mecab = Mecab()\n",
    "def nav_tokenizer(tagger, corpus, stopwords):\n",
    "    pos = tagger.pos(corpus)\n",
    "    pos = ['/'.join(t) for t in pos if not t[0] in stopwords]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeTaggedData(df, taggedDoc, tagger, stopwords):\n",
    "    w2v_docs = list()\n",
    "    for idx in tqdm(df.index):\n",
    "        text = df.loc[idx,'title']+'.\\n'+df.loc[idx,'mainText']\n",
    "        pos = nav_tokenizer(tagger, text, stopwords)\n",
    "        label = ['news_'+str(idx)]\n",
    "        w2v_docs.append(TaggedDocument(pos, label))\n",
    "    return w2v_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = int(multiprocessing.cpu_count())\n",
    "def Make_Doc2Vec_Model(modelPath, data, size, dm, dm_concat, dm_mean, hs, negative, epoch, window, alpha, min_alpha, workers, tagger):\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\")\n",
    "    from datetime import datetime\n",
    "    from gensim.models import doc2vec\n",
    "    start = datetime.now()\n",
    "    modelName = 'doc2vec_size-{}_epoch-{}_window-{}_negative-{}_hs-{}_dm-{}_dm_concat-{}_dm_mean-{}_by-{}.model'.format(\n",
    "        size, epoch, window, negative, hs, dm, dm_concat, dm_mean, tagger)\n",
    "    modelName = modelPath+modelName\n",
    "    print (modelName)\n",
    "    if window!=None:\n",
    "        d2v_model = doc2vec.Doc2Vec(vector_size = size, dm = dm, dm_concat = dm_concat,\n",
    "                   dm_mean = dm_mean, negative = negative, hs = hs, window = window,\n",
    "                   alpha = alpha, min_alpha = min_alpha, workers = workers, epochs= epoch)\n",
    "    else:\n",
    "        d2v_model = doc2vec.Doc2Vec(vector_size = size, dm = dm, dm_concat = dm_concat,\n",
    "                   dm_mean = dm_mean, negative = negative, hs = hs,\n",
    "                   alpha = alpha, min_alpha = min_alpha, workers = workers, epochs= epoch)\n",
    "    d2v_model.build_vocab(tqdm(data))\n",
    "    d2v_model.train_lbls = False # do not train labels of words\n",
    "    d2v_model.train(tqdm(data), total_examples=d2v_model.corpus_count, epochs=d2v_model.iter)\n",
    "    \n",
    "    end = datetime.now()\n",
    "    d2v_model.save(modelName)\n",
    "    print (\"Total running time: \", end-start)\n",
    "    return d2v_model\n",
    "print (cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = Twitter()\n",
    "mecab = Mecab()\n",
    "def nav_tokenizer(tagger, corpus, stopwords):\n",
    "    pos = tagger.pos(corpus)\n",
    "    pos = ['/'.join(t) for t in pos if not t[0] in stopwords]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_Similar_Doc(model, df, infer_vecs, target):\n",
    "    import re\n",
    "    intIndex = df.index.tolist().index(target.name)\n",
    "    print ('* News : {}'.format(target['title']))\n",
    "    print ('* Press : {}'.format(target['press']))\n",
    "    print ('* Date : {}'.format(target['date']))\n",
    "    print ('* Site : {}'.format(target['site']))\n",
    "    print ()\n",
    "    infer_vec = infer_vecs[intIndex]\n",
    "    simDocs = model.docvecs.most_similar(positive = [infer_vec])\n",
    "    print (' * Similar Document : {}'.format(len(simDocs)))\n",
    "    simDocs = pd.DataFrame(list(map(lambda x: df.loc[re.split('_', x[0])[1]], simDocs)))\n",
    "    simDocs = simDocs.loc[:, ['category','date', 'press', 'title', 'keywords', 'extracted_keywords', 'site']]\n",
    "    return simDocs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScatterPlot_by_kmeans(n_cluster, model):\n",
    "    print ('KMeans Clustering')\n",
    "    print ('Number of Cluster : {}'.format(n_cluster))\n",
    "    import timeit\n",
    "    start = timeit.default_timer()\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    kmeans = KMeans(n_clusters = n_cluster, \n",
    "                  init = 'k-means++', \n",
    "                  max_iter = 500)\n",
    "    X = kmeans.fit(model.docvecs.doctag_syn0)\n",
    "    labels = kmeans.labels_.tolist()\n",
    "    l = kmeans.fit_predict(model.docvecs.doctag_syn0)\n",
    "    pca = PCA(n_components = 2).fit(model.docvecs.doctag_syn0)\n",
    "    datapoint = pca.transform(model.docvecs.doctag_syn0)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    centroidpoint = pca.transform(centroids)\n",
    "    plt.figure\n",
    "    plt.scatter(datapoint[:, 0], datapoint[:, 1],\n",
    "                c = labels)\n",
    "    plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], \n",
    "               marker = '^', s = 150, c = '#000000')\n",
    "    end = timeit.default_timer()\n",
    "    execution_time = end - start\n",
    "    print ('Running Time : {}'.format(execution_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNaver = pickle.load(open('./data/pre_data/stastics/for_statistics_Naver_from_mongodb.pickled','rb'))\n",
    "dfNaver = pd.DataFrame.from_dict(dictNaver, orient='index')\n",
    "dfNaver['site'] = ['Naver'] * dfNaver.shape[0]\n",
    "print (dfNaver.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictDaum = pickle.load(open('./data/pre_data/stastics/for_statistics_daum_from_mongodb.pickled','rb'))\n",
    "dfDaum = pd.DataFrame.from_dict(dictDaum, orient='index')\n",
    "dfDaum['site'] = ['Daum'] * dfDaum.shape[0]\n",
    "print (dfDaum.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스기사 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf = pd.concat([dfNaver, dfDaum])\n",
    "combinedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    clusteringPath ='/Volumes/disk1/Clustering_doc2vec/'\n",
    "    \n",
    "elif sys.platform =='win32':\n",
    "    clusteringPath = 'd:/Clustering_doc2vec/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf[combinedDf.category == '스포츠']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec 기본 포맷으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_ct = 'ct'\n",
    "filename_ct = clusteringPath+'predata_doc2vec_{}'.format(tagger_ct)\n",
    "if os.path.isfile(filename_ct):\n",
    "    w2v_docs_ct = pickle.load(open(filename_ct, 'rb'))\n",
    "else:\n",
    "    w2v_docs_ct = MakeTaggedData(combinedDf, TaggedDocument, ct, stopwords)\n",
    "    pickle.dump(w2v_docs_ct, open(filename_ct,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model1 Using Tagger Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM W/\n",
    "modelName1_ct = clusteringPath + 'doc2vec_size-300_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-ct.model'\n",
    "if not os.path.isfile(modelName1_ct):\n",
    "    d2v_model1_ct = Make_Doc2Vec_Model(modelPath=clusteringPath, data=w2v_docs_ct, size = 300, dm = 1, dm_concat = 1,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = 5,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'ct')\n",
    "else:\n",
    "    d2v_model1_ct = doc2vec.Doc2Vec.load(modelName1_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_ct_name = modelName1_ct+'-infer_vector'\n",
    "if not os.path.isfile(m1_ct_name):\n",
    "    X_d2v_1_ct = [ d2v_model1_ct.infer_vector(x.words) for x in tqdm(w2v_docs_ct)]\n",
    "    pickle.dump(X_d2v_1_ct, open(m1_ct_name, 'wb'))\n",
    "else:\n",
    "    X_d2v_1_ct = pickle.load(open(m1_ct_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_1_ct = Print_Similar_Doc(d2v_model1_ct, combinedDf, X_d2v_1_ct, combinedDf.loc['5a381bb0588c13417c9a01a3'])\n",
    "test_1_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScatterPlot_by_kmeans(30, d2v_model1_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model2 Using Tagger Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM w/\n",
    "modelName2_ct = clusteringPath + 'doc2vec_size-300_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-ct.model'\n",
    "if not os.path.isfile(modelName2_ct):\n",
    "    d2v_model2_ct = Make_Doc2Vec_Model(modelPath=clusteringPath, data=w2v_docs_ct, size = 300, dm = 1, dm_concat = 0,\n",
    "                   dm_mean = 1, negative = 7, hs = 0, epoch = 20, window = 10,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'ct')\n",
    "else:\n",
    "    d2v_model2_ct = doc2vec.Doc2Vec.load(modelName2_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_ct_name = modelName2_ct+'-infer_vector'\n",
    "if not os.path.isfile(m2_ct_name):\n",
    "    X_d2v_2_ct = [ d2v_model2_ct.infer_vector(x.words) for x in tqdm(w2v_docs_ct)]\n",
    "    pickle.dump(X_d2v_2_ct, open(m2_ct_name, 'wb'))\n",
    "else:\n",
    "    X_d2v_2_ct = pickle.load(open(m2_ct_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_2_ct = Print_Similar_Doc(d2v_model2_ct, combinedDf, X_d2v_2_ct, combinedDf.loc['5a381bb0588c13417c9a01a3'])\n",
    "test_2_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model3 Using Tagger Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# PV - DBOW\n",
    "modelName3_ct = clusteringPath + 'doc2vec_size-300_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-ct.model'\n",
    "if not os.path.isfile(modelName3_ct):\n",
    "    d2v_model3_Ct = Make_Doc2Vec_Model(modelPath=clusteringPath, data=w2v_docs_ct, size = 300, dm = 0, dm_concat = 0,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = None,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'ct')\n",
    "else:\n",
    "    d2v_model3_ct = doc2vec.Doc2Vec.load(modelName3_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_ct_name = modelName3_ct+'-infer_vector'\n",
    "if not os.path.isfile(m3_ct_name):\n",
    "    X_d2v_3_ct = [ d2v_model3_ct.infer_vector(x.words) for x in tqdm(w2v_docs_ct)]\n",
    "    pickle.dump(X_d2v_3_ct, open(m3_ct_name, 'wb'))\n",
    "else:\n",
    "    X_d2v_3_ct = pickle.load(open(m3_ct_name, 'rb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_3_ct = Print_Similar_Doc(d2v_model3_ct, combinedDf, X_d2v_3_ct, combinedDf.loc['5a381bb0588c13417c9a01a3'])\n",
    "test_3_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_mecab = 'mecab'\n",
    "filename_mecab = clusteringPath+'predata_doc2vec_{}'.format(tagger_mecab)\n",
    "if os.path.isfile(filename_mecab):\n",
    "    w2v_docs_mecab = pickle.load(open(filename_mecab, 'rb'))\n",
    "else:\n",
    "    w2v_docs_mecab = MakeTaggedData(combinedDf, TaggedDocument, mecab, stopwords)\n",
    "    pickle.dump(w2v_docs_mecab, open(filename_mecab,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model1 Using Tagger Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM W/\n",
    "modelName1_mecab = clusteringPath + 'doc2vec_size-300_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-mecab.model'\n",
    "if not os.path.isfile(modelName1_mecab):\n",
    "    d2v_model1_mecab = Make_Doc2Vec_Model(modelPath=clusteringPath, data=w2v_docs_mecab, size = 300, dm = 1, dm_concat = 1,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = 5,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'mecab')\n",
    "else:\n",
    "    d2v_model1_mecab = doc2vec.Doc2Vec.load(modelName1_mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_mecab_name = modelName1_mecab+'-infer_vector'\n",
    "if not os.path.isfile(m1_mecab_name):\n",
    "    X_d2v_1_mecab = [ d2v_model1_mecab.infer_vector(x.words) for x in tqdm(w2v_docs_mecab)]\n",
    "    pickle.dump(X_d2v_1_mecab, open(m1_mecab_name, 'wb'))\n",
    "else:\n",
    "    X_d2v_1_mecab = pickle.load(open(m1_mecab_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_1_mecab = Print_Similar_Doc(d2v_model1_mecab, combinedDf, X_d2v_1_mecab, combinedDf.loc['5a381bb0588c13417c9a01a3'])\n",
    "test_1_mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model2 Using Tagger Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM w/\n",
    "modelName2_mecab = clusteringPath + 'doc2vec_size-300_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-mecab.model'\n",
    "if not os.path.isfile(modelName2_mecab):\n",
    "    d2v_model2_mecab = Make_Doc2Vec_Model(modelPath=clusteringPath, data=w2v_docs_mecab, size = 300, dm = 1, dm_concat = 0,\n",
    "                   dm_mean = 1, negative = 7, hs = 0, epoch = 20, window = 10,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'mecab')\n",
    "else:\n",
    "    d2v_model2_mecab = doc2vec.Doc2Vec.load(modelName2_mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_mecab_name = modelName2_mecab+'-infer_vector'\n",
    "if not os.path.isfile(m2_mecab_name):\n",
    "    X_d2v_2_mecab = [ d2v_model2_mecab.infer_vector(x.words) for x in tqdm(w2v_docs_mecab)]\n",
    "    pickle.dump(X_d2v_2_mecab, open(m2_mecab_name, 'wb'))\n",
    "else:\n",
    "    X_d2v_2_mecab = pickle.load(open(m2_mecab_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_mecab = Print_Similar_Doc(d2v_model2_mecab, combinedDf, X_d2v_2_mecab, combinedDf.loc['5a381bb0588c13417c9a01a3'])\n",
    "test_2_mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model3 Using Tagger Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# PV - DBOW\n",
    "modelName3_mecab = clusteringPath + 'doc2vec_size-300_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-mecab.model'\n",
    "if not os.path.isfile(modelName3_mecab):\n",
    "    d2v_model3_mecab = Make_Doc2Vec_Model(modelPath=clusteringPath, data=w2v_docs_mecab, size = 300, dm = 0, dm_concat = 0,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = None,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'mecab')\n",
    "else:\n",
    "    d2v_model3_mecab = doc2vec.Doc2Vec.load(modelName3_mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_mecab_name = modelName3_mecab+'-infer_vector'\n",
    "if not os.path.isfile(m3_mecab_name):\n",
    "    X_d2v_3_mecab = [ d2v_model3_mecab.infer_vector(x.words) for x in tqdm(w2v_docs_mecab)]\n",
    "    pickle.dump(X_d2v_3_mecab, open(m3_mecab_name, 'wb'))\n",
    "else:\n",
    "    X_d2v_3_mecab = pickle.load(open(m3_mecab_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_mecab = Print_Similar_Doc(d2v_model3_mecab, combinedDf, X_d2v_3_mecab, combinedDf.loc['5a381bb0588c13417c9a01a3'])\n",
    "test_3_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중간 점검\n",
    "* 만들어진 Doc2vec 모델을 하나의 뉴스(제목 : 안보전략硏 \"황병서·김원홍 '처벌'…공포통치 끝 아닌 시작\")로 확인을 해본 결과, 전체적으로 북한 관련 뉴스를 찾아주는 것으로 확인됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
