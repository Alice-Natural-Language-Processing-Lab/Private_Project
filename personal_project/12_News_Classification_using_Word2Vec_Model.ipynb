{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음의 News Classification Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "from numba import jit\n",
    "import warnings\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11749583939368616445\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = int(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Basic_Module as bm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daum : (9372, 11)\n"
     ]
    }
   ],
   "source": [
    "#Daum\n",
    "daumData = pickle.load(open('./data/pre_data/stastics/for_statistics_daum_from_mongodb.pickled','rb'))\n",
    "daumData = pd.DataFrame.from_dict(daumData, orient = 'index')\n",
    "daumData.reset_index(inplace = True)\n",
    "daumData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "extDaumData = daumData.loc[:,['id','title','extracted_keywords']].copy()\n",
    "print ('Daum : {}'.format(daumData.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IT/과학' '경제' '사회' '생활/문화' '세계' '스포츠' '연예' '정치']\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/pre_data/news_tagged_data/pre_data_category_label_encoder_by_ct_for_doc2vec_news_classification.pickled'):\n",
    "    le = pickle.load(open('./data/pre_data/news_tagged_data/pre_data_category_label_encoder_by_ct_for_doc2vec_news_classification.pickled','rb'))\n",
    "else:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(naverData['category'])\n",
    "    pickle.dump(le, open('./data/pre_data/news_tagged_data/pre_data_category_label_encoder_by_ct_for_doc2vec_news_classification.pickled','wb'))\n",
    "print (le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/news_model/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/news_model/'\n",
    "daumNewsPath = './data/pre_data/news_daum_news/'\n",
    "classifierPath = './data/pre_data/news_classifier/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>press</th>\n",
       "      <th>number_of_comment</th>\n",
       "      <th>number_of_crawled_comment</th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>mainText</th>\n",
       "      <th>keywords</th>\n",
       "      <th>extracted_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a2a61bf588c13481c229d1e</td>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>1093</td>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "      <td>\"밤이 무섭다\"..비아그라 공장 연기에 남성들 부작용 호소</td>\n",
       "      <td>주민들은 공장에서 배출된 연기가 '남성이 매우 건강해지는 부작용'을 일으킨다며, ...</td>\n",
       "      <td>[부작용, 비아그라, 아일랜드]</td>\n",
       "      <td>{공장, 건강, 남성들, 지역, 부작용, 연기, 세보 효과}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a2a61bf588c13481c229d1f</td>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>헬스조선</td>\n",
       "      <td>603</td>\n",
       "      <td>386</td>\n",
       "      <td>2</td>\n",
       "      <td>식후 커피·늦은 양치질..점심식사 후 하면 안 좋은 습관 3가지</td>\n",
       "      <td>점심식사를 마친 후 후식으로 커피를 마시는 사람들이 많다. 실제로 직장이 밀집돼 ...</td>\n",
       "      <td>[커피, 낮잠, 음식물]</td>\n",
       "      <td>{치아, 건강, 입냄새, 커피, 점심 식사, 낮잠, 디스크, 철분, 식후, 자세}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a2a61bf588c13481c229d20</td>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>1067</td>\n",
       "      <td>811</td>\n",
       "      <td>3</td>\n",
       "      <td>'십년지기 생매장' 진짜 이유는..\"'청부 통정' 알려질까 봐\"</td>\n",
       "      <td>(성남=연합뉴스) 최해민 기자 = 십년지기 지인을 산 채로 묻어 살해한 50대 여...</td>\n",
       "      <td>[살인혐의, 철원, 검찰송치]</td>\n",
       "      <td>{경찰, 성관계, 지인, 앙심, 진술, 철원, 아들, 주변, 남편, 범행}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a2a61bf588c13481c229d21</td>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>헤럴드경제</td>\n",
       "      <td>418</td>\n",
       "      <td>369</td>\n",
       "      <td>4</td>\n",
       "      <td>신영자, 억 소리나는 갑질</td>\n",
       "      <td>신영자, 적용안된 혐의→검찰 상고에서 인정\\n신영자, 얼마를 어떻게 받았나  [헤럴...</td>\n",
       "      <td>[신영자, 갑질, 롯데백화점]</td>\n",
       "      <td>{신영자 이사장, 혐의, 롯데, 검찰, 유통업체, 징역, 매장, 네이처리퍼블릭}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a2a61bf588c13481c229d22</td>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>434</td>\n",
       "      <td>368</td>\n",
       "      <td>5</td>\n",
       "      <td>\"배신하지마\" 20대女 살인 피의자 유치장서 공범 남친에 쪽지</td>\n",
       "      <td>(청주=연합뉴스) 이승민 기자 = 지난 9월 청주의 한 하천에서 20대 여성을 둔기...</td>\n",
       "      <td>[공범, 살인, 과자]</td>\n",
       "      <td>{경찰, 혐의, 폭행, 쪽지, 유치장, 남자친구, 과자, 범행}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id category        date  press  number_of_comment  \\\n",
       "0  5a2a61bf588c13481c229d1e       뉴스  2017.12.07   세계일보               1093   \n",
       "1  5a2a61bf588c13481c229d1f       뉴스  2017.12.07   헬스조선                603   \n",
       "2  5a2a61bf588c13481c229d20       뉴스  2017.12.07   연합뉴스               1067   \n",
       "3  5a2a61bf588c13481c229d21       뉴스  2017.12.07  헤럴드경제                418   \n",
       "4  5a2a61bf588c13481c229d22       뉴스  2017.12.07   연합뉴스                434   \n",
       "\n",
       "   number_of_crawled_comment rank                                title  \\\n",
       "0                        911    1     \"밤이 무섭다\"..비아그라 공장 연기에 남성들 부작용 호소   \n",
       "1                        386    2  식후 커피·늦은 양치질..점심식사 후 하면 안 좋은 습관 3가지   \n",
       "2                        811    3  '십년지기 생매장' 진짜 이유는..\"'청부 통정' 알려질까 봐\"   \n",
       "3                        369    4                       신영자, 억 소리나는 갑질   \n",
       "4                        368    5   \"배신하지마\" 20대女 살인 피의자 유치장서 공범 남친에 쪽지   \n",
       "\n",
       "                                            mainText           keywords  \\\n",
       "0   주민들은 공장에서 배출된 연기가 '남성이 매우 건강해지는 부작용'을 일으킨다며, ...  [부작용, 비아그라, 아일랜드]   \n",
       "1   점심식사를 마친 후 후식으로 커피를 마시는 사람들이 많다. 실제로 직장이 밀집돼 ...      [커피, 낮잠, 음식물]   \n",
       "2   (성남=연합뉴스) 최해민 기자 = 십년지기 지인을 산 채로 묻어 살해한 50대 여...   [살인혐의, 철원, 검찰송치]   \n",
       "3  신영자, 적용안된 혐의→검찰 상고에서 인정\\n신영자, 얼마를 어떻게 받았나  [헤럴...   [신영자, 갑질, 롯데백화점]   \n",
       "4  (청주=연합뉴스) 이승민 기자 = 지난 9월 청주의 한 하천에서 20대 여성을 둔기...       [공범, 살인, 과자]   \n",
       "\n",
       "                              extracted_keywords  \n",
       "0              {공장, 건강, 남성들, 지역, 부작용, 연기, 세보 효과}  \n",
       "1  {치아, 건강, 입냄새, 커피, 점심 식사, 낮잠, 디스크, 철분, 식후, 자세}  \n",
       "2      {경찰, 성관계, 지인, 앙심, 진술, 철원, 아들, 주변, 남편, 범행}  \n",
       "3   {신영자 이사장, 혐의, 롯데, 검찰, 유통업체, 징역, 매장, 네이처리퍼블릭}  \n",
       "4            {경찰, 혐의, 폭행, 쪽지, 유치장, 남자친구, 과자, 범행}  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daumData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "ct = Twitter()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로 부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12852/12852 [00:00<00:00, 271236.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12852, 73763)\n",
      "vocab size : 73763\n"
     ]
    }
   ],
   "source": [
    "trainName = './data/pre_data/news_train_test_Data/pre_data_word2vec_train_for_news_classification_by_ct.pickled'\n",
    "train = pickle.load(open(trainName, 'rb'))\n",
    "tfidf = bm.Build_tfidf(train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to Tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9372/9372 [1:17:52<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/pre_data/news_tagged_data/pre_data_daum_news_by_ct_for_word2vec_news_classification.pickled'):\n",
    "    daumData2 = pickle.load(open('./data/pre_data/news_tagged_data/pre_data_daum_news_by_ct_for_word2vec_news_classification.pickled', 'rb'))\n",
    "else:\n",
    "    daumData2 = bm.MakeTaggedDataDAUM2(daumData, TaggedDocument, ct, stopwords, 'daum')\n",
    "    pickle.dump(daumData2, open('./data/pre_data/news_tagged_data/pre_data_daum_news_by_ct_for_word2vec_news_classification.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec.load(loadModelPath+'word2vec_size-500_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-ct.model')\n",
    "model2 = Word2Vec.load(loadModelPath+'word2vec_size-500_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-ct.model')\n",
    "model3 = Word2Vec.load(loadModelPath+'word2vec_size-500_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-ct.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4542/80534 [00:00<00:01, 45269.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=80534, size=500, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80534/80534 [00:01<00:00, 42735.10it/s]\n",
      "2it [00:00, 16.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.912634\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:14, 36.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:17.644067\n"
     ]
    }
   ],
   "source": [
    "wv1, vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 500, daumData2)\n",
    "modelName = bm.Return_ModelName('word2vec', model1,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_2\n",
      "XGBoost\n",
      "RandomForestClassifier\n",
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "SVC\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 377738.86it/s]\n",
      "9372it [00:00, 330172.50it/s]\n",
      "9372it [00:00, 377177.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.6 s, sys: 1.22 s, total: 49.8 s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome = dict(map(lambda x: bm.PredictNewsClassification(vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome = pd.DataFrame.from_dict(predictOutcome)\n",
    "predictOutcome = predictOutcome.applymap(lambda x: le.inverse_transform(int(x)))\n",
    "predictOutcome = extDaumData.merge(predictOutcome,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome.to_csv('./outcome_news_classification/outcome_news_classification_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2063/80534 [00:00<00:03, 20623.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=80534, size=500, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80534/80534 [00:02<00:00, 30755.15it/s]\n",
      "1it [00:00,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:02.624976\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:42, 33.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:45.615299\n"
     ]
    }
   ],
   "source": [
    "wv1, vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 500, daumData2)\n",
    "modelName = bm.Return_ModelName('word2vec', model2,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_1\n",
      "SVC\n",
      "XGBoost\n",
      "LogisticRegression\n",
      "RandomForestClassifier\n",
      "NeuralNetwork_2\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 151431.98it/s]\n",
      "9372it [00:00, 75444.97it/s]\n",
      "9372it [00:00, 401887.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.7 s, sys: 1.31 s, total: 43 s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome = dict(map(lambda x: bm.PredictNewsClassification(vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome = pd.DataFrame.from_dict(predictOutcome)\n",
    "predictOutcome = predictOutcome.applymap(lambda x: le.inverse_transform(int(x)))\n",
    "predictOutcome = extDaumData.merge(predictOutcome,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome.to_csv('./outcome_news_classification/outcome_news_classification_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1820/80534 [00:00<00:04, 18195.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=80534, size=500, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80534/80534 [00:02<00:00, 30770.39it/s]\n",
      "1it [00:00,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:02.632258\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:48, 32.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:52.261539\n"
     ]
    }
   ],
   "source": [
    "wv1, vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 500, daumData2)\n",
    "modelName = bm.Return_ModelName('word2vec', model3,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_1\n",
      "SVC\n",
      "XGBoost\n",
      "RandomForestClassifier\n",
      "LogisticRegression\n",
      "NeuralNetwork_2\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 134862.40it/s]\n",
      "9372it [00:00, 164340.85it/s]\n",
      "9372it [00:00, 148127.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 1.33 s, total: 1min 9s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome = dict(map(lambda x: bm.PredictNewsClassification(vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome = pd.DataFrame.from_dict(predictOutcome)\n",
    "predictOutcome = predictOutcome.applymap(lambda x: le.inverse_transform(int(x)))\n",
    "predictOutcome = extDaumData.merge(predictOutcome,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome.to_csv('./outcome_news_classification/outcome_news_classification_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로 부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12852/12852 [00:00<00:00, 757414.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12852, 73416)\n",
      "vocab size : 73416\n"
     ]
    }
   ],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags category')\n",
    "trainName = './data/pre_data/news_train_test_Data/pre_data_word2vec_train_for_news_classification_by_mecab.pickled'\n",
    "train = pickle.load(open(trainName, 'rb'))\n",
    "tfidf = bm.Build_tfidf(train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to Tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9372/9372 [10:34<00:00, 14.78it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/pre_data/news_tagged_data/pre_data_daum_news_by_mecab_for_word2vec_news_classification.pickled'):\n",
    "    daumData2 = pickle.load(open('./data/pre_data/news_tagged_data/pre_data_daum_news_by_mecab_for_word2vec_news_classification.pickled', 'rb'))\n",
    "else:\n",
    "    daumData2 = bm.MakeTaggedDataDAUM2(daumData, TaggedDocument, mecab, stopwords, 'daum')\n",
    "    pickle.dump(daumData2, open('./data/pre_data/news_tagged_data/pre_data_daum_news_by_mecab_for_word2vec_news_classification.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec.load(loadModelPath+'word2vec_size-500_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "model2 = Word2Vec.load(loadModelPath+'word2vec_size-500_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-mecab.model')\n",
    "model3 = Word2Vec.load(loadModelPath+'word2vec_size-500_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-mecab.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1912/80260 [00:00<00:04, 18783.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=80260, size=500, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80260/80260 [00:03<00:00, 21288.49it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:03.796981\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [05:20, 29.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:05:30.228746\n"
     ]
    }
   ],
   "source": [
    "wv1, vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 500, daumData2)\n",
    "modelName = bm.Return_ModelName('word2vec', model1,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "XGBoost\n",
      "NeuralNetwork_2\n",
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "SVC\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 317284.55it/s]\n",
      "9372it [00:00, 79645.30it/s]\n",
      "9372it [00:00, 165258.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.1 s, sys: 1.7 s, total: 55.8 s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome = dict(map(lambda x: bm.PredictNewsClassification(vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome = pd.DataFrame.from_dict(predictOutcome)\n",
    "predictOutcome = predictOutcome.applymap(lambda x: le.inverse_transform(int(x)))\n",
    "predictOutcome = extDaumData.merge(predictOutcome,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome.to_csv('./outcome_news_classification/outcome_news_classification_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80260 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=80260, size=500, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80260/80260 [00:03<00:00, 25197.30it/s]\n",
      "1it [00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:03.194765\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8511it [04:56, 28.70it/s]"
     ]
    }
   ],
   "source": [
    "wv1, vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 500, daumData2)\n",
    "modelName = bm.Return_ModelName('word2vec', model2,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 223441.90it/s]\n",
      "9372it [00:00, 103733.32it/s]\n",
      "9372it [00:00, 112154.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.1 s, sys: 1.59 s, total: 45.6 s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome = dict(map(lambda x: bm.PredictNewsClassification(vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome = pd.DataFrame.from_dict(predictOutcome)\n",
    "predictOutcome = predictOutcome.applymap(lambda x: le.inverse_transform(int(x)))\n",
    "predictOutcome = extDaumData.merge(predictOutcome,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome.to_csv('./outcome_news_classification/outcome_news_classification_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 917/80260 [00:00<00:09, 8752.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=80260, size=500, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80260/80260 [00:03<00:00, 26628.90it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:03.023462\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:42, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:46.134652\n"
     ]
    }
   ],
   "source": [
    "wv1, vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 500, daumData2)\n",
    "modelName = bm.Return_ModelName('word2vec', model3,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_2\n",
      "LogisticRegression\n",
      "SVC\n",
      "NeuralNetwork_1\n",
      "RandomForestClassifier\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 393996.36it/s]\n",
      "9372it [00:00, 415515.54it/s]\n",
      "9372it [00:00, 337633.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.4 s, sys: 872 ms, total: 45.3 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome = dict(map(lambda x: bm.PredictNewsClassification(vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome = pd.DataFrame.from_dict(predictOutcome)\n",
    "predictOutcome = predictOutcome.applymap(lambda x: le.inverse_transform(int(x)))\n",
    "predictOutcome = extDaumData.merge(predictOutcome,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome.to_csv('./outcome_news_classification/outcome_news_classification_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
