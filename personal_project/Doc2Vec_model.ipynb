{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import html\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "import multiprocessing\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import numpy as np\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "mecab = Mecab()\n",
    "ct = ctwitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스 빅데이터 현황으로부터 감정분석 문장 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이젠 민주개혁의 길로 사설  이 90년대가 우리 모두에게 성취의 시대이기를 기원하고...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>민주당과 공화당은 새해부터 정계개편을 적극 추진한다는 방침이며 양당의 김영삼ㆍ김종필...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4당,지자제 연합공천 대비  파트너 탐색전  이같은 구상을 실현시키기 위해 민정당은...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90년대는 국내적으로 정치민주화와 선진국에로의 도약여부가 판가름날 것이고 한반도를 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>특히 노태우대통령이 3일 대국민 특별담화를 발표, 지난시대 의 완전청산을 선언한 후...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  이젠 민주개혁의 길로 사설  이 90년대가 우리 모두에게 성취의 시대이기를 기원하고...  1\n",
       "1  민주당과 공화당은 새해부터 정계개편을 적극 추진한다는 방침이며 양당의 김영삼ㆍ김종필...  1\n",
       "2  4당,지자제 연합공천 대비  파트너 탐색전  이같은 구상을 실현시키기 위해 민정당은...  1\n",
       "3  90년대는 국내적으로 정치민주화와 선진국에로의 도약여부가 판가름날 것이고 한반도를 ...  1\n",
       "4  특히 노태우대통령이 3일 대국민 특별담화를 발표, 지난시대 의 완전청산을 선언한 후...  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsSentence = pd.read_csv('./data/sentiment_data/merged_sentiment_data.txt',encoding='utf-8', header=None)\n",
    "newsSentence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveWords = pd.read_csv('./data/sentiment_data/positive_sentiment_word_from_news.csv',encoding='utf-8',header=None)\n",
    "negativeWords = pd.read_csv('./data/sentiment_data/negative_sentiment_word_from_news.csv', encoding='utf-8', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveWords = pd.concat([positiveWords, pd.DataFrame(len(positiveWords) * [1], columns=[1])], axis = 1)\n",
    "negativeWords = pd.concat([negativeWords, pd.DataFrame(len(negativeWords) * [0], columns=[1])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.concat([newsSentence, positiveWords, negativeWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.to_csv('./data/sentiment_data/raw_data_sentiment.txt',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 영화 리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:] # header\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = read_data('./data/sentiment_data/ratings.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter\n",
    "def tokenize1(doc):\n",
    "    return ['/'.join(t) for t in ct.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab\n",
    "def tokenize2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_doc_ct = [(tokenize1(row[1]), row[2]) for row in rating]\n",
    "news_doc_ct = [(tokenize1(newsSentence.loc[idx][0]), newsSentence.loc[idx][1]) for idx in newsSentence.index]\n",
    "ct_token = rating_doc_ct+news_doc_ct\n",
    "pickle.dump(ct_token, open('./data/pre_data/pre_data_by_ct_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_ct = [TaggedDocument(d, [c]) for d, c in ct_token]\n",
    "pickle.dump(tagged_ct, open('./data/pre_data/pre_by_ct_data_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_ct = pickle.load(open('./data/pre_data/pre_by_ct_data_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(tagged_ct, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('문/Noun', 0.29109200835227966),\n",
      " ('안철수/Noun', 0.27684521675109863),\n",
      " ('박근혜/Noun', 0.25886741280555725),\n",
      " ('정세균/Noun', 0.25190475583076477),\n",
      " ('손학규/Noun', 0.24621742963790894),\n",
      " ('이태근/Noun', 0.24516235291957855),\n",
      " ('박/Noun', 0.2449999749660492),\n",
      " ('김주학/Noun', 0.22950048744678497),\n",
      " ('김한길/Noun', 0.2208537608385086),\n",
      " ('추미애/Noun', 0.21908104419708252)]\n",
      "[('노/Noun', 0.4200497269630432),\n",
      " ('이명박/Noun', 0.4080064594745636),\n",
      " ('박근혜/Noun', 0.3919544219970703),\n",
      " ('박/Noun', 0.3099808096885681),\n",
      " ('김영삼/Noun', 0.3044901192188263),\n",
      " ('노태우/Noun', 0.2750924825668335),\n",
      " ('全斗換/Foreign', 0.27245914936065674),\n",
      " ('盧/Foreign', 0.2671722173690796),\n",
      " ('박정희/Noun', 0.2647184133529663),\n",
      " ('김대중/Noun', 0.2624438405036926)]\n",
      "[('박/Noun', 0.525599479675293),\n",
      " ('이명박/Noun', 0.4496101438999176),\n",
      " ('노무현/Noun', 0.3919544219970703),\n",
      " ('노/Noun', 0.3626362681388855),\n",
      " ('김/Noun', 0.3456989526748657),\n",
      " ('김대중/Noun', 0.3315896689891815),\n",
      " ('김영삼/Noun', 0.3314313292503357),\n",
      " ('“朴/Foreign', 0.314728707075119),\n",
      " ('金泳三/Foreign', 0.3001052737236023),\n",
      " ('朴/Foreign', 0.29720813035964966)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer = doc2vec.Doc2Vec(size=3000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(train)\n",
    "\n",
    "for epoch in range(30):\n",
    "    doc_vectorizer.train(train, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "    doc_vectorizer.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer.save('./model/doc2vec_size3000_epoch30_by_ct.model')\n",
    "pprint(doc_vectorizer.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size 2000, epoch 30 으로 모델 생성\n",
    "* size 3000, epoch 30 으로 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_doc_mecab = [(tokenize2(row[1]), row[2]) for row in rating]\n",
    "news_doc_mecab = [(tokenize2(newsSentence.loc[idx][0]), newsSentence.loc[idx][1]) for idx in newsSentence.index]\n",
    "mecab_token = rating_doc_mecab+news_doc_mecab\n",
    "pickle.dump(mecab_token, open('./data/pre_data/pre_data_by_mecab_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_mecab = [TaggedDocument(d, [c]) for d, c in mecab_token]\n",
    "pickle.dump(tagged_mecab, open('./data/pre_data/pre_by_mecab_data_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_mecab = pickle.load(open('./data/pre_data/pre_by_mecab_data_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, test2 = train_test_split(tagged_mecab, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('안철수/NNP', 0.4411509037017822),\n",
      " ('문/VV+ETM', 0.43933624029159546),\n",
      " ('손학규/NNP', 0.43808022141456604),\n",
      " ('정세균/NNP', 0.42908358573913574),\n",
      " ('김한길/NNP', 0.4065040349960327),\n",
      " ('김종인/NNP', 0.400411993265152),\n",
      " ('문재/NNG', 0.361728698015213),\n",
      " ('추미애/NNP', 0.3589673936367035),\n",
      " ('전혜숙/NNP', 0.3589380383491516),\n",
      " ('한화갑/NNP', 0.35712987184524536)]\n",
      "[('이명박/NNP', 0.5369610786437988),\n",
      " ('노/NNP', 0.5254369378089905),\n",
      " ('김영삼/NNP', 0.4930700957775116),\n",
      " ('박근혜/NNP', 0.47881996631622314),\n",
      " ('노/IC', 0.4319179356098175),\n",
      " ('노태우/NNP', 0.41837140917778015),\n",
      " ('최규하/NNP', 0.4099385738372803),\n",
      " ('김대중/NNP', 0.3965388536453247),\n",
      " ('이승만/NNP', 0.387555330991745),\n",
      " ('노/XPN', 0.3838627338409424)]\n",
      "[('이명박/NNP', 0.5188106894493103),\n",
      " ('박/NNP', 0.506790280342102),\n",
      " ('노무현/NNP', 0.47881999611854553),\n",
      " ('노태우/NNP', 0.3853256404399872),\n",
      " ('三/SH', 0.375473290681839),\n",
      " ('최규하/NNP', 0.37304985523223877),\n",
      " ('유셴코/NNP', 0.3641968369483948),\n",
      " ('김영삼/NNP', 0.36159783601760864),\n",
      " ('룰라/IC', 0.3606945872306824),\n",
      " ('MB/SL', 0.35922878980636597)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer2 = doc2vec.Doc2Vec(size=3000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer2.build_vocab(train2)\n",
    "\n",
    "for epoch in range(30):\n",
    "    doc_vectorizer2.train(train2, total_examples=doc_vectorizer2.corpus_count, epochs=doc_vectorizer2.iter)\n",
    "    doc_vectorizer2.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer2.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer2.save('./model/doc2vec_size3000_epoch30_by_mecab.model')\n",
    "pprint(doc_vectorizer2.most_similar('문재인/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('노무현/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('박근혜/NNP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size 2000, epoch 30 으로 모델 생성\n",
    "* size 3000, epoch 30 으로 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
