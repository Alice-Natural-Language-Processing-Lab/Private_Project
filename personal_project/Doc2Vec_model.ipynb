{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec model\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import html\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "import multiprocessing\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import numpy as np\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "mecab = Mecab()\n",
    "ct = ctwitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vector size\n",
    " [ 1000, 2000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None,encoding='utf-8')\n",
    "print (rawdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter\n",
    "def tokenize1(doc):\n",
    "    return ['/'.join(t) for t in ct.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc_ct = [(tokenize1(rawdata.loc[idx][0]), rawdata.loc[idx][1]) for idx in rawdata.index]\n",
    "pickle.dump(raw_doc_ct, open('./data/pre_data/pre_data_by_ct_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_ct = [TaggedDocument(d, [c]) for d, c in raw_doc_ct]\n",
    "pickle.dump(tagged_ct, open('./data/pre_data/pre_by_ct_data_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_ct = pickle.load(open('./data/pre_data/pre_by_ct_data_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(tagged_ct, test_size=0.15, random_state=42)\n",
    "\n",
    "pickle.dump(train, open('./data/pre_data/pre_by_ct_train.pickled','wb'))\n",
    "pickle.dump(test, open('./data/pre_data/pre_by_ct_test.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/pre_by_ct_train.pickled','rb'))\n",
    "test = pickle.load(open('./data/pre_data/pre_by_ct_test.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer = doc2vec.Doc2Vec(size=1000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(train)\n",
    "\n",
    "for epoch in range(20):\n",
    "    doc_vectorizer.train(train, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "    doc_vectorizer.alpha -= 0.0015  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer.save('./model/doc2vec_size1000_epoch30_by_ct.model')\n",
    "pprint(doc_vectorizer.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer = doc2vec.Doc2Vec(size=2000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(train)\n",
    "\n",
    "for epoch in range(20):\n",
    "    doc_vectorizer.train(train, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "    doc_vectorizer.alpha -= 0.0015  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer.save('./model/doc2vec_size2000_epoch20_by_ct.model')\n",
    "pprint(doc_vectorizer.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size 1000, epoch 20 으로 모델 생성\n",
    "* size 2000, epoch 20 으로 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer = doc2vec.Doc2Vec(size=2500, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(train)\n",
    "\n",
    "for epoch in range(20):\n",
    "    doc_vectorizer.train(train, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "    doc_vectorizer.alpha -= 0.0015  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer.save('./model/doc2vec_size2500_epoch20_by_ct.model')\n",
    "pprint(doc_vectorizer.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab\n",
    "def tokenize2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc_mecab = [(tokenize2(rawdata.loc[idx][0]), rawdata.loc[idx][1]) for idx in rawdata.index]\n",
    "pickle.dump(raw_doc_mecab, open('./data/pre_data/pre_data_by_mecab_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_mecab = [TaggedDocument(d, [c]) for d, c in raw_doc_mecab]\n",
    "pickle.dump(tagged_mecab, open('./data/pre_data/pre_by_mecab_data_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_mecab = pickle.load(open('./data/pre_data/pre_by_mecab_data_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, test2 = train_test_split(tagged_mecab, test_size=0.15, random_state=42)\n",
    "\n",
    "pickle.dump(train2, open('./data/pre_data/pre_by_mecab_train.pickled','wb'))\n",
    "pickle.dump(test2, open('./data/pre_data/pre_by_mecab_test.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pickle.load(open('./data/pre_data/pre_by_mecab_train.pickled','rb'))\n",
    "test2 = pickle.load(open('./data/pre_data/pre_by_mecab_test.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer2 = doc2vec.Doc2Vec(size=1000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer2.build_vocab(train2)\n",
    "\n",
    "for epoch in range(20):\n",
    "    doc_vectorizer2.train(train2, total_examples=doc_vectorizer2.corpus_count, epochs=doc_vectorizer2.iter)\n",
    "    doc_vectorizer2.alpha -= 0.0015  # decrease the learning rate\n",
    "    doc_vectorizer2.min_alpha = doc_vectorizer2.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer2.save('./model/doc2vec_size1000_epoch20_by_mecab.model')\n",
    "pprint(doc_vectorizer2.most_similar('문재인/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('노무현/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('박근혜/NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer2 = doc2vec.Doc2Vec(size=2000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer2.build_vocab(train2)\n",
    "\n",
    "for epoch in range(20):\n",
    "    doc_vectorizer2.train(train2, total_examples=doc_vectorizer2.corpus_count, epochs=doc_vectorizer2.iter)\n",
    "    doc_vectorizer2.alpha -= 0.0015  # decrease the learning rate\n",
    "    doc_vectorizer2.min_alpha = doc_vectorizer2.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer2.save('./model/doc2vec_size2000_epoch20_by_mecab.model')\n",
    "pprint(doc_vectorizer2.most_similar('문재인/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('노무현/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('박근혜/NNP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size 1000, epoch 20 으로 모델 생성\n",
    "* size 2000, epoch 20 으로 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer2 = doc2vec.Doc2Vec(size=2500, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer2.build_vocab(train2)\n",
    "\n",
    "for epoch in range(20):\n",
    "    doc_vectorizer2.train(train2, total_examples=doc_vectorizer2.corpus_count, epochs=doc_vectorizer2.iter)\n",
    "    doc_vectorizer2.alpha -= 0.0015  # decrease the learning rate\n",
    "    doc_vectorizer2.min_alpha = doc_vectorizer2.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer2.save('./model/doc2vec_size2500_epoch20_by_mecab.model')\n",
    "pprint(doc_vectorizer2.most_similar('문재인/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('노무현/NNP'))\n",
    "pprint(doc_vectorizer2.most_similar('박근혜/NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
