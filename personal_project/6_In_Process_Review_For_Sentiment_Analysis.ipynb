{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "> ## * Objective \n",
    "> * 직접 수집한 뉴스와 댓글의 긍부정 분석을 위하여 Doc2Vec, Word2Vec, FastText를 이용하여 만든 총 18개의 모델 중에서 어떠한 모델을 사용하는 것이 좀 더 정확할 것인지를 알아보기 위함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## * Tagger\n",
    "> 임의로 사용자사전을 추가할 수 있는 것을 택함\n",
    ">> * [customized-Konlpy](https://github.com/lovit/customized_konlpy)의 Twitter\n",
    ">> * [konlpy](http://konlpy.org/en/v0.4.4/)의 Mecab\n",
    "> * 2개의 Tagger로 Tagging하여 분석을 각각 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## * Classification\n",
    "> 생성된 모델에 대하여 5개의 알고리즘 총 6개(Neural Network는 2개)의 분류모델을 사용함.\n",
    ">> Logistic Regression, Random Forest, Kernel SVM, XGBoost, Neural Network\n",
    "\n",
    "> 동일한 train dataset과 test dataset로 classification 모델을 만듦\n",
    "> #### * [Logistic Regression](https://datascienceschool.net/view-notebook/d0df94cf8dd74e8daec7983531f68dfc/)\n",
    "> #### * [Random Forest](https://datascienceschool.net/view-notebook/766fe73c5c46424ca65329a9557d0918/)\n",
    "> #### * [Kernel SVM](https://datascienceschool.net/view-notebook/69278a5de79449019ad1f51a614ef87c/)\n",
    "> #### * [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n",
    "> #### * [Neural Network](https://datascienceschool.net/view-notebook/0178802a219c4e6bb9b820b49bf57f91/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "saveTrainPath = './data/pre_data/train_test_Data2/'\n",
    "saveClassifierPath = './data/pre_data/classifier/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491510, 2)\n"
     ]
    }
   ],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None,encoding='utf-8')\n",
    "print (rawdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 총 491510개의 데이터로 구성됨\n",
    "> * [NAVER sentiment movie corpus](https://github.com/e9t/nsmc)의 20만건과 뉴스데이터베이스 'Kinds' 기반 분석 자료인 [공공데이터포털-뉴스빅데이터 분석 정보](https://www.data.go.kr/dataset/15012945/fileData.do)의 일부 자료에서 제공하는 긍정문장과 부정문장을 사용함\n",
    "> * 추가 사전작업을 통해 총 491,510건의 감정분석을 위한 사전 데이터를 수집\n",
    "> * train dataset과 test dataset은 9:1의 비율로 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## * Model\n",
    "> Twitter와 Mecab으로 각각 tagging된 데이터에 대하여 모델을 생성.  \n",
    "> Continuous Bag of Words(CBOW), Skip-Gram 각각 이용\n",
    "> * 단어를 vector로 바꿔주는 알고리즘 : Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### * [Word2Vec](4_Make_Word2Vec_model_For_sentiment_analysis.ipynb)\n",
    "> * Neural Network Language Model(NNLM)을 계승하면서도 학습 속도와 성능을 끌어올림\n",
    "> * 단어를 vectorization할 때 단어의 문맥적 의미를 보존함.\n",
    "> * In case you missed the buzz, word2vec is a widely featured as a member of the “new wave” of machine learning algorithms based on neural networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow).\n",
    "> * Using large amounts of unannotated plain text, word2vec learns relationships between words automatically.\n",
    "> * The output are vectors, one vector per word, with remarkable linear relationships that allow us to do things like vec(“king”) – vec(“man”) + vec(“woman”) =~ vec(“queen”), or vec(“Montreal Canadiens”) – vec(“Montreal”) + vec(“Toronto”) resembles the vector for “Toronto Maple Leafs”.\n",
    "> * Word2vec is very useful in automatic text tagging, recommender systems and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### * Continuous Bag of Words (CBOW)  \n",
    "> * 주변에 있는 단어들을 가지고 중심에 있는 단어를 맞추는 방식\n",
    "> ex) 나는 _에 간다.\n",
    "\n",
    "> ![cbow](./cbow.png)\n",
    "> * 주어진 단어에 대해 앞 뒤로 c/2개 씩 총 C개의 단어를 Input으로 사용하여, 주어진 단어를 맞추기 위한 네트워크를 만든다. \n",
    "> * Input Layer, Projection Layer, Output Layer로 구성\n",
    "> * Input Layer에서 중간 레이어로 가는 과정이 weight를 곱해주는 것이라기 보다는 단순히 Projection하는 과정에 가까움\n",
    "> * Input Layer에서 Projection Layer로 갈 때는 모든 단어들이 공통적으로 사용하는 V*N 크기의 Projection Matrix W가 있다. (N은 Projection Layer의 길이 = 사용할 벡터의 길이)\n",
    "> * Projection Layer에서 Output Layer로 갈 때는 N*V 크기의 Weight Matrix W'가 있다. \n",
    "> * Input layer에서는 NNLM모델과 똑같이 단어를 One-hot encoding으로 넣어주고, 여러 개의 단어를 각각 Projection시킨 후 그 벡터들의 평균을 구해서 Projection Layer에 보낸다. \n",
    "> * 그 뒤는 여기에 Weight Matrix를 곱해서 Output Layer로 보내고 softmax계산을 한후, 이 결과를 진짜 단어의 one-hot encoding과 비교하여 에러를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### * Skip-Gram\n",
    "> * 중심에 있는 단어로 주변 단어를 예측하는 방법\n",
    "> ex) _ 외나무다리 __\n",
    "\n",
    "> ![skip-gram](./skip-gram.png)\n",
    "> * 예측하는 단어들의 경우 현재 단어 주위에서 샘플링하는데, '가까이 위치해있는 단어일수록 현재 단어와 관련이 더 많은 단어일것이다'라는 생각을 적용하기 위해 멀리 떨어져 있는 단어수록 낮은 확률로 택하는 방법을 사용한다. \n",
    "> * 나머지 구조는 CBOW와 방향만 반대일 뿐 굉장히 유사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### * 각 모델의 Classifier 생성 결과\n",
    "> #### [Word2Vec](5_Train_Classifier_Using_Word2Vec_For_Sentiment_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2Vec을 만드는데 사용한 Parameters\n",
    "> * sg : the training algorithm \n",
    ">> 1 : Continuous Bag of Words(CBOW)  \n",
    ">> 0 : Skip-gram\n",
    "> * size : Dimensionality of the feature vectors  \n",
    "> * window : The maximum distance between the current and predicted word within a sentence  \n",
    "> * min_count : lgnores alll words with total frequency lower than this  \n",
    "> * cbow_mean : the context word vectors\n",
    ">> 0 : sum of the context word vectors  \n",
    ">> 1 : the mean, only applies when cbow is used  \n",
    "> * negative : negative sampling\n",
    ">> \\>0 : the int for negative specifies how many 'noise words' should be drawn (usually between 5-20)  \n",
    ">> 0 : no negative sampling\n",
    "> * hs : Hierarchical softmax\n",
    ">> 1 : hierarchical softmax will be used for model training  \n",
    ">> 0 : & negative is non-zero, negative sampling will be used  \n",
    "> * workers : Use these many worker threads to train the model ( = faster training with multicore machines)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model을 총 3개 만듦\n",
    "> 1. sg옵션 - Skip-gram, cbow_mean 0\n",
    "> 2. sg옵션 - Skip-gram, cbow_mean 1 \n",
    ">> CBOW로 만들었어야 했는데 잘못 만든 것으로 판단\n",
    "> 3. sg옵션 - CBOW, cbow_mean 0\n",
    "\n",
    "> 나머지 옵션은 동일함\n",
    ">> size : 1000  \n",
    ">> hs : 0  \n",
    ">> min_count : 2  \n",
    ">> epoch : 20  \n",
    ">> window : 10  \n",
    ">> negative : 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * tagger : Twitter\n",
    "* number of words in model's vocabulary\n",
    "> 162640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1의 결과\n",
    "> Skip-gram, cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.780\n",
    "* Random Forest : 0.739\n",
    "* C-Support Vector : 0.489\n",
    "* XGBoost : 0.772\n",
    "* Neural Network1 : 0.788\n",
    "* Neural Network2 : 0.785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2의 결과\n",
    "> Skip-gram, cbow_mean - the mean of the context word vectors\n",
    "* Logistic Regression : 0.816\n",
    "* Random Forest : 0.804\n",
    "* C-Support Vector : 0.497\n",
    "* XGBoost : 0.825\n",
    "* Neural Network1 : 0.8318\n",
    "* Neural Network2 : 0.8336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3의 결과\n",
    "> Continuous Bag of Words (CBOW), cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.828\n",
    "* Random Forest : 0.803\n",
    "* C-Support Vector : 0.491\n",
    "* XGBoost : 0.830\n",
    "* Neural Network1 : 0.8427\n",
    "* Neural Network2 : 0.8487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *  tagger : Mecab\n",
    "* number of words in model's vocabulary\n",
    "> 165361"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1의 결과\n",
    "> Skip-gram, cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.790\n",
    "* Random Forest : 0.745\n",
    "* C-Support Vector : 0.488\n",
    "* XGBoost : 0.774\n",
    "* Neural Network1 : 0.788\n",
    "* Neural Network2 : 0.788"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2의 결과\n",
    "> Skip-gram, cbow_mean - the mean of the context word vectors\n",
    "* Logistic Regression : 0.819\n",
    "* Random Forest : 0.803\n",
    "* C-Support Vector : 0.492\n",
    "* XGBoost : 0.825\n",
    "* Neural Network1 : 0.8294\n",
    "* Neural Network2 : 0.8340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3의 결과\n",
    "> Continuous Bag of Words (CBOW), cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.829\n",
    "* Random Forest : 0.802\n",
    "* C-Support Vector : 0.489\n",
    "* XGBoost : 0.826\n",
    "* Neural Network1 : 0.8421\n",
    "* Neural Network2 : 0.8465"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* C-Support Vector의 Classification accuracy가 다른 classifier보다 떨어지는 것을 확인할 수 있다. \n",
    "> 시간이 너무 오래 걸려서 iteration의 최대치를 1500으로 제한을 둚.   \n",
    "> scale도 하는등의 방법을 통해 시간을 단축시켜 보려함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### * [FastText](./4_Make_FastText_model_For_sentiment_analysis.ipynb)\n",
    "> * 페이스북에서 개발한 단어 임베딩 기술  \n",
    "> * 구글에서 개발한 fastText을 기본으로 하되 부분단어들을 Embedding하는 기법.  \n",
    "> * 단어가 가지는 형태 정보를 학습할 수 있어, 다양한 접사가 존재하는 한국어같은 언어에 대해서 잘 동작  \n",
    "> * The main principle behind fastText is that **<U>the morphological structure of a word carries important information about the meaning of the word, which is not taken into account by traditional word embeddings, which train a unique word embedding for every individual word**</U>. This is especially significant for morphologically rich languages (German, Turkish) in which a single word can have a large number of morphological forms, each of which might occur rarely, thus making it hard to train good word embeddings.\n",
    "> * fastText attempts to solve this by treating each word as the aggregation of its subwords. For the sake of simplicity and language-independence, subwords are taken to be the character ngrams of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.\n",
    "> * According to a detailed comparison of Word2Vec and FastText in this notebook, fastText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. Word2Vec slightly outperforms FastText on semantic tasks though. The differences grow smaller as the size of training corpus increases. Training time for fastText is significantly higher than the Gensim version of Word2Vec (15min 42s vs 6min 42s on text8, 17 mil tokens, 5 epochs, and a vector size of 100).\n",
    ". * fastText can be used to obtain vectors for out-of-vocabulary (OOV) words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data.\n",
    "\n",
    "> ### * 각 모델의 Classifier 생성 결과\n",
    "> #### [FastText](./5_Train_Classifier_Using_FastText_For_Sentiment_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FastText을 만드는데 사용한 Parameters\n",
    "> * sg : the training algorithm \n",
    ">> 1 : Skip-gram  \n",
    ">> 0 : Continuous Bag of Words(CBOW)  \n",
    "> * size : Dimensionality of the feature vectors  \n",
    "> * window : The maximum distance between the current and predicted word within a sentence  \n",
    "> * min_count : lgnores alll words with total frequency lower than this  \n",
    "> * cbow_mean : the context word vectors\n",
    ">> 0 : sum of the context word vectors  \n",
    ">> 1 : the mean, only applies when cbow is used  \n",
    "> * negative : negative sampling\n",
    ">> \\>0 : the int for negative specifies how many 'noise words' should be drawn (usually between 5-20)  \n",
    ">> 0 : no negative sampling\n",
    "> * hs : Hierarchical softmax\n",
    ">> 1 : hierarchical softmax will be used for model training  \n",
    ">> 0 : & negative is non-zero, negative sampling will be used  \n",
    "> * workers : Use these many worker threads to train the model ( = faster training with multicore machines)    \n",
    "> * word_ngrams : subword(ngrams) information\n",
    ">> 1 : uses enriches word vectors with subword(ngrams) information  \n",
    ">> 0 : equivalent to word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model을 총 3개 만듦\n",
    "> 1. sg옵션 - Continuous Bag of Words (CBOW), cbow_mean 0\n",
    "> 2. sg옵션 - Continuous Bag of Words (CBOW), cbow_mean 1 \n",
    "> 3. sg옵션 - Skip-gram, cbow_mean 0\n",
    "\n",
    "> 나머지 옵션은 동일함\n",
    ">> size : 1000  \n",
    ">> hs : 0  \n",
    ">> min_count : 2  \n",
    ">> epoch : 20  \n",
    ">> window : 10  \n",
    ">> negative : 7  \n",
    ">> word_ngrams : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * tagger : Twitter\n",
    "* number of words in model's vocabulary\n",
    "> 162564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1의 결과\n",
    "> Continuous Bag of Words (CBOW), cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.794\n",
    "* Random Forest : 0.750\n",
    "* C-Support Vector : 0.490\n",
    "* XGBoost : 0.781\n",
    "* Neural Network1 : 0.8036\n",
    "* Neural Network2 : 0.8053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2의 결과\n",
    "> Continuous Bag of Words (CBOW), cbow_mean - the mean of the context word vectors\n",
    "* Logistic Regression : 0.818\n",
    "* Random Forest : 0.804\n",
    "* C-Support Vector : 0.493\n",
    "* XGBoost : 0.828\n",
    "* Neural Network1 : 0.8321\n",
    "* Neural Network2 : 0.8396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3의 결과\n",
    "> Skip-Gram, cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.830\n",
    "* Random Forest : 0.806\n",
    "* C-Support Vector : 0.492\n",
    "* XGBoost : 0.832\n",
    "* Neural Network1 : 0.8511\n",
    "* Neural Network2 : 0.8499"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *  tagger : Mecab\n",
    "* number of words in model's vocabulary\n",
    "> 165823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1의 결과\n",
    "> Continuous Bag of Words (CBOW), cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.799\n",
    "* Random Forest : 0.758\n",
    "* C-Support Vector : 0.488\n",
    "* XGBoost : 0.787\n",
    "* Neural Network1 : 0.7980\n",
    "* Neural Network2 : 0.8070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2의 결과\n",
    "> Continuous Bag of Words (CBOW), cbow_mean - the mean of the context word vectors\n",
    "* Logistic Regression : 0.818\n",
    "* Random Forest : 0.804\n",
    "* C-Support Vector : 0.495\n",
    "* XGBoost : 0.824\n",
    "* Neural Network1 : 0.8350\n",
    "* Neural Network2 : 0.8294"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3의 결과\n",
    "> Skip-Gram, cbow_mean - sum of the context word vectors\n",
    "* Logistic Regression : 0.828\n",
    "* Random Forest : 0.801\n",
    "* C-Support Vector : 0.490\n",
    "* XGBoost : 0.828\n",
    "* Neural Network1 : 0.8463\n",
    "* Neural Network2 : 0.8477"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* C-Support Vector의 Classification accuracy가 다른 classifier보다 떨어지는 것을 확인할 수 있다. \n",
    "> 시간이 너무 오래 걸려서 iteration의 최대치를 1500으로 제한을 둚.   \n",
    "> scale도 하는등의 방법을 통해 시간을 단축시켜 보려함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### * [Doc2Vec](./4_Make_Doc2Vec_model_for_Sentiment_analysis.ipynb)\n",
    "> #### * Paragraph Vector\n",
    "> * Le and Mikolov 2014 introduces the Paragraph Vector, which outperforms more naïve representations of documents such as averaging the Word2vec word vectors of a document. The idea is straightforward: we act as if a paragraph (or document) is just another vector like a word vector, but we will call it a paragraph vector. We determine the embedding of the paragraph in vector space in the same way as words. Our paragraph vector model considers local word order like bag of n-grams, but gives us a denser representation in vector space compared to a sparse, high-dimensional representation.\n",
    "> * Paragraph Vector - Distributed Memory (PV-DM)\n",
    "> * This is the Paragraph Vector model **<U>analogous to Continuous-bag-of-words Word2vec</U>**. The paragraph vectors are obtained by training a neural network on the fake task of **<U>inferring a center word based on context words and a context paragraph.</U>** A paragraph is a context for all words in the paragraph, and a word in a paragraph can have that paragraph as a context.\n",
    "\n",
    "> * Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "> * This is the Paragraph Vector model **<U>analogous to Skip-gram Word2vec</U>**. The paragraph vectors are obtained by training a neural network on the fake task of **<U>predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph</U>**.\n",
    "\n",
    "> ### * 각 모델의 Classifier 생성 결과\n",
    "> #### [Doc2vec](./5_Train_Classifier_Using_Doc2Vec_For_Sentiment_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Doc2Vec을 만드는데 사용한 Parameters\n",
    "> * dm : the training algorithm \n",
    ">> 1 : distributed memory (PV-DM)  \n",
    ">> 0 : distributed bag of words (PV-DBOW)\n",
    "> * size : Dimensionality of the feature vectors  \n",
    "> * window : The maximum distance between the current and predicted word within a sentence  \n",
    "> * negative : negative sampling\n",
    ">> \\>0 : the int for negative specifies how many 'noise words' should be drawn (usually between 5-20)  \n",
    ">> 0 : no negative sampling\n",
    "> * hs : Hierarchical softmax\n",
    ">> 1 : hierarchical softmax will be used for model training  \n",
    ">> 0 : & negative is non-zero, negative sampling will be used  \n",
    "> * workers : Use these many worker threads to train the model ( = faster training with multicore machines)  \n",
    "> * alpha : the initial learning rate  \n",
    "> * min_alpha : learning rate will linearly drop to min_alpha as training progresses  \n",
    "> * dm_concat : concatenation of context vectors rather than sum/average  \n",
    ">> 1 : use  \n",
    ">> 0 : not use\n",
    "> * dm_mean : the mean of the context word vectors\n",
    ">> 1 : use the mean. only applies when dm is used in non-concatenative mode  \n",
    ">> 0 : use the sum of the context word vectors  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model을 총 3개 만듦\n",
    "> 1. dm옵션 - distributed memory (PV-DM), dm_mean 1, window 10\n",
    "> 2. dm옵션 - distributed memory (PV-DM), dm_concat 1, window 5\n",
    "> 3. dm옵션 - distributed bag of words (PV-DBOW), cbow_mean 0, dm_concat 0, window default\n",
    "\n",
    "> 나머지 옵션은 동일함\n",
    ">> size : 1000  \n",
    ">> hs : 0   \n",
    ">> epoch : 20  \n",
    ">> negative : 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * tagger : Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1의 결과\n",
    "> dm옵션 - distributed memory (PV-DM), dm_mean 1, window 10\n",
    "* Logistic Regression : 0.692\n",
    "* Random Forest : 0.622\n",
    "* C-Support Vector : 0.570\n",
    "* XGBoost : 0.688\n",
    "* Neural Network1 : 0.7104\n",
    "* Neural Network2 : 0.7059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2의 결과\n",
    "> dm옵션 - distributed memory (PV-DM), dm_concat 1, window 5\n",
    "* Logistic Regression : 0.507\n",
    "* Random Forest : 0.535\n",
    "* C-Support Vector : 0.489\n",
    "* XGBoost : 0.542\n",
    "* Neural Network1 : 0.5405\n",
    "* Neural Network2 : 0.5421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3의 결과\n",
    "> dm옵션 - distributed bag of words (PV-DBOW), cbow_mean 0, dm_concat 0, window default\n",
    "* Logistic Regression : 0.827\n",
    "* Random Forest : 0.741\n",
    "* C-Support Vector : 0.609\n",
    "* XGBoost : 0.809\n",
    "* Neural Network1 : 0.8474\n",
    "* Neural Network2 : 0.8481"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *  tagger : Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1의 결과\n",
    "> dm옵션 - distributed memory (PV-DM), dm_mean 1, window 10\n",
    "* Logistic Regression : 0.704\n",
    "* Random Forest : 0.636\n",
    "* C-Support Vector : 0.573\n",
    "* XGBoost : 0.698\n",
    "* Neural Network1 : 0.7290\n",
    "* Neural Network2 : 0.7280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2의 결과\n",
    "> dm옵션 - distributed memory (PV-DM), dm_concat 1, window 5\n",
    "* Logistic Regression : 0.508\n",
    "* Random Forest : 0.535\n",
    "* C-Support Vector : 0.489\n",
    "* XGBoost : 0.542\n",
    "* Neural Network1 : 0.5405\n",
    "* Neural Network2 : 0.5421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3의 결과\n",
    "> dm옵션 - distributed bag of words (PV-DBOW), cbow_mean 0, dm_concat 0, window default\n",
    "* Logistic Regression : 0.830\n",
    "* Random Forest : 0.743\n",
    "* C-Support Vector : 0.610\n",
    "* XGBoost : 0.816\n",
    "* Neural Network1 : 0.8537\n",
    "* Neural Network2 : 0.8522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* C-Support Vector의 Classification accuracy가 다른 classifier보다 떨어지는 것을 확인할 수 있다. \n",
    "> 시간이 너무 오래 걸려서 iteration의 최대치를 1500으로 제한을 둚.   \n",
    "> scale도 하는등의 방법을 통해 시간을 단축시켜 보려함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타 모델에 대한 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Language Model (NNLM)\n",
    "> * Feed-Forward Neural Network Language Model\n",
    "> * 단어를 vector로 바꾸는 neural network 기반 방법론\n",
    "> * 컴퓨터는 단어를 숫자로 바꿔서 입력해야 컴퓨터는 연산을 수행\n",
    "> * Input Layer, Projection Layer, Hidden Layer, Output Layer로 이우러진 Neural Network\n",
    "> ![image](./nnlm.png)\n",
    "\n",
    ">> 1. 현재 보고 있는 단어 이전의 단어들 N개를 one-hot encoding으로 벡터화\n",
    ">> 2. 사전의 크기를 V라고 하고 Projection Layer의 크기를 P라고 했을 때, 각각의 벡터들은 V*P 크기의 Projection Matrix에 의해 다음 layer로 넘어가게 된다.\n",
    ">> 3. Projection Layer의 값을 input이라고 생각하고, 크기 H인 hidden layer를 거쳐서 output layer에서 **각 단어들이 나올 확률**을 계산\n",
    ">> 4. 실제 단어의 one-hot encoding 벡터와 비교하여 에러를 계산하고, 이를 back-propagation해서 네트워크의 weight들을 최적화해나가는 것\n",
    "\n",
    "* 단점\n",
    "> * 몇 개의 단어를 볼 건지에 대한 parameter N이 고정되어 있고, 정해주어야 한다. \n",
    "> * 이전의 단어들에 대해서만 신경쓸 수 있고, 현재 보고 있는 단어 앞에 있는 단어들을 고려하지 못한다. \n",
    "> * **느리다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network Language Model ( RNNLM)\n",
    "> * NNLM을 Recurrent Neural Network의 형태로 변형한 것\n",
    "> * Projection Layer없이 input, hidden, output layer로만 구성\n",
    "> * **hidden layer에 recurrent한 연결이 있어 이전 시간의 Hidden Layer의 입력이 다시 입력되는 형식으로 구성**\n",
    "> ![image](./rnnlm.png)\n",
    "> * 그림에서 U라고 나타나 있는 부분이 Word Embedding으로 사용되며, **Hidden layer의 크기를 H라고 할 때 각 단어는 길이 H의 벡터로 표현**\n",
    "> * NNLM과 달리 몇 개의 단어인지에 대해 정해줄 필요가 없이, 학습시켜줄 단어를 순차적으로 입력해주는 방식으로 학습\n",
    "> * NNLM보다 연산량이 적다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이론 참고 \n",
    "\n",
    "> [ratsgo's blog](https://ratsgo.github.io/)  \n",
    ">> 1. [Word2Vec으로 문장 분류하기](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)  \n",
    ">> 2. [빈도수 세기의 놀라운 마법 Word2Vec, Glove, Fasttext](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/)  \n",
    ">> 3. [Neural Network Language Model](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/)  \n",
    ">> 4. [Word2Vec의 학습 방식](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/)  \n",
    "\n",
    "> [data scienceschool](https://datascienceschool.net)  \n",
    ">> 1. [단어 임베딩과 word2vec](https://datascienceschool.net/view-notebook/6927b0906f884a67b0da9310d3a581ee/)  \n",
    ">> 2. [Scikit-Learn의 문서 전처리 기능](https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/)  \n",
    ">> 3. [확률론적 언어 모형](https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/)  \n",
    "\n",
    "> [BEOMSU KIM](https://shuuki4.wordpress.com/category/deep-learning/)\n",
    ">>  1. [word2vec 관련 이론 정리](https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
