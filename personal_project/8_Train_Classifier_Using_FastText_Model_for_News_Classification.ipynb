{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier For News Classification\n",
    "> ## * fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import FastText, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Basic_Module as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "print (cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * words : 기사에서 나온 단어들 or keywords\n",
    "> * tags : 문서 tag\n",
    "> * classes : category\n",
    ">> 기사분류가 daum보다 naver에서 더 세분화되어 있기 때문에 네이버의 category 분류를 이용하기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/pre_data/news_tagged_data/pre_data_category_label_encoder_by_ct_for_fastText_news_classification.pickled'):\n",
    "    le = pickle.load(open('./data/pre_data/news_tagged_data/pre_data_category_label_encoder_by_ct_for_fastText_news_classification.pickled','rb'))\n",
    "else:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(naverData['category'])\n",
    "    pickle.dump(le, open('./data/pre_data/news_tagged_data/pre_data_category_label_encoder_by_ct_for_fastText_news_classification.pickled','wb'))\n",
    "print (le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = './news_model/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/news_model/'\n",
    "saveTrainPath = './data/pre_data/news_train_test_Data2/'\n",
    "saveClassifierPath = './data/pre_data/news_classifier/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 전 단계에서 필요한 사전 데이터는 만들어 둠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Set & Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainName = './data/pre_data/news_train_test_Data/pre_data_fastText_train_for_news_classification_by_ct.pickled'\n",
    "testName = './data/pre_data/news_train_test_Data/pre_data_fastText_test_for_news_classification_by_ct.pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(trainName) & os.path.isfile(testName):\n",
    "    train = pickle.load(open(trainName, 'rb'))\n",
    "    test = pickle.load(open(testName, 'rb'))\n",
    "else:\n",
    "    train, test = train_test_split(w2v_docs, test_size = 0.15)\n",
    "    pickle.dump(train,open(trainName,'wb'))\n",
    "    pickle.dump(test,open(testName,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set을 사용하여 Tf-Idf vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = bm.Build_tfidf(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ x.words for x in tqdm(train)] \n",
    "y_train = np.array([ x.category for x in tqdm(train)])\n",
    "x_test = [ x.words for x in tqdm(test)] \n",
    "y_test = np.array([ x.category for x in tqdm(test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y2, test_y2 = bm.ReMake_Outcome(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FastText.load(loadModelPath+'fastText_size-500_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-ct.model')\n",
    "model2 = FastText.load(loadModelPath+'fastText_size-500_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-ct.model')\n",
    "model3 = FastText.load(loadModelPath+'fastText_size-500_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-ct.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = bm.Make_Pre_Data(model1, tfidf, 500, train, test)\n",
    "modelName = bm.Return_ModelName('fastText', model1,'ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bm.Make_TSNE2(2, model1, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 :  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out1 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out2 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier2, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier2, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier3 =  SVC(kernel = 'rbf',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out3 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier3, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier3, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree = 0.7\n",
    "params = {\n",
    "    'objective' : 'multi:softmax', \n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : max_depth, \n",
    "    'subsample' : subsample,\n",
    "    #'eval_metric' : 'auc', \n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'silent' : 1, \n",
    "    'eta' : 0.175,\n",
    "    'nthread' : cores,\n",
    "    'num_class' : 8\n",
    "}\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist,\n",
    "                early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_acc = accuracy_score(y_test, test_prediction)\n",
    "print (test_acc)\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out4 = bm.Roc_Curve_MultiClass(xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(xgb.DMatrix(train_vecs_w2v), train_y2, xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Plot_Roc_Curver_Micro_Macro(roc_auc_out1, roc_auc_out2, roc_auc_out3, roc_auc_out4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델  : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np_utils.to_categorical(y_test,8)\n",
    "y_train2 = np_utils.to_categorical(y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation = 'sigmoid', kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dense(64, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(32, activation = 'relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = bm.Make_Pre_Data(model2, tfidf, 500, train, test)\n",
    "modelName = bm.Return_ModelName('fastText', model2,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bm.Make_TSNE2(2, model2, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 :  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out1 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out2 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier2, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier2, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier3 =  SVC(kernel = 'rbf',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out3 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier3, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier3, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree = 0.7\n",
    "params = {\n",
    "    'objective' : 'multi:softmax', \n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : max_depth, \n",
    "    'subsample' : subsample,\n",
    "    #'eval_metric' : 'auc', \n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'silent' : 1, \n",
    "    'eta' : 0.175,\n",
    "    'nthread' : cores,\n",
    "    'num_class' : 8\n",
    "}\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist,\n",
    "                early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_acc = accuracy_score(y_test, test_prediction)\n",
    "print (test_acc)\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out4 = bm.Roc_Curve_MultiClass(xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(xgb.DMatrix(train_vecs_w2v), train_y2, xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Plot_Roc_Curver_Micro_Macro(roc_auc_out1, roc_auc_out2, roc_auc_out3, roc_auc_out4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델  : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np_utils.to_categorical(y_test,8)\n",
    "y_train2 = np_utils.to_categorical(y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation = 'sigmoid', kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dense(64, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(32, activation = 'relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = bm.Make_Pre_Data(model3, tfidf, 500, train, test)\n",
    "modelName = bm.Return_ModelName('fastText', model3,'ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bm.Make_TSNE2(2, model3, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 :  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out1 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out2 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier2, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier2, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier3 =  SVC(kernel = 'rbf',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out3 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier3, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier3, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree = 0.7\n",
    "params = {\n",
    "    'objective' : 'multi:softmax', \n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : max_depth, \n",
    "    'subsample' : subsample,\n",
    "    #'eval_metric' : 'auc', \n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'silent' : 1, \n",
    "    'eta' : 0.175,\n",
    "    'nthread' : cores,\n",
    "    'num_class' : 8\n",
    "}\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist,\n",
    "                early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_acc = accuracy_score(y_test, test_prediction)\n",
    "print (test_acc)\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out4 = bm.Roc_Curve_MultiClass(xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(xgb.DMatrix(train_vecs_w2v), train_y2, xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Plot_Roc_Curver_Micro_Macro(roc_auc_out1, roc_auc_out2, roc_auc_out3, roc_auc_out4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델  : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np_utils.to_categorical(y_test,8)\n",
    "y_train2 = np_utils.to_categorical(y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation = 'sigmoid', kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dense(64, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(32, activation = 'relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 전 단계에서 필요한 사전 데이터는 만들어 둠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Set & Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainName = './data/pre_data/news_train_test_Data/pre_data_fastText_train_for_news_classification_by_mecab.pickled'\n",
    "testName = './data/pre_data/news_train_test_Data/pre_data_fastText_test_for_news_classification_by_mecab.pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(trainName) & os.path.isfile(testName):\n",
    "    train = pickle.load(open(trainName, 'rb'))\n",
    "    test = pickle.load(open(testName, 'rb'))\n",
    "else:\n",
    "    train, test = train_test_split(w2v_docs, test_size = 0.15)\n",
    "    pickle.dump(train,open(trainName,'wb'))\n",
    "    pickle.dump(test,open(testName,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set을 사용하여 Tf-Idf vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = bm.Build_tfidf(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ x.words for x in tqdm(train)] \n",
    "y_train = np.array([ x.category for x in tqdm(train)])\n",
    "x_test = [ x.words for x in tqdm(test)] \n",
    "y_test = np.array([ x.category for x in tqdm(test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y2, test_y2 = bm.ReMake_Outcome(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FastText.load(loadModelPath+'fastText_size-500_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "model2 = FastText.load(loadModelPath+'fastText_size-500_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-mecab.model')\n",
    "model3 = FastText.load(loadModelPath+'fastText_size-500_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-mecab.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = bm.Make_Pre_Data(model1, tfidf, 500, train, test)\n",
    "modelName = bm.Return_ModelName('fastText', model1,'mecab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bm.Make_TSNE2(2, model1, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 :  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out1 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out2 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier2, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier2, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier3 =  SVC(kernel = 'rbf',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out3 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier3, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier3, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree = 0.7\n",
    "params = {\n",
    "    'objective' : 'multi:softmax', \n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : max_depth, \n",
    "    'subsample' : subsample,\n",
    "    #'eval_metric' : 'auc', \n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'silent' : 1, \n",
    "    'eta' : 0.175,\n",
    "    'nthread' : cores,\n",
    "    'num_class' : 8\n",
    "}\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist,\n",
    "                early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_acc = accuracy_score(y_test, test_prediction)\n",
    "print (test_acc)\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out4 = bm.Roc_Curve_MultiClass(xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(xgb.DMatrix(train_vecs_w2v), train_y2, xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Plot_Roc_Curver_Micro_Macro(roc_auc_out1, roc_auc_out2, roc_auc_out3, roc_auc_out4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델  : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np_utils.to_categorical(y_test,8)\n",
    "y_train2 = np_utils.to_categorical(y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation = 'sigmoid', kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dense(64, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(32, activation = 'relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = bm.Make_Pre_Data(model2, tfidf, 500, train, test)\n",
    "modelName = bm.Return_ModelName('fastText', model2,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bm.Make_TSNE2(2, model2, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 :  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out1 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out2 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier2, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier2, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier3 =  SVC(kernel = 'rbf',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out3 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier3, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier3, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree = 0.7\n",
    "params = {\n",
    "    'objective' : 'multi:softmax', \n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : max_depth, \n",
    "    'subsample' : subsample,\n",
    "    #'eval_metric' : 'auc', \n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'silent' : 1, \n",
    "    'eta' : 0.175,\n",
    "    'nthread' : cores,\n",
    "    'num_class' : 8\n",
    "}\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist,\n",
    "                early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_acc = accuracy_score(y_test, test_prediction)\n",
    "print (test_acc)\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out4 = bm.Roc_Curve_MultiClass(xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(xgb.DMatrix(train_vecs_w2v), train_y2, xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Plot_Roc_Curver_Micro_Macro(roc_auc_out1, roc_auc_out2, roc_auc_out3, roc_auc_out4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델  : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np_utils.to_categorical(y_test,8)\n",
    "y_train2 = np_utils.to_categorical(y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation = 'sigmoid', kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dense(64, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(32, activation = 'relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = bm.Make_Pre_Data(model3, tfidf, 500, train, test)\n",
    "modelName = bm.Return_ModelName('fastText', model3,'mecab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bm.Make_TSNE2(2, model3, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 :  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out1 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out2 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier2, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier2, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier3 =  SVC(kernel = 'rbf',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out3 = bm.Roc_Curve_MultiClass(test_vecs_w2v, test_y2, classifier3, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(train_vecs_w2v, train_y2, test_vecs_w2v, test_y2, classifier3, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree = 0.7\n",
    "params = {\n",
    "    'objective' : 'multi:softmax', \n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : max_depth, \n",
    "    'subsample' : subsample,\n",
    "    #'eval_metric' : 'auc', \n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'silent' : 1, \n",
    "    'eta' : 0.175,\n",
    "    'nthread' : cores,\n",
    "    'num_class' : 8\n",
    "}\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist,\n",
    "                early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_acc = accuracy_score(y_test, test_prediction)\n",
    "print (test_acc)\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_out4 = bm.Roc_Curve_MultiClass(xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le, np.unique(train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.ConfusionMatrix_To_Heatmap(xgb.DMatrix(train_vecs_w2v), train_y2, xgb.DMatrix(test_vecs_w2v), test_y2, gbm, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Plot_Roc_Curver_Micro_Macro(roc_auc_out1, roc_auc_out2, roc_auc_out3, roc_auc_out4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델  : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = np_utils.to_categorical(y_test,8)\n",
    "y_train2 = np_utils.to_categorical(y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation = 'sigmoid', kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=500))\n",
    "model.add(Dense(64, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(32, activation = 'relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train2, epochs=300,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test2, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
