{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec model\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import html\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from collections import namedtuple\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import numpy as np\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "mecab = Mecab()\n",
    "ct = ctwitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vector size\n",
    " [ 1000, 2000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491510, 2)\n"
     ]
    }
   ],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None,encoding='utf-8')\n",
    "print (rawdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이젠 민주개혁의 길로 사설  이 90년대가 우리 모두에게 성취의 시대이기를 기원하고...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>민주당과 공화당은 새해부터 정계개편을 적극 추진한다는 방침이며 양당의 김영삼ㆍ김종필...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4당,지자제 연합공천 대비  파트너 탐색전  이같은 구상을 실현시키기 위해 민정당은...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90년대는 국내적으로 정치민주화와 선진국에로의 도약여부가 판가름날 것이고 한반도를 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>특히 노태우대통령이 3일 대국민 특별담화를 발표, 지난시대 의 완전청산을 선언한 후...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  이젠 민주개혁의 길로 사설  이 90년대가 우리 모두에게 성취의 시대이기를 기원하고...  1\n",
       "1  민주당과 공화당은 새해부터 정계개편을 적극 추진한다는 방침이며 양당의 김영삼ㆍ김종필...  1\n",
       "2  4당,지자제 연합공천 대비  파트너 탐색전  이같은 구상을 실현시키기 위해 민정당은...  1\n",
       "3  90년대는 국내적으로 정치민주화와 선진국에로의 도약여부가 판가름날 것이고 한반도를 ...  1\n",
       "4  특히 노태우대통령이 3일 대국민 특별담화를 발표, 지난시대 의 완전청산을 선언한 후...  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/491510 [00:00<?, ?it/s]\n",
      "  0%|          | 4/491510 [00:00<5:03:33, 26.99it/s]\n",
      "  0%|          | 9/491510 [00:00<3:47:53, 35.95it/s]\n",
      "  0%|          | 14/491510 [00:00<3:28:01, 39.38it/s]\n",
      "  0%|          | 20/491510 [00:00<3:06:38, 43.89it/s]\n",
      "  0%|          | 24/491510 [00:00<3:10:03, 43.10it/s]\n",
      "  0%|          | 30/491510 [00:00<3:01:01, 45.25it/s]\n",
      "  0%|          | 35/491510 [00:00<3:01:10, 45.21it/s]\n",
      "  0%|          | 40/491510 [00:00<3:06:13, 43.99it/s]\n",
      "  0%|          | 46/491510 [00:01<3:02:15, 44.94it/s]\n",
      "  0%|          | 52/491510 [00:01<2:58:06, 45.99it/s]\n",
      "  0%|          | 60/491510 [00:01<2:48:09, 48.71it/s]\n",
      "  0%|          | 70/491510 [00:01<2:36:05, 52.47it/s]\n",
      "  0%|          | 77/491510 [00:01<2:32:45, 53.62it/s]\n",
      "  0%|          | 87/491510 [00:01<2:26:25, 55.94it/s]\n",
      "  0%|          | 95/491510 [00:01<2:23:50, 56.94it/s]\n",
      "  0%|          | 103/491510 [00:01<2:22:37, 57.42it/s]\n",
      "  0%|          | 111/491510 [00:01<2:21:17, 57.97it/s]\n",
      "  0%|          | 118/491510 [00:02<2:21:14, 57.98it/s]\n",
      "  0%|          | 125/491510 [00:02<2:19:57, 58.51it/s]\n",
      "  0%|          | 134/491510 [00:02<2:16:40, 59.92it/s]\n",
      "  0%|          | 142/491510 [00:02<2:15:37, 60.39it/s]\n",
      "  0%|          | 156/491510 [00:02<2:08:48, 63.58it/s]\n",
      "  0%|          | 167/491510 [00:02<2:05:16, 65.37it/s]\n",
      "  0%|          | 177/491510 [00:02<2:03:59, 66.04it/s]\n",
      "  0%|          | 187/491510 [00:02<2:01:52, 67.19it/s]\n",
      "  0%|          | 197/491510 [00:02<2:01:05, 67.62it/s]\n",
      "  0%|          | 206/491510 [00:03<2:04:00, 66.03it/s]\n",
      "  0%|          | 214/491510 [00:03<2:04:23, 65.83it/s]\n",
      "  0%|          | 224/491510 [00:03<2:02:34, 66.80it/s]\n",
      "  0%|          | 232/491510 [00:03<2:03:13, 66.45it/s]\n",
      "  0%|          | 242/491510 [00:03<2:01:34, 67.35it/s]\n",
      "  0%|          | 250/491510 [00:03<2:02:20, 66.92it/s]\n",
      "  0%|          | 258/491510 [00:03<2:02:48, 66.67it/s]\n",
      "  0%|          | 265/491510 [00:03<2:03:05, 66.52it/s]\n",
      "  0%|          | 273/491510 [00:04<2:02:47, 66.68it/s]\n",
      "  0%|          | 280/491510 [00:04<2:03:01, 66.55it/s]\n",
      "  0%|          | 291/491510 [00:04<2:01:17, 67.49it/s]\n",
      "  0%|          | 299/491510 [00:04<2:01:12, 67.55it/s]\n",
      "  0%|          | 307/491510 [00:04<2:01:02, 67.64it/s]\n",
      "  0%|          | 315/491510 [00:04<2:02:54, 66.61it/s]\n",
      "  0%|          | 325/491510 [00:04<2:02:11, 66.99it/s]\n",
      "  0%|          | 332/491510 [00:04<2:02:52, 66.62it/s]\n",
      "  0%|          | 339/491510 [00:05<2:03:59, 66.02it/s]\n",
      "  0%|          | 345/491510 [00:05<2:04:27, 65.77it/s]\n",
      "  0%|          | 351/491510 [00:05<2:04:38, 65.67it/s]\n",
      "  0%|          | 357/491510 [00:05<2:05:43, 65.11it/s]\n",
      "  0%|          | 365/491510 [00:05<2:05:28, 65.24it/s]\n",
      "  0%|          | 371/491510 [00:05<2:05:59, 64.97it/s]\n",
      "  0%|          | 379/491510 [00:05<2:05:33, 65.19it/s]\n",
      "  0%|          | 388/491510 [00:05<2:04:50, 65.57it/s]\n",
      "  0%|          | 399/491510 [00:06<2:03:30, 66.27it/s]\n",
      "  0%|          | 410/491510 [00:06<2:02:29, 66.82it/s]\n",
      "  0%|          | 419/491510 [00:06<2:02:55, 66.58it/s]\n",
      "  0%|          | 427/491510 [00:06<2:03:25, 66.31it/s]\n",
      "  0%|          | 435/491510 [00:06<2:03:56, 66.03it/s]\n",
      "  0%|          | 442/491510 [00:06<2:04:31, 65.73it/s]\n",
      "  0%|          | 449/491510 [00:06<2:04:31, 65.72it/s]\n",
      "  0%|          | 457/491510 [00:06<2:04:10, 65.91it/s]\n",
      "  0%|          | 466/491510 [00:07<2:03:50, 66.09it/s]\n",
      "  0%|          | 475/491510 [00:07<2:03:13, 66.41it/s]\n",
      "  0%|          | 486/491510 [00:07<2:02:19, 66.90it/s]\n",
      "  0%|          | 495/491510 [00:07<2:01:49, 67.18it/s]\n",
      "  0%|          | 505/491510 [00:07<2:01:10, 67.53it/s]\n",
      "  0%|          | 515/491510 [00:07<2:00:27, 67.93it/s]\n",
      "  0%|          | 527/491510 [00:07<1:59:26, 68.51it/s]\n",
      "  0%|          | 537/491510 [00:07<1:59:17, 68.60it/s]\n",
      "  0%|          | 548/491510 [00:07<1:58:29, 69.06it/s]\n",
      "  0%|          | 558/491510 [00:08<1:58:17, 69.17it/s]\n",
      "  0%|          | 567/491510 [00:08<1:57:57, 69.37it/s]\n",
      "  0%|          | 576/491510 [00:08<1:57:44, 69.50it/s]\n",
      "  0%|          | 592/491510 [00:08<1:56:10, 70.43it/s]\n",
      "  0%|          | 604/491510 [00:08<1:55:20, 70.94it/s]\n",
      "  0%|          | 617/491510 [00:08<1:54:17, 71.59it/s]\n",
      "  0%|          | 628/491510 [00:08<1:53:34, 72.04it/s]\n",
      "  0%|          | 640/491510 [00:08<1:52:54, 72.46it/s]\n",
      "  0%|          | 651/491510 [00:08<1:52:20, 72.83it/s]\n",
      "  0%|          | 662/491510 [00:09<1:51:45, 73.20it/s]\n",
      "  0%|          | 673/491510 [00:09<1:51:21, 73.46it/s]\n",
      "  0%|          | 685/491510 [00:09<1:50:41, 73.90it/s]\n",
      "  0%|          | 696/491510 [00:09<1:50:17, 74.17it/s]\n",
      "  0%|          | 707/491510 [00:09<1:50:08, 74.27it/s]\n",
      "  0%|          | 717/491510 [00:09<1:50:09, 74.25it/s]\n",
      "  0%|          | 726/491510 [00:09<1:49:58, 74.38it/s]\n",
      "  1%|          | 5874/491510 [01:20<1:50:18, 73.37it/s]\n",
      "  0%|          | 747/491510 [00:09<1:49:25, 74.75it/s]Exception in thread Thread-32:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\pc\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "  0%|          | 757/491510 [00:10<1:50:03, 74.32it/s]\n",
      "100%|██████████| 491510/491510 [1:01:09<00:00, 133.94it/s]\n"
     ]
    }
   ],
   "source": [
    "d2v_docs = list()\n",
    "for idx in tqdm(rawdata.index):\n",
    "    text = rawdata.loc[idx,0]\n",
    "    pos = mecab.pos(text)\n",
    "    pos = [x[0] for x in pos ]#if (x[1] == u'VV' or x[1] == u'VA' or x[1] == u'NNB' or x[1] == u'NNP' or x[1] == u'NNG')]\n",
    "    pos = [x for x in pos if not x in stopwords]\n",
    "    label = rawdata.loc[idx,1]\n",
    "    d2v_docs.append(TaggedDocument(pos, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(d2v_docs, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(d2v_docs,open('./data/pre_data/tagged_data/pre_data_d2v_docs_for_word2vec_sentiment_by_mecab.pickled','wb'))\n",
    "pickle.dump(train,open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_mecab.pickled','wb'))\n",
    "pickle.dump(test,open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1314565.40it/s]\n",
      "100%|██████████| 442359/442359 [00:00<00:00, 1330404.04it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1291191.62it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1291507.09it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = [ x.words for x in tqdm(train)] \n",
    "y_train = [ x.tags for x in tqdm(train)] \n",
    "x_test = [ x.words for x in tqdm(test)] \n",
    "y_test = [ x.tags for x in tqdm(test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:12<00:00, 35420.45it/s]\n",
      "100%|██████████| 442359/442359 [20:31<00:00, 359.16it/s]\n"
     ]
    }
   ],
   "source": [
    "w2v_model = word2vec.Word2Vec(sg=1, size=3000,window=8, workers = 6, iter = 20)\n",
    "w2v_model.build_vocab(tqdm(x_train))\n",
    "w2v_model.train(tqdm(x_train), total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(w2v_model, open('./data/pre_data/pre_data_w2v_model_for_word2vec_sentiment_by_mecab.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing bokeh library for interactive dataviz\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "# defining the chart\n",
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of word vectors\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "# getting a list of word vectors. limit to 10000. each is of 200 dimensions\n",
    "word_vectors = [w2v_model[w] for w in list(w2v_model.wv.vocab.keys())[:10000]]\n",
    "\n",
    "# dimensionality reduction. converting the vectors to 2d vectors\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "# putting everything in a dataframe\n",
    "tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "tsne_df['words'] = list(w2v_model.wv.vocab.keys())[:10000]\n",
    "\n",
    "# plotting. the corresponding word appears when you hover on the data point.\n",
    "plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"word\": \"@words\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print ('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform(x_train)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 3000) for z in tqdm(map(lambda x: x, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 3000) for z in tqdm(map(lambda x: x, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Embedding, embeddings, merge\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=3000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, np.array(y_train), epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_vecs_w2v, np.array(y_test), verbose=2)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
