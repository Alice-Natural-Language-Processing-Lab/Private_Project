{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Twitter, Mecab,Kkma\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chat_bot as cb\n",
    "import Database_Handler as dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 1464079, 5532]\n",
      "[('이', 'Noun'), ('것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n",
      "best: [('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "mecab = Mecab()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "print (list(map(lambda x: len(xxxx[x]), xxxx)))\n",
    "\n",
    "score_weights = {\n",
    "    'num_nouns': -0.1,\n",
    "    'num_words': -0.2,\n",
    "    'no_noun': -1\n",
    "}\n",
    "\n",
    "def my_score(candidate):\n",
    "    num_nouns = len([w for w,t in candidate if t == 'Noun'])\n",
    "    num_words = len(candidate)\n",
    "    no_noun = 1 if num_nouns == 0 else 0\n",
    "    \n",
    "    score = (num_nouns * score_weights['num_nouns'] \n",
    "             + num_words * score_weights['num_words']\n",
    "             + no_noun * score_weights['no_noun'])\n",
    "    return score\n",
    "\n",
    "ct.set_selector(score_weights, my_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언론사 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> stopwords를 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressList = pd.read_excel('./data/presslist.xlsx')\n",
    "\n",
    "with(open('./data/newspress.txt','w',encoding='utf-8')) as f:\n",
    "    f.write('\\n'.join(set(pressList['언론사'].tolist())))\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwordList = Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords =['이제','너무','중인' ,'만큼', '마찬가지', '꼬집었', '기자','습니다','어요','이렇게', \"아\", \"휴\", \n",
    "         \"아이구\", \"아이쿠\", \"아이고\",'라고','지난','당시','오전','오후',\"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\",'데일리',\n",
    "         '이날','갑자기','같다','는데','다른','에서','하는','하게','이후','통해','이날','정말','다시','스스로','같은','지난주',\n",
    "         '이번주','다음주','그런','이런','첫째','둘째','셋째','넷째','다섯째','여섯째','일곱째','여덟째','아홉째','열째','기성',\n",
    "             '이었','라는','저번','이번','면서','이렇게','저렇게','그렇게','정근','판청','아르테',\n",
    "            '앵커','아나운서','좌윤','주고', 'B씨','A씨','경우','서로','대로','지사','인근','스완','위해','대한',\n",
    "            '통한','기성','영제','간다','대해','수준','젊은','스탠','이씨','헤아','하라','패스트','좌안','이토','2TV',\n",
    "            '성은','산다','나요','선우','누나','억원','태수','그냥','사이','어마','위엄','남자','로서','우리','누리','관련',\n",
    "            '윤종','다친','승헌','골망','있는','까지','반종빈','녹이','현재','세이','카스티','스티','물론','라덴','상대로',\n",
    "            '여러','아미','아사다','이닝','유기','신서','티브이데일리','공동','윤이','치원','타고','박씨','루수','유력','타고',\n",
    "            '하고','처음','김씨','연재','사람','다이어','이다','인근','니다','아직','이씨','탈린','윤계','이날',\n",
    "            '이것','저것','그것','신잡','장영','고씨','박씨','사실','경정','손새','장의','미도','말했',\n",
    "            '빈살','실제','분들','건너','최근','조씨','장씨','올해','내년','작년','지난해','여서','A군','B군',\n",
    "            '고든','램지','진혁','심지어','D씨','최근','니다','협회','그리즈','미오','그것','메이','이재','윈터',\n",
    "            '임오','레이먼','사는','잡고','이야','치치','있다','1호','2호','3호','4호','5호','6호','7호','8호','9호',\n",
    "            '다양한','부분','심씨','트루시','트루','심씨','부분','전체','주씨','펀치','원투','A사','1차','2차','3차','4차',\n",
    "            '5차','6차','7차','8차','9차','10차','계속','MK','매경닷컴','MK스포츠','티노','처음','모현','들이','있는',\n",
    "            '더욱','차장','부장','시점','000','달라','모든','구름','맑음','방향','같아','믹스','나인','윤균','모두',\n",
    "            '라며','헤아','제트','라카','소속','1분','신잡','울스','일단','번째','어서','이지','원장','교수','선수',\n",
    "            '안젤리나','브래드','드려','조원','둥지','빅리그','어떻','대답했','루프','트리플','빅토르','지금','때문','해성',\n",
    "             '빠졌','그리팅','5일','점프','그룹','시즌','크게','없는']\n",
    "swords2 = ['중인' ,'만큼', '마찬가지', '꼬집었', \"기자\",\"아\", \"휴\", \"아이구\",\n",
    "                      \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"가\"] + pressList['언론사'].tolist() + [\"저\", \"것\",\n",
    "    \"동시에\", \"몇\", \"고려하면\", \"관련이\", \"놀라다\", \"무엇\", \"어느쪽\", \"오\", \"정도의\", \"더구나\", \"아무도\", \"줄은모른다\", \"참\", \"아니\",\n",
    "         \"휘익\", \"향하다\", \"응당\", \"알겠는가\", \"인젠\", \"그래서\", \"자신\", \"해서는\", \"둘\", \"이었다\", \"임에\", \"하도록시키다\", \"누구\", \"이때\",\n",
    "         \"삼\", \"제외하고\", \"쿵\", \"하면\", \"좀\", \"그렇지않으면\", \"아니었다면\", \"이라면\", \"팍\", \"일\", \"통하여\", \"무엇때문에\", \n",
    "         \"보아\", \"하게하다\", \"하는\", \"이르다\", \"타다\", \"까지도\", \"오직\", \"도달하다\", \"잠깐\", \"외에\", \"심지어\", \"하려고하다\", \"게다가\",\n",
    "         \"후\", \"알\", \"비하면\", \"헉헉\", \"근거로\", \"월\", \"따라서\", \"않는다면\", \"일지라도\", \"함께\", \"이유는\", \"흥\", \"혼자\", \"관하여\", \n",
    "         \"붕붕\", \"하다\", \"진짜로\", \"의해\", \"바와같이\", \"대하면\", \"퍽\", \"보다더\", \"그렇게\", \"끼익\", \"댕그\", \"시초에\", \"당장\", \n",
    "         \"하는것만\", \"누가\", \"만이\", \"만일\", \"이지만\", \"하마터면\", \"꽈당\", \"만은\", \"우선\", \"없다\", \"휴\", \"하도록하다\", \"그런데\",\n",
    "         \"비로소\", \"하게될것이다\", \"만큼 어찌됏든\", \"오히려\", \"을\", \"더라도\", \"안\", \"왜냐하면\", \"습니다\", \"줄은\", \"그리하여\", \"하\",\n",
    "         \"어떻게\", \"대로\", \"기대여\", \"끙끙\", \"예를\", \"와르르\", \"이리하여\", \"이\", \"조차\", \"하고\", \"이젠\", \"뒤이어\", \"할줄알다\", \"반대로\",\n",
    "         \"시각\", \"펄렁\", \"잇따라\", \"공동으로\", \"비록\", \"가까스로\", \"여덟\", \"비슷하다\", \"이상\", \"차라리\", \"이어서\", \"모두\", \"툭\", \"조차도\",\n",
    "         \"헉\", \"부터\", \"혹시\", \"않고\", \"우리\", \"삐걱\", \"여보시오\", \"허\", \"해요\", \"견지에서\", \"하기는한데\", \"토하다\", \"않으면\", \"이봐\", \"관계가\", \n",
    "         \"한다면\", \"시작하여\", \"연이서\", \"이외에도\", \"그\", \"운운\", \"에게\", \"그럼에도\", \"예\", \"만약에\", \"했어요\", \"결과에\", \"제\", \"오자마자\",\n",
    "         \"것들\", \"약간\", \"것과\", \"일때\", \"셋\", \"각종\", \"아이구\", \"같은\", \"향해서\", \"일것이다\", \"해야한다\", \"아이야\", \"로\", \"편이\", \"등등\", \n",
    "         \"해도좋다\", \"하기에\", \"김에\", \"몰랏다\", \"같이\", \"하도다\", \"즉시\", \"갖고말하자면\", \"우에\", \"어느\", \"허허\", \"하자마자\", \"에서\", \"그래도\",\n",
    "         \"하여야\", \"된이상\", \"까악\", \"한켠으로는\", \"많은\", \"그중에서\", \"사\", \"낼\", \"뿐만\", \"저쪽\", \"어쩔수\", \"어떤것들\", \"물론\", \"결론을\", \"이만큼\",\n",
    "         \"이렇게되면\", \"소인\", \"바꾸어말하면\", \"들\", \"이렇구나\", \"하물며\", \"얼마간\", \"얼마든지\", \"한항목\", \"하는것도\", \"졸졸\", \"한마디\", \"말할것도\", \n",
    "         \"만약\", \"남들\", \"총적으로\", \"허걱\", \"그리고\", \"따지지\", \"구체적으로\", \"못하다    하기보다는\", \"언제\", \"따르는\", \"구토하다\", \"앞에서\",\n",
    "         \"대해서\", \"아\", \"앞의것\", \"비걱거리다\", \"헐떡헐떡\", \"어찌하든지\", \"입장에서\", \"의\", \"마저\", \"바로\", \"하기만\", \"않기\", \"또한\", \"쓰여\",\n",
    "         \"위해서\", \"의거하여\", \"인\", \"아니면\", \"를\", \"사람들\", \"할수있다\", \"일곱\", \"근거하여\", \"한적이있다\", \"함으로써\", \"낫다\", \"어떤것\",\n",
    "         \"방면으로\", \"중의하나\", \"어\", \"무릎쓰고\", \"저것만큼\", \"서술한바와같이\", \"그런즉\", \"들자면\", \"하지\", \"아이고\", \"불문하고\", \"만\", \n",
    "         \"마저도\", \"얼마만큼\", \"예컨대\", \"이렇게말하자면\", \"연관되다\", \"않다면\", \"들면\", \"이쪽\", \"의지하여\", \"여섯\", \"그저\", \"아니다\", \n",
    "         \"그렇지만\", \"기준으로\", \"되어\", \"가\", \"무렵\", \"즉\", \"말하면\", \"어찌\", \"그럼\", \"그위에\", \"그런\", \"조금\", \"매번\", \"혹은\",\n",
    "         \"이천구\", \"중에서\", \"따름이다\", \"하기\", \"가령\", \"잠시\", \"아무거나\", \"하기보다는\", \"주저하지\", \"당신\", \"봐라\", \"그렇지\",\n",
    "         \"자기집\", \"할지라도\", \"요만한걸\", \"우르르\", \"못하다\", \"왜\", \"이렇게\", \"퉤\", \"관계없이\", \"그래\", \"대해\", \"쪽으로\", \n",
    "         \"저것\", \"자기\", \"아홉\", \"지만\", \"구\", \"하지마\", \"따위\", \"하지만\", \"나\", \"해도\", \"전자\", \"그만이다\", \"안된다\", \"까닭으로\", \n",
    "         \"되다\", \"오르다\", \"딱\", \"다음에\", \"너희들\", \"점에서\", \"아이쿠\", \"쾅쾅\", \"종합한것과같이\", \"할수있어\", \"그치지\", \"비교적\",\n",
    "         \"륙\", \"되는\", \"개의치않고\", \"엉엉\", \"하든지\", \"때가\", \"영차\", \"바꿔\", \"더불어\", \"주룩주룩\", \"따라\", \"이용하여\", \"우리들\",\n",
    "         \"여기\", \"더욱이는\", \"하더라도\", \"입각하여\", \"여러분\", \"마치\", \"하느니\", \"너\", \"어디\", \"제각기\", \"밖에\", \"봐\", \"위하여\", \n",
    "         \"팔\", \"요만큼\", \"가서\", \"아니라면\", \"지든지\", \"참나\", \"할만하다\", \"타인\", \"든간에\", \"하겠는가\", \"거바\", \"겨우\", \"다음\", \n",
    "         \"이러한\", \"이럴정도로\", \"각자\", \"어때\", \"지말고\", \"형식으로\", \"그러한즉\", \"아니나다를가\", \"할\", \"불구하고\", \"지경이다\",\n",
    "    \"어떠한\", \"기점으로\", \"할때\", \"등\", \"다시\", \"시키다\", \"답다\", \"소생\", \"라\", \"로써\", \"각\", \"부류의\", \"알았어\", \"훨씬\", \"위에서\",\n",
    "    \"뿐이다\", \"시간\", \"그러나\", \"하곤하였다\", \"일단\", \"막론하고\", \"좋아\", \"솨\", \"이곳\", \"뿐만아니라\", \"아울러\", \"옆사람\", \"다수\", \n",
    "    \"예하면\", \"령\", \"어떤\", \"어떻해\", \"할수록\", \"말하자면\", \"전후\", \"메쓰겁다\", \"에\", \"으로써\", \"이번\", \"하면된다\", \"이것\", \"딩동\",\n",
    "\"양자\", \"달려\", \"본대로\", \"탕탕\", \"마음대로\", \"쉿\", \"미치다\", \"다시말하면\", \"동안\", \"그러니까\", \"과연\", \"뚝뚝\", \"거의\", \"이천팔\",\n",
    "\"이로\", \"않도록\", \"또\", \"한하다\", \"아래윗\", \"수\", \"다소\", \"어느것\", \"까지\", \"남짓\", \"저기\", \"관한\", \"무슨\", \"그에\", \"년도\", \n",
    "\"삐걱거리다\", \"이러이러하다\", \"와\", \"넷\", \"쳇\", \"논하지\", \"습니까\", \"이천육\", \"기타\", \"오로지\", \"어느곳\", \"설령\", \"할지언정\", \"칠\",\n",
    "\"다만\", \"반드시\", \"한데\", \"곧\", \"의해서\", \"얼마나\", \"아니라\", \"상대적으로\", \"너희\", \"있다\", \"인하여\", \"다섯\", \"생각이다\", \"몰라도\", \n",
    "\"정도에\", \"버금\", \"까닭에\", \"얼마큼\", \"전부\", \"로부터\", \"힘입어\", \"틈타\", \"해도된다\", \"나머지는\", \"흐흐\", \"그때\", \"하여금\", \"모\", \n",
    "\"이런\", \"바꾸어서\", \"비추어\", \"각각\", \"설사\", \"이래\", \"비길수\", \"하지마라\", \"응\", \"다른\", \"듯하다\", \"보는데서\", \"어쨋든\", \"대하여\", \n",
    "\"좍좍\", \"으로\", \"여차\", \"틀림없다\", \"과\", \"고로\", \"요컨대\", \"일반적으로\", \"줄\", \"하는바\", \"그들\", \"요만한\", \"윙윙\", \"콸콸\", \"어기여차\",\n",
    "\"언젠가\", \"이와\", \"할망정\", \"이천칠\", \"네\", \"없고\", \"둥둥\", \"겸사겸사\", \"그러므로\", \"안다\", \"거니와\", \"년\", \"여부\", \"때문에\", \"된바에야\",\n",
    "\"향하여\", \"때\", \"하하\", \"및\", \"오호\", \"하면서\", \"더군다나\", \"한\", \"이유만으로\", \"어이\", \"하나\", \"저희\", \"더욱더\", \"두번째로\", \"바꾸어말하자면\",\n",
    "\"이와같다면\", \"이르기까지\", \"단지\", \"그러면\", \"야\", \"결국\", \"영\",\"뒤따라\", \"즈음하여\", \"도착하다\", \"와아\", \"다음으로\", \"같다\", \"자\",\n",
    "\"아하\", \"생각한대로\", \"외에도\", \"의해되다\", \"설마\", \"으로서\", \"보면\", \"할뿐\", \"첫번째로\", \"아야\", \"어째서\", \"하는것이\", \"하구나\", \"않다\",\n",
    "\"힘이\", \"육\", \"그러니\", \"여전히\", \"어찌됏어\", \"어찌하여\", \"어느해\", \"앗\", \"게우다\",\"보드득\", \"관해서는\", \"자마자\", \"매\", \"하고있었다\", \n",
    "\"어느때\", \"여\", \"실로\", \"해봐요\", \"얼마\", \"아이\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.twitter = ot\n",
    "        self.ctwitter = ct\n",
    "        self.mecab = mecab\n",
    "        \n",
    "        #self.stopwords = []\n",
    "        self.stopwords = swords +swords2 + stopwordList\n",
    "    def text2sentences(self, text):\n",
    "        sentences = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                #nouns.append(' '.join([noun for noun in self.ctwitter.phrases(str(sentence))\n",
    "                #                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                #nouns.append(' '.join([noun for noun in self.ctwitter.nouns(str(sentence))\n",
    "                #                                  if noun not in self.stopwords and len(noun) > 1]))\n",
    "                #nouns.append(' '.join([noun for noun in self.ctwitter.morphs(str(sentence))\n",
    "                #                                   if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.mecab.nouns(str(sentence))\n",
    "                                                  if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.mecab.morphs(str(sentence))\n",
    "                                                   if noun not in self.stopwords and len(noun) > 1]))\n",
    "        nouns = list(set(nouns))\n",
    "        return nouns\n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(ngram_range=(1,4),sublinear_tf=True, lowercase=False, min_df=0.0001)\n",
    "        self.cnt_vec = CountVectorizer(ngram_range=(1,4), lowercase=False,min_df=0.0001,)\n",
    "        self.graph_sentence = []\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        #tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = sknorm(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "class TextRankX(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/Keyword_outcome/mecab-naver.*'\n",
    "extracted_file = glob(path)\n",
    "extracted = dict()\n",
    "for file in extracted_file:\n",
    "    ex = pickle.load(open(file,'rb'))\n",
    "    extracted.update(ex)\n",
    "outlen = str(len(extracted_file))\n",
    "\n",
    "outname = './data/Keyword_outcome/mecab-naver.pickled_'+outlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://sports.news.naver.com/sports/index.nhn?ctg=ranking_news&mod=read&ranking_type=popular_day&date=20171212&office_id=410&article_id=0000425760\n",
      "[윈터미팅 1일차] 모리스의 눈물, 스탠튼의 뒤끝\n",
      "['마이애미 말린스', '지안카를로 스탠튼']\n",
      "['말린스', '스탠튼', '계약', '윈터미팅', '명예 전당', '전당', '명예', '이적', '양키스', '생각']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_keywords_dict = dict()\n",
    "site = 'Naver'\n",
    "#site = 'daum'\n",
    "if site == 'daum':\n",
    "    binList = []\n",
    "    collection = 'newsDaum'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/daum2/'\n",
    "elif site.lower() == 'naver':\n",
    "    binList = []\n",
    "    collection = 'newsNaver'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/naver2/'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, collection)\n",
    "keywordList = list()\n",
    "for data in useCollection.find({'site':site}):\n",
    "    idIs = data['_id']._ObjectId__id\n",
    "    tis2 = html.unescape(data['title'])+'. '+html.unescape(data['mainText'])\n",
    "    if data['keywords'] !='NaN':\n",
    "        keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "    else:\n",
    "        pass\n",
    "    if not idIs.hex() in extracted:\n",
    "        print (data['link'])\n",
    "        print (data['title'])\n",
    "        print (data['keywords'])\n",
    "        try:\n",
    "            workrank = TextRankX(tis2)\n",
    "        except:\n",
    "            try:\n",
    "                workrank = TextRankX(tis2)\n",
    "            except:\n",
    "                workrank = TextRankX(tis2)\n",
    "                break\n",
    "        extract_keywords_dict[idIs.hex()]= workrank.keywords()\n",
    "        print (workrank.keywords())\n",
    "        print ()\n",
    "pickle.dump(extract_keywords_dict, open(outname,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 형태소 분석 : morp\n",
    "* 어휘의미 분석 : wsd (동음이의어)\n",
    "* 어휘의미 분석 : wsd_poly (다의어)\n",
    "* 개체명 인식 : ner\n",
    "* 의존구문 분석 : dependency\n",
    "* 의미역 인식 : srl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
