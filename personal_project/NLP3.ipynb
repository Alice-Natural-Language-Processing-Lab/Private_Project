{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 빅데이터 분석 데이터 추출 & 사용자 사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath = 'C:/Users/pc/Anaconda3/lib/site-packages/ckonlpy/data/twitter/noun/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './data/nnews/high_frequency_noun/'\n",
    "path2 = './data/news/mainissue/'\n",
    "path3 = './data/news/newstopic/'\n",
    "path4 = './data/news/people/'\n",
    "path5 = './data/news/4th_industry/'\n",
    "path6 = './data/news/have_negative_positive/constitution/'\n",
    "path7 = './data/news/have_negative_positive/household_debt/'\n",
    "path8 = './data/news/have_negative_positive/olymphic/'\n",
    "path9 = './data/news/have_negative_positive/'\n",
    "path10 = './data/news/'\n",
    "\n",
    "pathlist = [path1, path2, path3,\n",
    "            path4, path5, path6,\n",
    "            path7, path8, path9,\n",
    "           path10]\n",
    "if not os.path.isfile(dbpath+'from_news.txt'):\n",
    "    with open(dbpath+'from_news.txt','w') as f:\n",
    "        for path in pathlist:\n",
    "            out = Extract_Keyword_from_news_bigdata(path)\n",
    "            f.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Mecab, Twitter\n",
    "from konlpy.utils import pprint\n",
    "import os\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwords = Stopwords('./data/koreanStopwords.txt') +Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512, 5068471, 5532]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "list(map(lambda x: len(xxxx[x]), xxxx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "sys.path.append('/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/')\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import time, re, pickle, itertools\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from multiprocessing import Pool\n",
    "import urllib3, json\n",
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import os\n",
    "\n",
    "def OpenAPI():\n",
    "    import pickle\n",
    "    openapi = pickle.load(open('./etri_open_api_access_key', 'rb'))\n",
    "    return openapi\n",
    "def USE_ETRI_ANALYSIS(analysisCode, text):    \n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    requestJson = {\n",
    "        'access_key' : accessKey,\n",
    "        'argument' : {\n",
    "            'text' : text,\n",
    "            'analysis_code' : analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "    return json.loads(str(response.data,'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_INFO(fWord, sWord):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/WordRel\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    firstWord = fWord\n",
    "    #firstSenseId = fSenseId\n",
    "    secondWord = sWord\n",
    "    #secondSenseId = sSenseId\n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "        'first_word': firstWord,\n",
    "        #'first_sense_id': firstSenseId,\n",
    "        'second_word': secondWord,\n",
    "        #'second_sense_id': secondSenseId\n",
    "        }}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "    body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_RELATION(word):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/Word\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    word = word\n",
    "    requestJson = {\n",
    "    \"access_key\": accessKey,\n",
    "    \"argument\": {\"word\": word}}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "                            body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def Extract_Text_Info(idx):\n",
    "    morp = list(map(lambda x: (x['lemma'], x['type']),idx['morp']))\n",
    "    wsd = list(map(lambda x: (x['text'], x['type']),idx['WSD']))\n",
    "    word = list(map(lambda x: (x['text'], x['type']),idx['word']))\n",
    "    ne = list(map(lambda x: (x['text'], x['type']),idx['NE']))\n",
    "    return morp, wsd, word, ne\n",
    "\n",
    "def Run_ETRI_Analysis(stopwordsList, text):\n",
    "    etri = USE_ETRI_ANALYSIS('srl', text)['return_object']    \n",
    "    etri = etri['sentence']\n",
    "    morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "    for i in etri:\n",
    "        x = Extract_Text_Info(i)\n",
    "        morp += x[0]\n",
    "        wsd += x[1]\n",
    "        word +=x[2]\n",
    "        ne += x[3]\n",
    "        for ii in i['SRL']:\n",
    "            srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "        dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "    return morp, wsd, word, ne, srl, dependency, etri\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    collection = 'newsDaum'\n",
    "    etri_outcome = './data/ETRI_OUTCOME/daum2/'\n",
    "elif site.lower() == 'naver':\n",
    "    collection = 'newsNaver_copy'\n",
    "    etri_outcome = './data/ETRI_OUTCOME/naver2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywordList = list()\n",
    "pressList = list()\n",
    "for data in useCollection.find({'site':site}):\n",
    "    pressList.append(data['press'])\n",
    "    \n",
    "    if len(data['mainText'].strip()) !=0:\n",
    "        if site=='daum':\n",
    "            if data['keywords'] !='NaN':\n",
    "                keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "        else:\n",
    "            pass\n",
    "        idIs = data['_id']._ObjectId__id\n",
    "        if not os.path.isfile(etri_outcome+idIs.hex()+'.picked.txt'):\n",
    "            #print (idIs.hex())\n",
    "            try:\n",
    "                etri = USE_ETRI_ANALYSIS('srl', data['mainText'])\n",
    "            except:\n",
    "                print (etri[1])\n",
    "                break\n",
    "            else:\n",
    "                pickle.dump(etri[0], open(etri_outcome+idIs.hex()+'.picked.txt','wb'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pickle.load(open(etri_outcome+idIs.hex()+'.picked.txt','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이날은 특집 명에 맞게 녹화 전까지 철저히 출연자들이 서로 이름을 공유하지 못하도록 조치를 취했다.\n",
      "\n",
      "출연자들은 서로의 이름을 추측하며 자신의 이름보다 특이 한 지 궁금해 했다.\n",
      "\n",
      "이날 8명의 출연자는 모두 \"출석 부르는 시간, 자기소개 시간을 가장 싫어한다\"며 다른 이름이지만 같은 심정으로 공감 대화를 나눴다.\n",
      "\n",
      "keywords : ['이름', '한명회', '출연자', '방송', '이날', '시간', '이름을']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.twitter = ct\n",
    "        #self.twitter = ot\n",
    "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "        ,\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]    \n",
    "    def text2sentences(self, text):\n",
    "        sentences = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.phrases(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = sknorm(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.82): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=7):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords\n",
    "textrank = TextRank(data['mainText'])\n",
    "for row in textrank.summarize(3):\n",
    "    print(row)\n",
    "    print()\n",
    "print('keywords :',textrank.keywords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.hangle import normalize\n",
    "def RunWordKRwordRank(text, min_count):\n",
    "    #min_count = 2   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "    max_length = 10 # 단어의 최대 길이\n",
    "    wordrank_extractor = KRWordRank(min_count, max_length)\n",
    "    beta = 0.85   # PageRank의 decaying factor beta\n",
    "    max_iter = 100\n",
    "    verbose = False\n",
    "    textIs = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "    textIs = list(map(lambda x: normalize(x, english = True, number = True), textIs))\n",
    "    keywords, rank, graph = wordrank_extractor.extract(textIs, beta, max_iter, verbose)\n",
    "    return keywords, rank, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "iter = 1\r",
      "iter = 2\r",
      "iter = 3\r",
      "iter = 4\r",
      "iter = 5\r",
      "iter = 6\r",
      "iter = 7\r",
      "iter = 8\r",
      "iter = 9\r",
      "iter = 10\r",
      "iter = 11\r",
      "iter = 12\r",
      "iter = 13\r",
      "iter = 14\r",
      "iter = 15\r",
      "iter = 16\r",
      "iter = 17\r",
      "iter = 18\r",
      "iter = 19\r",
      "iter = 20\r",
      "iter = 21\r",
      "iter = 22\r",
      "iter = 23\r",
      "iter = 24\r",
      "iter = 25\r",
      "iter = 26\r",
      "iter = 27\r",
      "iter = 28\r",
      "iter = 29\r",
      "iter = 30\r",
      "iter = 31\r",
      "iter = 32\r",
      "iter = 33\r",
      "iter = 34\r",
      "iter = 35\r",
      "iter = 36\r",
      "iter = 37\r",
      "iter = 38\r",
      "iter = 39\r",
      "iter = 40\r",
      "iter = 41\r",
      "iter = 42\r",
      "iter = 43\r",
      "iter = 44\r",
      "iter = 45\r",
      "iter = 46\r",
      "iter = 47\r",
      "iter = 48\r",
      "iter = 49\r",
      "iter = 50\r",
      "iter = 51\r",
      "iter = 52\r",
      "iter = 53\r",
      "iter = 54\r",
      "iter = 55\r",
      "iter = 56\r",
      "iter = 57\r",
      "iter = 58\r",
      "iter = 59\r",
      "iter = 60\r",
      "iter = 61\r",
      "iter = 62\r",
      "iter = 63\r",
      "iter = 64\r",
      "iter = 65\r",
      "iter = 66\r",
      "iter = 67\r",
      "iter = 68\r",
      "iter = 69\r",
      "iter = 70\r",
      "iter = 71\r",
      "iter = 72\r",
      "iter = 73\r",
      "iter = 74\r",
      "iter = 75\r",
      "iter = 76\r",
      "iter = 77\r",
      "iter = 78\r",
      "iter = 79\r",
      "iter = 80\r",
      "iter = 81\r",
      "iter = 82\r",
      "iter = 83\r",
      "iter = 84\r",
      "iter = 85\r",
      "iter = 86\r",
      "iter = 87\r",
      "iter = 88\r",
      "iter = 89\r",
      "iter = 90\r",
      "iter = 91\r",
      "iter = 92\r",
      "iter = 93\r",
      "iter = 94\r",
      "iter = 95\r",
      "iter = 96\r",
      "iter = 97\r",
      "iter = 98\r",
      "iter = 99\r",
      "iter = 100\r",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'방송': 1.360025084375334,\n",
       " '시간': 1.3916619125260357,\n",
       " '이름': 1.6555387386756775,\n",
       " '출연자': 0.816209243859062,\n",
       " '한명회': 1.9429708517486093}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workrank1 = RunWordKRwordRank(data['mainText'], 3)\n",
    "workrank1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexrankr import LexRank\n",
    "lexrank = LexRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank.summarize(data['mainText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"영상 바로보기 [마이데일리 = 이승길 기자] JTBC '내 이름을 불러줘-한명(名)회'(이하 '한명회')가 막을 내린다\",\n",
       " '이날은 특집 명에 맞게 녹화 전까지 철저히 출연자들이 서로 이름을 공유하지 못하도록 조치를 취했다']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexrank.probe(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"영상 바로보기 [마이데일리 = 이승길 기자] JTBC '내 이름을 불러줘-한명(名)회'(이하 '한명회')가 막을 내린다\",\n",
       " '이날은 특집 명에 맞게 녹화 전까지 철저히 출연자들이 서로 이름을 공유하지 못하도록 조치를 취했다',\n",
       " '그중 유독 눈에 띄는 한 남성 출연자의 기상천외한 이름이 밝혀져, 나머지 출연진은 물론 MC들도 당황하며 이름을 재차 물어 웃음을 자아냈다']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = lexrank.probe(k=3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'.\\n'.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영상', 'Noun'),\n",
       " ('바로', 'Noun'),\n",
       " ('보기', 'Noun'),\n",
       " ('[', 'Punctuation'),\n",
       " ('마이데일리', 'Noun'),\n",
       " ('=', 'Punctuation'),\n",
       " ('이승', 'Noun'),\n",
       " ('길', 'Noun'),\n",
       " ('기자', 'Noun'),\n",
       " (']', 'Punctuation'),\n",
       " ('JTBC', 'Alpha'),\n",
       " (\"'\", 'Punctuation'),\n",
       " ('내', 'Noun'),\n",
       " ('이름', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('불러', 'Verb'),\n",
       " ('줘', 'Eomi'),\n",
       " ('-', 'Punctuation'),\n",
       " ('한', 'Determiner'),\n",
       " ('명', 'Noun'),\n",
       " ('(', 'Punctuation'),\n",
       " ('名', 'Foreign'),\n",
       " (')', 'Punctuation'),\n",
       " ('회', 'Noun'),\n",
       " (\"'(\", 'Punctuation'),\n",
       " ('이하', 'Noun'),\n",
       " (\"'\", 'Punctuation'),\n",
       " ('한명회', 'Noun'),\n",
       " (\"')\", 'Punctuation'),\n",
       " ('가', 'Verb'),\n",
       " ('막', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('내', 'Determiner'),\n",
       " ('린다', 'Noun'),\n",
       " ('.', 'Punctuation'),\n",
       " ('이', 'Determiner'),\n",
       " ('날', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('특집', 'Noun'),\n",
       " ('명', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('맞게', 'Verb'),\n",
       " ('녹화', 'Noun'),\n",
       " ('전', 'Noun'),\n",
       " ('까지', 'Josa'),\n",
       " ('철저히', 'Adjective'),\n",
       " ('출연자', 'Noun'),\n",
       " ('들', 'Suffix'),\n",
       " ('이', 'Josa'),\n",
       " ('서로', 'Noun'),\n",
       " ('이름', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('공유', 'Noun'),\n",
       " ('하지', 'Verb'),\n",
       " ('못', 'VerbPrefix'),\n",
       " ('하도', 'Verb'),\n",
       " ('록', 'Eomi'),\n",
       " ('조치', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('취했', 'Verb'),\n",
       " ('다', 'Eomi'),\n",
       " ('.', 'Punctuation'),\n",
       " ('그중', 'Adverb'),\n",
       " ('유독', 'Noun'),\n",
       " ('눈', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('띄는', 'Verb'),\n",
       " ('한', 'Verb'),\n",
       " ('남성', 'Noun'),\n",
       " ('출연자', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('기상', 'Noun'),\n",
       " ('천', 'Suffix'),\n",
       " ('외한', 'Noun'),\n",
       " ('이름', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('밝혀져', 'Verb'),\n",
       " (',', 'Punctuation'),\n",
       " ('나머지', 'Noun'),\n",
       " ('출연', 'Noun'),\n",
       " ('진', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('물론', 'Noun'),\n",
       " ('MC', 'Alpha'),\n",
       " ('들', 'Verb'),\n",
       " ('도', 'Eomi'),\n",
       " ('당황하며', 'Verb'),\n",
       " ('이름', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('재차', 'Noun'),\n",
       " ('물어', 'Noun'),\n",
       " ('웃음', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('자아', 'Noun'),\n",
       " ('냈', 'Verb'),\n",
       " ('다', 'Eomi')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot.pos('.\\n'.join(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pickle.load(open(etri_outcome+idIs.hex()+'.picked.txt','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "etri = testData['return_object']['sentence']\n",
    "morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "\n",
    "for i in etri:\n",
    "    x = Extract_Text_Info(i)\n",
    "    morp += x[0]\n",
    "    wsd += x[1]\n",
    "    word +=x[2]\n",
    "    ne += x[3]\n",
    "    for ii in i['SRL']:\n",
    "        srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "    dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 형태소 분석 : morp\n",
    "* 어휘의미 분석 : wsd (동음이의어)\n",
    "* 어휘의미 분석 : wsd_poly (다의어)\n",
    "* 개체명 인식 : ner\n",
    "* 의존구문 분석 : dependency\n",
    "* 의미역 인식 : srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "morp1 = list(map(lambda x: x[0], morp))\n",
    "wsd1 = list(map(lambda x: x[0], wsd))\n",
    "word1 = list(map(lambda x: x[0], word))\n",
    "ne1 = list(map(lambda x: x[0], ne))\n",
    "srl1 = list(map(lambda x: x[0], srl))\n",
    "dependency1 = list(map(lambda x: x[0], dependency))\n",
    "outlist = []\n",
    "for idx in range(len(etri)):\n",
    "    y = pd.DataFrame(etri[idx]['dependency'])\n",
    "    y2 = y[y['mod'].apply(lambda x: len(x)) ==y['mod'].apply(lambda x: len(x)).max()]\n",
    "    y3 = y2[y2['weight'] == y2['weight'].max()]\n",
    "    y4 = y[y['id'].isin(y3['mod'][y3['mod'].index[0]])]\n",
    "    y5 = y[y['weight'] ==y4['weight'].max()]\n",
    "    y6 = y2.text.values.tolist() + y5.text.values.tolist()\n",
    "    y7 = y[y.text.isin(y6)]    \n",
    "    out = ' '.join(y7.text.tolist())\n",
    "    outlist.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이름', 3.7396725708214373),\n",
       " ('한명회', 2.3645921404341106),\n",
       " ('방송', 1.4051585357702066),\n",
       " ('서로', 1.2071067811865475),\n",
       " ('녹화', 1.0),\n",
       " ('내린다', 0.9136254000951356),\n",
       " ('jtbc', 0.81298699525717766),\n",
       " ('jtbc 제공', 0.70710678118654746),\n",
       " ('가장', 0.70710678118654746),\n",
       " ('물론', 0.57735026918962573),\n",
       " ('물어', 0.57735026918962573),\n",
       " ('인생 스토리', 0.57735026918962562),\n",
       " ('공유', 0.5),\n",
       " ('9회 녹화', 0.5),\n",
       " ('한명회 내린다', 0.45943360466390393),\n",
       " ('오후 30', 0.44721359549995793),\n",
       " ('오후', 0.44721359549995793),\n",
       " ('30', 0.44721359549995793),\n",
       " ('세계일주', 0.35355339059327373),\n",
       " ('세계일주 뭉쳐', 0.35355339059327373),\n",
       " ('jtbc 제공 jtbc 제공', 0.35355339059327373),\n",
       " ('jtbc 제공 jtbc', 0.35355339059327373),\n",
       " ('세계 일주', 0.35355339059327373),\n",
       " ('뭉쳐', 0.35355339059327373),\n",
       " ('일주', 0.35355339059327373),\n",
       " ('세계', 0.35355339059327373),\n",
       " ('제공', 0.35355339059327373),\n",
       " ('배경', 0.28867513459481281),\n",
       " ('기도', 0.28867513459481281),\n",
       " ('끌기도', 0.28867513459481281),\n",
       " ('스토리', 0.28867513459481281),\n",
       " ('스토리 기도', 0.28867513459481281),\n",
       " ('인생', 0.28867513459481281),\n",
       " ('인생 스토리 기도', 0.28867513459481281),\n",
       " ('금지', 0.23570226039551584),\n",
       " ('재배포 금지 무단', 0.23570226039551584),\n",
       " ('전재 배포', 0.23570226039551584),\n",
       " ('전재', 0.23570226039551584),\n",
       " ('재배포 금지 무단 전재 배포', 0.23570226039551584),\n",
       " ('무단', 0.23570226039551584),\n",
       " ('재배포 금지 무단 전재', 0.23570226039551584),\n",
       " ('전재 배포 금지', 0.23570226039551584),\n",
       " ('재배포 금지', 0.23570226039551584),\n",
       " ('무단 전재', 0.23570226039551584),\n",
       " ('무단 전재 배포', 0.23570226039551584),\n",
       " ('배포 금지', 0.23570226039551584),\n",
       " ('배포', 0.23570226039551584),\n",
       " ('무단 전재 배포 금지', 0.23570226039551584),\n",
       " ('무단전재', 0.23570226039551584),\n",
       " ('무단전재 재배포 금지', 0.23570226039551584),\n",
       " ('무단전재 재배포 금지 무단', 0.23570226039551584),\n",
       " ('무단전재 재배포 금지 무단 전재', 0.23570226039551584),\n",
       " ('서로 공유 서로', 0.0),\n",
       " ('끌기도 인생 스토리 기도', 0.0),\n",
       " ('물론 물어', 0.0),\n",
       " ('끌기도 인생 스토리', 0.0),\n",
       " ('인생 스토리 끌기도 인생 스토리', 0.0),\n",
       " ('인생 스토리 끌기도 인생', 0.0),\n",
       " ('인생 스토리 끌기도', 0.0),\n",
       " ('뭉쳐 세계', 0.0),\n",
       " ('뭉쳐 세계 일주', 0.0),\n",
       " ('서로 공유 서로 공유', 0.0),\n",
       " ('끌기도 인생', 0.0),\n",
       " ('오후 30분 방송 오후 30', 0.0),\n",
       " ('오후 30분 방송 오후', 0.0),\n",
       " ('오후 30분 방송', 0.0),\n",
       " ('오후 30분', 0.0),\n",
       " ('방송 오후', 0.0),\n",
       " ('방송 오후 30', 0.0),\n",
       " ('배경 인생 스토리', 0.0),\n",
       " ('배경 인생 스토리 끌기도', 0.0),\n",
       " ('세계일주 뭉쳐 세계 일주', 0.0),\n",
       " ('세계일주 뭉쳐 세계', 0.0),\n",
       " ('서로 공유', 0.0),\n",
       " ('9회 녹화 녹화', 0.0),\n",
       " ('배경 인생 스토리 끌기도 인생', 0.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1, 4),\n",
    "                    stop_words=stopwords)\n",
    "vect.fit(morp1)\n",
    "vect.fit(wsd1)\n",
    "vect.fit(word1)\n",
    "vect.fit(ne1)\n",
    "vect.fit(srl1)\n",
    "vect.fit(outlist)\n",
    "count = vect.transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')).toarray()\n",
    "count1 = count.sum(axis = 0)\n",
    "idx = np.argsort(-count1)\n",
    "count1 = count1[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count1))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1, 1),\n",
    "                    stop_words=stopwords)\n",
    "count = vect.fit_transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Search(x,li):\n",
    "    if x in li:\n",
    "        return li.index(x)\n",
    "dx = list(map(lambda x: Search(x, vect.get_feature_names()), outlist))\n",
    "dx = list(filter(lambda x: x!=None, dx))\n",
    "y =  [1] * len(vect.get_feature_names())\n",
    "for ix in dx:\n",
    "    y[ix] = 1.5\n",
    "count = count * y\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이름', 1.462073472281384),\n",
       " ('jtbc', 1.3410760885726434),\n",
       " ('한명회', 1.200812294088129),\n",
       " ('방송', 1.0777650988856593),\n",
       " ('출연자들', 0.91285085704502411),\n",
       " ('서로', 0.82937538578881664),\n",
       " ('특집', 0.63162804749320345),\n",
       " ('녹화', 0.59395102181825776),\n",
       " ('서로의 이름', 0.5625987354027806),\n",
       " ('이날', 0.54611399773526981),\n",
       " ('출연자', 0.45317762861951294),\n",
       " ('시간', 0.44065002381314938),\n",
       " ('지난 10월 10일', 0.42887908533317826),\n",
       " ('지난', 0.42887908533317826),\n",
       " ('10', 0.42887908533317826),\n",
       " ('10월 10일', 0.42887908533317826),\n",
       " ('최근', 0.39841145730442984),\n",
       " ('이름 특집', 0.36197879812939515),\n",
       " ('별난 이름 특집', 0.36197879812939515),\n",
       " ('별난', 0.36197879812939515),\n",
       " ('특집 명', 0.35025212164224351),\n",
       " ('조치', 0.35025212164224351),\n",
       " ('녹화 전', 0.35025212164224351),\n",
       " ('오후 9시 30분', 0.34428281216771534),\n",
       " ('오후', 0.34428281216771534),\n",
       " ('마지막 회', 0.34428281216771534),\n",
       " ('마지막', 0.34428281216771534),\n",
       " ('9시 30분', 0.34428281216771534),\n",
       " ('30', 0.34428281216771534),\n",
       " ('5일 오후 9시 30분', 0.34428281216771534),\n",
       " ('기자', 0.34107608857264343),\n",
       " ('이승', 0.34107608857264343),\n",
       " ('내 이름', 0.34107608857264343),\n",
       " ('이승길 기자', 0.34107608857264343),\n",
       " ('한명', 0.34107608857264343),\n",
       " ('이하', 0.34107608857264343),\n",
       " ('9회 녹화', 0.33612941507074895),\n",
       " ('물어', 0.3203940008032935),\n",
       " ('웃음', 0.3203940008032935),\n",
       " ('재차', 0.3203940008032935),\n",
       " ('공유', 0.31822338366502678),\n",
       " ('영상', 0.30988645117668373),\n",
       " ('끌기도', 0.30663016297506673),\n",
       " ('스토리', 0.30663016297506673),\n",
       " ('마냥', 0.30663016297506673),\n",
       " ('인생 스토리', 0.30663016297506673),\n",
       " ('기도', 0.30663016297506673),\n",
       " ('눈길', 0.30663016297506673),\n",
       " ('인생', 0.30663016297506673),\n",
       " ('편성 시간', 0.30460320138476582),\n",
       " ('편성', 0.30460320138476582),\n",
       " ('전파', 0.30460320138476582),\n",
       " ('일주', 0.30460320138476582),\n",
       " ('세계', 0.30460320138476582),\n",
       " ('뭉쳐', 0.30460320138476582),\n",
       " ('방송 시간', 0.30460320138476582),\n",
       " ('세계일주', 0.30460320138476582),\n",
       " ('기상천외한 이름', 0.29109563295020013),\n",
       " ('출연자의 기상천외한 이름', 0.29109563295020013),\n",
       " ('남성 출연자의 기상천외한 이름', 0.29109563295020013),\n",
       " ('남성', 0.29109563295020013),\n",
       " ('기상천', 0.29109563295020013),\n",
       " ('외한', 0.29109563295020013),\n",
       " ('남성 출연자', 0.29109563295020013),\n",
       " ('탄생 배경', 0.27859042662802741),\n",
       " ('배경', 0.27859042662802741),\n",
       " ('탄생', 0.27859042662802741),\n",
       " ('이름 탄생 배경', 0.27859042662802741),\n",
       " ('패키지', 0.25698582215195864),\n",
       " ('다른 이름', 0.2508274852751764),\n",
       " ('다른', 0.2508274852751764),\n",
       " ('대화', 0.2508274852751764),\n",
       " ('8명의 출연자', 0.2508274852751764),\n",
       " ('명의', 0.2508274852751764),\n",
       " ('이날 8명의 출연자', 0.2508274852751764),\n",
       " ('공감 대화', 0.2508274852751764),\n",
       " ('자기소개 시간', 0.2508274852751764),\n",
       " ('공감', 0.2508274852751764),\n",
       " ('소개', 0.2508274852751764),\n",
       " ('출석 부르는 시간', 0.2508274852751764),\n",
       " ('출석', 0.227890614070243),\n",
       " ('가장', 0.227890614070243),\n",
       " ('심정', 0.227890614070243),\n",
       " ('9회', 0.0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1, 1),\n",
    "                    stop_words=stopwords)\n",
    "vect.fit(morp1)\n",
    "vect.fit(wsd1)\n",
    "vect.fit(word1)\n",
    "vect.fit(ne1)\n",
    "vect.fit(srl1)\n",
    "#vect.fit(outlist)\n",
    "count = vect.transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')).toarray()\n",
    "count1 = count.sum(axis = 0)\n",
    "def Search(x,li):\n",
    "    if x in li:\n",
    "        return li.index(x)\n",
    "dx = list(map(lambda x: Search(x, vect.get_feature_names()), outlist))\n",
    "dx = list(filter(lambda x: x!=None, dx))\n",
    "y =  [1] * len(vect.get_feature_names())\n",
    "for ix in dx:\n",
    "    y[ix] = 5\n",
    "p = count1 * y\n",
    "#idx = np.argsort(-count1)\n",
    "idx = np.argsort(-p)\n",
    "count1 = count1[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count1))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1[dx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1[dx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([1]*len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[dx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (count1[d])\n",
    "count1[d] * np.array([1.25]*len(d)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (count1[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keywords(text, stopwords):\n",
    "    etri = Run_ETRI_Analysis(stopwords, text)\n",
    "    soy1 = RunLRNounExtractor(text)\n",
    "    soy2 = RunWordExtractor(text)\n",
    "    et0 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[0]))\n",
    "    et0 = list(map(lambda x: x[0], et0))\n",
    "    et1 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[1]))\n",
    "    et1 = list(map(lambda x: x[0], et1))\n",
    "    et2 = list(map(lambda x: x[0], etri[2]))\n",
    "    et3 = list(map(lambda x: x[0], etri[3]))\n",
    "    et4 = list(filter(lambda x: x[1] in ['ARG0','ARG1','ARG2','ARG3','ARG4'], etri[4]))\n",
    "    et4 = list(map(lambda x: x[0], et4))\n",
    "    et5 = list(filter(lambda x: x[1] in ['NP','NP_SBJ','NP_OBJ','NP_AJT','VNP'], etri[5]))\n",
    "    et5 = list(map(lambda x: x[0], et5))\n",
    "    mecabout = list(filter(lambda x: x[1] in ['NNG','NNB','NNP'], mecab.pos(text)))\n",
    "    ctout = list(filter(lambda x: x[1] == 'Noun' , ct.pos(text)))\n",
    "    otout = list(filter(lambda x: x[1] == 'Noun' , ot.pos(text)))\n",
    "    mcout = list(map(lambda x: x[0], mecabout))\n",
    "    ctout = list(map(lambda x: x[0], ctout))\n",
    "    otout = list(map(lambda x: x[0], otout))\n",
    "    out = list(soy1.keys())+list(soy2.keys())+ctout+otout+mcout+et3+et4+et0+et1+et2+et5\n",
    "    vect = TfidfVectorizer().fit(out)\n",
    "    y = [list(soy1.keys()), list(soy2.keys()),et3, et4, ctout, otout, mcout,et0, et1, et2, et5]\n",
    "    y = list(filter(lambda x: len(x) !=0, y))\n",
    "    outdict = dict()\n",
    "    for i in y:\n",
    "        count = vect.transform(i).toarray().sum(axis = 0)\n",
    "        idx = np.argsort(-count)\n",
    "        count = count[idx]\n",
    "        feature_name = np.array(vect.get_feature_names())[idx]\n",
    "        out = dict(zip(feature_name, count))\n",
    "        for ii in out:\n",
    "            if not ii in outdict:\n",
    "                outdict[ii] = out[ii]\n",
    "            else:\n",
    "                outdict[ii] +=out[ii]\n",
    "    x = sorted(outdict.items(), key = itemgetter(1), reverse=True)[:5]\n",
    "    output = list(map(lambda x: x[0], x))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data in useCollection.find({'site':'daum'}):\n",
    "    if len(data['mainText'].strip()) !=0:\n",
    "        if data['keywords'] !='NaN':\n",
    "            ct.add_dictionary(list(map(lambda x: x.split(' · ')[-1], data['keywords'])), 'Noun')\n",
    "        y = Extract_Keywords2(data['mainText'],stopwords)\n",
    "        z = TextRank(data['mainText'])\n",
    "        p = RunLRNounExtractor(data['mainText'])\n",
    "        q = RunWordExtractor(data['mainText'])\n",
    "        #print (y)\n",
    "        print (data['keywords'])\n",
    "        print (z.keywords())\n",
    "        #print (p.keys())\n",
    "        #print (q.keys())\n",
    "        print ()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding = 'utf-8', mode='r') as f:\n",
    "        data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
