{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 빅데이터 분석 데이터 추출 & 사용자 사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath = 'C:/Users/pc/Anaconda3/lib/site-packages/ckonlpy/data/twitter/noun/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './data/nnews/high_frequency_noun/'\n",
    "path2 = './data/news/mainissue/'\n",
    "path3 = './data/news/newstopic/'\n",
    "path4 = './data/news/people/'\n",
    "path5 = './data/news/4th_industry/'\n",
    "path6 = './data/news/have_negative_positive/constitution/'\n",
    "path7 = './data/news/have_negative_positive/household_debt/'\n",
    "path8 = './data/news/have_negative_positive/olymphic/'\n",
    "path9 = './data/news/have_negative_positive/'\n",
    "path10 = './data/news/'\n",
    "\n",
    "pathlist = [path1, path2, path3,\n",
    "            path4, path5, path6,\n",
    "            path7, path8, path9,\n",
    "           path10]\n",
    "if not os.path.isfile(dbpath+'from_news.txt'):\n",
    "    with open(dbpath+'from_news.txt','w') as f:\n",
    "        for path in pathlist:\n",
    "            out = Extract_Keyword_from_news_bigdata(path)\n",
    "            f.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Mecab, Twitter\n",
    "from konlpy.utils import pprint\n",
    "import os\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwords = Stopwords('./data/koreanStopwords.txt') +Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "list(map(lambda x: len(xxxx[x]), xxxx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETRI의 엑소브레인을 이용한 한글 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "sys.path.append('/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/')\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import time, re, pickle, itertools\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from multiprocessing import Pool\n",
    "import urllib3, json\n",
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import os\n",
    "\n",
    "def OpenAPI():\n",
    "    import pickle\n",
    "    openapi = pickle.load(open('./etri_open_api_access_key', 'rb'))\n",
    "    return openapi\n",
    "def USE_ETRI_ANALYSIS(analysisCode, text):    \n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    requestJson = {\n",
    "        'access_key' : accessKey,\n",
    "        'argument' : {\n",
    "            'text' : text,\n",
    "            'analysis_code' : analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "    return json.loads(str(response.data,'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_INFO(fWord, sWord):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/WordRel\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    firstWord = fWord\n",
    "    #firstSenseId = fSenseId\n",
    "    secondWord = sWord\n",
    "    #secondSenseId = sSenseId\n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "        'first_word': firstWord,\n",
    "        #'first_sense_id': firstSenseId,\n",
    "        'second_word': secondWord,\n",
    "        #'second_sense_id': secondSenseId\n",
    "        }}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "    body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_RELATION(word):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/Word\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    word = word\n",
    "    requestJson = {\n",
    "    \"access_key\": accessKey,\n",
    "    \"argument\": {\"word\": word}}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "                            body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def Extract_Text_Info(idx):\n",
    "    morp = list(map(lambda x: (x['lemma'], x['type']),idx['morp']))\n",
    "    wsd = list(map(lambda x: (x['text'], x['type']),idx['WSD']))\n",
    "    word = list(map(lambda x: (x['text'], x['type']),idx['word']))\n",
    "    ne = list(map(lambda x: (x['text'], x['type']),idx['NE']))\n",
    "    return morp, wsd, word, ne\n",
    "\n",
    "def Run_ETRI_Analysis(stopwordsList, text):\n",
    "    etri = USE_ETRI_ANALYSIS('srl', text)['return_object']    \n",
    "    etri = etri['sentence']\n",
    "    morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "    for i in etri:\n",
    "        x = Extract_Text_Info(i)\n",
    "        morp += x[0]\n",
    "        wsd += x[1]\n",
    "        word +=x[2]\n",
    "        ne += x[3]\n",
    "        for ii in i['SRL']:\n",
    "            srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "        dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "    return morp, wsd, word, ne, srl, dependency, etri\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "site = 'Naver'\n",
    "#site = 'daum'\n",
    "if site == 'daum':\n",
    "    collection = 'newsDaum'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/daum2/'\n",
    "elif site.lower() == 'naver':\n",
    "    collection = 'newsNaver'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/naver2/'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, collection)\n",
    "\n",
    "keywordList = list()\n",
    "pressList = list()\n",
    "for data in useCollection.find({'site':site}):\n",
    "    pressList.append(data['press'])\n",
    "    tis = data['title']+'. '+data['mainText']\n",
    "    tis2 = '.\\n'.join(tis.split('. '))\n",
    "    if 'keywords' in data.keys():\n",
    "        if data['keywords'] !='NaN':\n",
    "            keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "        else:\n",
    "            pass\n",
    "    idIs = data['_id']._ObjectId__id\n",
    "    if not os.path.isfile(etri_outcome+idIs.hex()+'.picked.txt'):\n",
    "        try:\n",
    "            etri = USE_ETRI_ANALYSIS('srl', tis2)\n",
    "            print (etri[1])\n",
    "        except:\n",
    "            print (etri[1])\n",
    "            break\n",
    "        else:\n",
    "            pickle.dump(etri[0], open(etri_outcome+idIs.hex()+'.picked.txt','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.twitter = ct\n",
    "        #self.twitter = ot\n",
    "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "        ,\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]    \n",
    "    def text2sentences(self, text):\n",
    "        sentences = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.phrases(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = sknorm(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=5):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.hangle import normalize\n",
    "def RunWordKRwordRank(text, min_count):\n",
    "    #min_count = 2   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "    max_length = 10 # 단어의 최대 길이\n",
    "    wordrank_extractor = KRWordRank(min_count, max_length)\n",
    "    beta = 0.85   # PageRank의 decaying factor beta\n",
    "    max_iter = 100\n",
    "    verbose = False\n",
    "    textIs = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "    textIs = list(map(lambda x: normalize(x, english = True, number = True), textIs))\n",
    "    keywords, rank, graph = wordrank_extractor.extract(textIs, beta, max_iter, verbose)\n",
    "    return keywords, rank, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pickle.load(open(etri_outcome+idIs.hex()+'.picked.txt','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sTest = data['title']+'. '+data['mainText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank = TextRank(sTest)\n",
    "for row in textrank.summarize(len(sTest.split('. ')) // 3):\n",
    "    print(row)\n",
    "    print()\n",
    "print('keywords :',textrank.keywords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etri = testdata['return_object']['sentence']\n",
    "morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "\n",
    "for i in etri:\n",
    "    x = Extract_Text_Info(i)\n",
    "    morp += x[0]\n",
    "    wsd += x[1]\n",
    "    word +=x[2]\n",
    "    ne += x[3]\n",
    "    for ii in i['SRL']:\n",
    "        srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "    dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "morp1 = list(map(lambda x: x[0], morp))\n",
    "wsd1 = list(map(lambda x: x[0], wsd))\n",
    "word1 = list(map(lambda x: x[0], word))\n",
    "ne1 = list(map(lambda x: x[0], ne))\n",
    "srl1 = list(map(lambda x: x[0], srl))\n",
    "dependency1 = list(map(lambda x: x[0], dependency))\n",
    "outlist = []\n",
    "for idx in range(len(etri)):\n",
    "    y = pd.DataFrame(etri[idx]['dependency'])\n",
    "    y2 = y[y['mod'].apply(lambda x: len(x)) ==y['mod'].apply(lambda x: len(x)).max()]\n",
    "    y3 = y2[y2['weight'] == y2['weight'].max()]\n",
    "    y4 = y[y['id'].isin(y3['mod'][y3['mod'].index[0]])]\n",
    "    y5 = y[y['weight'] ==y4['weight'].max()]\n",
    "    y6 = y2.text.values.tolist() + y5.text.values.tolist()\n",
    "    y7 = y[y.text.isin(y6)]    \n",
    "    out = ' '.join(y7.text.tolist())\n",
    "    outlist.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 형태소 분석 : morp\n",
    "* 어휘의미 분석 : wsd (동음이의어)\n",
    "* 어휘의미 분석 : wsd_poly (다의어)\n",
    "* 개체명 인식 : ner\n",
    "* 의존구문 분석 : dependency\n",
    "* 의미역 인식 : srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(filter(lambda x: (x[1] !='OGG_MEDIA') & (x[1] !='TMI_SITE') & (x[1] !='DT_OTHERS') & (x[1] !='TI_OTHERS'), ne))\n",
    "f = list(filter(lambda x: (x[1] =='OGG_MEDIA') | (x[1] =='TMI_SITE') | (x[1] =='DT_OTHERS') | (x[1] =='TI_OTHERS'), ne))\n",
    "p1 = list(map(lambda x: x[0], p))\n",
    "f1 = list(map(lambda x: x[0], f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=ct.nouns, ngram_range=(1, 1),\n",
    "                       norm='l2',sublinear_tf  = True,\n",
    "                    stop_words=stopwords,max_df = 0.5, min_df = 0.005)\n",
    "#vect.fit(morp1)\n",
    "vect.fit(word1)\n",
    "vect.fit(wsd1)\n",
    "vect.fit(ct.nouns(data['title']))\n",
    "vect.fit(ct.phrases(data['title']))\n",
    "vect.fit(ct.nouns(data['mainText']))\n",
    "vect.fit(ct.phrases(data['mainText']))\n",
    "count = vect.transform(t).toarray()\n",
    "count1 = count.sum(axis = 0)\n",
    "idx = np.argsort(-count1)\n",
    "count1 = count1[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count1))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workrank1 = RunWordKRwordRank(data['title']+'. '+data['mainText'], 3)\n",
    "workrank1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.phrases(data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexrankr import LexRank\n",
    "lexrank = LexRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank.summarize(data['title']+'. '+data['mainText'])\n",
    "lexrank.probe(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "random.seed(0)\n",
    "K=4\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(tokenizer=ct.nouns, ngram_range=(1,4), stop_words=stopwords)\n",
    "vect.fit(ct.nouns(data['title'])\n",
    "vect.fit(ct.nouns(data['mainText']))\n",
    "count = vect.transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')+[data['title']]).toarray()\n",
    "count1 = count.sum(axis = 0)\n",
    "idx = np.argsort(-count1)\n",
    "count1 = count1[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count1))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search(x,li):\n",
    "    if x in li:\n",
    "        return li.index(x)\n",
    "dx = list(map(lambda x: Search(x, vect.get_feature_names()), ct.nouns(data['title'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1, 1),\n",
    "                       norm='l2',sublinear_tf  = True,\n",
    "                    stop_words=stopwords,max_df = 0.5, min_df = 0.005)\n",
    "vect.fit(morp1)\n",
    "vect.fit(wsd1)\n",
    "vect.fit(word1)\n",
    "vect.fit(ne1)\n",
    "vect.fit(srl1)\n",
    "vect.fit(outlist)\n",
    "vect.fit(ct.nouns(data['title']))\n",
    "vect.fit(ct.phrases(data['title']))\n",
    "vect.fit(ct.nouns(data['mainText']))\n",
    "vect.fit(ct.phrases(data['mainText']))\n",
    "count = vect.transform(t).toarray()\n",
    "count1 = count.sum(axis = 0)\n",
    "idx = np.argsort(-count1)\n",
    "count1 = count1[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count1))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1, 1),\n",
    "                    stop_words=stopwords)\n",
    "count = vect.fit_transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Search(x,li):\n",
    "    if x in li:\n",
    "        return li.index(x)\n",
    "dx = list(map(lambda x: Search(x, vect.get_feature_names()), outlist))\n",
    "dx = list(filter(lambda x: x!=None, dx))\n",
    "y =  [1] * len(vect.get_feature_names())\n",
    "for ix in dx:\n",
    "    y[ix] = 1.5\n",
    "count = count * y\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1, 1),\n",
    "                    stop_words=stopwords)\n",
    "'''\n",
    "vect.fit(morp1)\n",
    "vect.fit(wsd1)\n",
    "vect.fit(word1)\n",
    "vect.fit(ne1)\n",
    "vect.fit(srl1)\n",
    "vect.fit(outlist)'''\n",
    "vect.fit(ct.phrases(data['title']))\n",
    "vect.fit(ct.phrases(data['mainText']))\n",
    "count = vect.fit_transform('\\n'.join(t).split('\\n')).toarray()\n",
    "count1 = count.sum(axis = 0)\n",
    "def Search(x,li):\n",
    "    if x in li:\n",
    "        return li.index(x)\n",
    "#dx = list(map(lambda x: Search(x, vect.get_feature_names()), outlist))\n",
    "dx = list(map(lambda x: Search(x, vect.get_feature_names()), ct.phrases(data['title'])))\n",
    "dx = list(filter(lambda x: x!=None, dx))\n",
    "y =  [1] * len(vect.get_feature_names())\n",
    "for ix in dx:\n",
    "    y[ix] = 1.5\n",
    "p = count1 * y\n",
    "#idx = np.argsort(-count1)\n",
    "idx = np.argsort(-p)\n",
    "count1 = count1[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "out = dict(zip(feature_name, count1))\n",
    "sorted(out.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keywords(text, stopwords):\n",
    "    etri = Run_ETRI_Analysis(stopwords, text)\n",
    "    soy1 = RunLRNounExtractor(text)\n",
    "    soy2 = RunWordExtractor(text)\n",
    "    et0 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[0]))\n",
    "    et0 = list(map(lambda x: x[0], et0))\n",
    "    et1 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[1]))\n",
    "    et1 = list(map(lambda x: x[0], et1))\n",
    "    et2 = list(map(lambda x: x[0], etri[2]))\n",
    "    et3 = list(map(lambda x: x[0], etri[3]))\n",
    "    et4 = list(filter(lambda x: x[1] in ['ARG0','ARG1','ARG2','ARG3','ARG4'], etri[4]))\n",
    "    et4 = list(map(lambda x: x[0], et4))\n",
    "    et5 = list(filter(lambda x: x[1] in ['NP','NP_SBJ','NP_OBJ','NP_AJT','VNP'], etri[5]))\n",
    "    et5 = list(map(lambda x: x[0], et5))\n",
    "    mecabout = list(filter(lambda x: x[1] in ['NNG','NNB','NNP'], mecab.pos(text)))\n",
    "    ctout = list(filter(lambda x: x[1] == 'Noun' , ct.pos(text)))\n",
    "    otout = list(filter(lambda x: x[1] == 'Noun' , ot.pos(text)))\n",
    "    mcout = list(map(lambda x: x[0], mecabout))\n",
    "    ctout = list(map(lambda x: x[0], ctout))\n",
    "    otout = list(map(lambda x: x[0], otout))\n",
    "    out = list(soy1.keys())+list(soy2.keys())+ctout+otout+mcout+et3+et4+et0+et1+et2+et5\n",
    "    vect = TfidfVectorizer().fit(out)\n",
    "    y = [list(soy1.keys()), list(soy2.keys()),et3, et4, ctout, otout, mcout,et0, et1, et2, et5]\n",
    "    y = list(filter(lambda x: len(x) !=0, y))\n",
    "    outdict = dict()\n",
    "    for i in y:\n",
    "        count = vect.transform(i).toarray().sum(axis = 0)\n",
    "        idx = np.argsort(-count)\n",
    "        count = count[idx]\n",
    "        feature_name = np.array(vect.get_feature_names())[idx]\n",
    "        out = dict(zip(feature_name, count))\n",
    "        for ii in out:\n",
    "            if not ii in outdict:\n",
    "                outdict[ii] = out[ii]\n",
    "            else:\n",
    "                outdict[ii] +=out[ii]\n",
    "    x = sorted(outdict.items(), key = itemgetter(1), reverse=True)[:5]\n",
    "    output = list(map(lambda x: x[0], x))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data in useCollection.find({'site':'daum'}):\n",
    "    if len(data['mainText'].strip()) !=0:\n",
    "        if data['keywords'] !='NaN':\n",
    "            ct.add_dictionary(list(map(lambda x: x.split(' · ')[-1], data['keywords'])), 'Noun')\n",
    "        y = Extract_Keywords2(data['mainText'],stopwords)\n",
    "        z = TextRank(data['mainText'])\n",
    "        p = RunLRNounExtractor(data['mainText'])\n",
    "        q = RunWordExtractor(data['mainText'])\n",
    "        #print (y)\n",
    "        print (data['keywords'])\n",
    "        print (z.keywords())\n",
    "        #print (p.keys())\n",
    "        #print (q.keys())\n",
    "        print ()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding = 'utf-8', mode='r') as f:\n",
    "        data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
