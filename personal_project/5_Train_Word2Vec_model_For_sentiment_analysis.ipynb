{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model 생성을 위한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "#sg (int {1, 0}) – Defines the training algorithm. If 1, CBOW is used, otherwise, skip-gram is employed.\n",
    "def Make_Word2Vec_Model(data, size, epoch, sg, window,min_count, cbow_mean, workers, negative, hs, tagger):\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\")\n",
    "    from datetime import datetime\n",
    "    from gensim.models import word2vec\n",
    "    start = datetime.now()\n",
    "    modelPath = './model/'\n",
    "    modelName = 'word2vec_size-{}_epoch-{}_window-{}_negative-{}_hs-{}_sg-{}_cbow_mean-{}_min_count-{}_by-{}.model'.format(\n",
    "        size, epoch, window, negative, hs, sg, cbow_mean, min_count, tagger)\n",
    "    modelName = modelPath+modelName\n",
    "    print (modelName)\n",
    "    w2v_model = word2vec.Word2Vec(size = size, sg = sg, cbow_mean = cbow_mean, negative = negative,\n",
    "                                  hs = hs, window = window, workers = workers, iter=epoch, min_count = min_count)\n",
    "    w2v_model.build_vocab(tqdm(data))\n",
    "    w2v_model.train(tqdm(data), total_examples=w2v_model.corpus_count, epochs=w2v_model.iter) \n",
    "    end = datetime.now()\n",
    "    w2v_model.save(modelName)\n",
    "    print (\"Total running time: \", end-start)\n",
    "    return d2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491510, 2)\n"
     ]
    }
   ],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None,encoding='utf-8')\n",
    "print (rawdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>주택조합 승인 자체가 몹시 어려웠던 시절 한국기자협회 주택조합 형식으로 추진된 이 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서 의원은 여의도  회견을 열어 “압도적으로 승리할 것이고 국민, 당원이 주문하신 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>박근혜 대통령이 14일 열린 새누리당 전당대회에 참석한 것은 그만큼 아직 여권에서 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“살기 좋은 전북 만들기, 중앙정부와 연결고리 최선” “월드컵에 출전한 우리 국가대...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"여러분 너무나 고맙고 감사합니다. 도와주신 여러분께 저의 마음속의 큰 절을 받아주...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  주택조합 승인 자체가 몹시 어려웠던 시절 한국기자협회 주택조합 형식으로 추진된 이 ...  1\n",
       "1  서 의원은 여의도  회견을 열어 “압도적으로 승리할 것이고 국민, 당원이 주문하신 ...  1\n",
       "2  박근혜 대통령이 14일 열린 새누리당 전당대회에 참석한 것은 그만큼 아직 여권에서 ...  1\n",
       "3  “살기 좋은 전북 만들기, 중앙정부와 연결고리 최선” “월드컵에 출전한 우리 국가대...  1\n",
       "4  \"여러분 너무나 고맙고 감사합니다. 도와주신 여러분께 저의 마음속의 큰 절을 받아주...  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOPWORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 포맷으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tagging(sentence, tagger, stopwords):\n",
    "    pos = tagger.pos(sentence)\n",
    "    pos = [x[0] for x in pos]\n",
    "    pos = [x for x in pos if not x in stopwords]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def Tagging2(sentence, tagger, stopwords):\n",
    "    pos = pd.Series(tagger.pos(sentence)).str[0]\n",
    "    pos = pos[~pos.isin(stopwords)]\n",
    "    return pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Pre_Data_Sub(series, tagger, stopwords):\n",
    "    from gensim.models.doc2vec import TaggedDocument\n",
    "    pos = Tagging2(series[0], tagger, stopwords)\n",
    "    label = series[1]\n",
    "    return TaggedDocument(pos, [label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def Make_Pre_Data(rawdata, tagger, stopwords):\n",
    "    outList = list()\n",
    "    for idx in tqdm(rawdata.index):\n",
    "        outList.append([Tagging2(rawdata.loc[idx, 0], tagger, stopwords), rawdata.loc[idx, 1]])\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "ct = ctwitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 491510/491510 [2:20:05<00:00, 58.48it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 10min 35s, sys: 3min 3s, total: 2h 13min 38s\n",
      "Wall time: 2h 20min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm.pandas(desc=\"progress\")\n",
    "pre_data = rawdata.progress_apply(lambda x: Make_Pre_Data_Sub(x, ct, stopwords), axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pre_data,open('./data/pre_data/tagged_data/pre_data__for_word2vec_sentiment_by_ct.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(pre_data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train,open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_ct.pickled','wb'))\n",
    "pickle.dump(test,open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_ct.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_ct.pickled','rb'))\n",
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_ct.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ x.words for x in tqdm(train)] \n",
    "y_train = [ x.tags for x in tqdm(train)] \n",
    "x_test = [ x.words for x in tqdm(test)] \n",
    "y_test = [ x.tags for x in tqdm(test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Make_Word2Vec_Model(data = x_train, size = 3000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7,\n",
    "                    cbow_mean = 1, hs = 0, tagger = 'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Make_Word2Vec_Model(data = x_train, size = 3000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Make_Word2Vec_Model(data = x_train, size = 3000, epoch = 20, \n",
    "                   sg = 1, window = 10, workers = cores, negative = 7,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 491510/491510 [22:27<00:00, 364.82it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 31s, sys: 3min 16s, total: 21min 48s\n",
      "Wall time: 22min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pre_data = rawdata.progress_apply(lambda x: Make_Pre_Data_Sub(x, mecab, stopwords), axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 113689/491510 [06:41<22:13, 283.37it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pre_data = Make_Pre_Data(rawdata, mecab, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pre_data,open('./data/pre_data/tagged_data/pre_data__for_word2vec_sentiment_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(pre_data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train,open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_mecab.pickled','wb'))\n",
    "pickle.dump(test,open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_mecab.pickled','rb'))\n",
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_mecab.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ x.words for x in tqdm(train)] \n",
    "y_train = [ x.tags for x in tqdm(train)] \n",
    "x_test = [ x.words for x in tqdm(test)] \n",
    "y_test = [ x.tags for x in tqdm(test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Make_Word2Vec_Model(data = x_train, size = 3000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7,\n",
    "                    cbow_mean = 1, hs = 0, tagger = 'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Make_Word2Vec_Model(data = x_train, size = 3000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Make_Word2Vec_Model(data = x_train, size = 3000, epoch = 20, \n",
    "                   sg = 1, window = 10, workers = cores, negative = 7,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importing bokeh library for interactive dataviz\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "# defining the chart\n",
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of word vectors\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "# getting a list of word vectors. limit to 10000. each is of 200 dimensions\n",
    "word_vectors = [w2v_model[w] for w in list(w2v_model.wv.vocab.keys())[:10000]]\n",
    "\n",
    "# dimensionality reduction. converting the vectors to 2d vectors\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "# putting everything in a dataframe\n",
    "tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "tsne_df['words'] = list(w2v_model.wv.vocab.keys())[:10000]\n",
    "\n",
    "# plotting. the corresponding word appears when you hover on the data point.\n",
    "plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"word\": \"@words\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print ('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform(x_train)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 3000) for z in tqdm(map(lambda x: x, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 3000) for z in tqdm(map(lambda x: x, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Embedding, embeddings, merge\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=3000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, np.array(y_train), epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_vecs_w2v, np.array(y_test), verbose=2)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1500 : 0.835\n",
    "* 3000 :  0.8513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/keras_model_size3000_window8_iter20_from_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
