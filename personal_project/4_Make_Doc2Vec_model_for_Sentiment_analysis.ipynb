{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec model\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Paragraph Vector\n",
    "> Le and Mikolov 2014 introduces the Paragraph Vector, which outperforms more naïve representations of documents such as averaging the Word2vec word vectors of a document. The idea is straightforward: we act as if a paragraph (or document) is just another vector like a word vector, but we will call it a paragraph vector. We determine the embedding of the paragraph in vector space in the same way as words. Our paragraph vector model considers local word order like bag of n-grams, but gives us a denser representation in vector space compared to a sparse, high-dimensional representation.\n",
    "\n",
    "> * Paragraph Vector - Distributed Memory (PV-DM)\n",
    ">> This is the Paragraph Vector model analogous to Continuous-bag-of-words Word2vec. The paragraph vectors are obtained by training a neural network on the fake task of inferring a center word based on context words and a context paragraph. A paragraph is a context for all words in the paragraph, and a word in a paragraph can have that paragraph as a context.\n",
    "\n",
    "> * Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    ">> This is the Paragraph Vector model analogous to Skip-gram Word2vec. The paragraph vectors are obtained by training a neural network on the fake task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성을 위한 함수정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = int(multiprocessing.cpu_count()) / 2\n",
    "def Make_Doc2Vec_Model(data, size, dm, dm_concat, dm_mean, hs, negative, epoch, window, alpha, min_alpha, workers, tagger):\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\")\n",
    "    from datetime import datetime\n",
    "    from gensim.models import doc2vec\n",
    "    start = datetime.now()\n",
    "    modelPath = './model/'\n",
    "    modelName = 'doc2vec_size-{}_epoch-{}_window-{}_negative-{}_hs-{}_dm-{}_dm_concat-{}_dm_mean-{}_by-{}.model'.format(\n",
    "        size, epoch, window, negative, hs, dm, dm_concat, dm_mean, tagger)\n",
    "    modelName = modelPath+modelName\n",
    "    print (modelName)\n",
    "    if window!=None:\n",
    "        d2v_model = doc2vec.Doc2Vec(vector_size = size, dm = dm, dm_concat = dm_concat,\n",
    "                   dm_mean = dm_mean, negative = negative, hs = hs, window = window,\n",
    "                   alpha = alpha, min_alpha = min_alpha, workers = workers, epochs= epoch)\n",
    "    else:\n",
    "        d2v_model = doc2vec.Doc2Vec(vector_size = size, dm = dm, dm_concat = dm_concat,\n",
    "                   dm_mean = dm_mean, negative = negative, hs = hs,\n",
    "                   alpha = alpha, min_alpha = min_alpha, workers = workers, epochs= epoch)\n",
    "    d2v_model.build_vocab(tqdm(data))\n",
    "    d2v_model.train(tqdm(data), total_examples=d2v_model.corpus_count, epochs=d2v_model.iter)\n",
    "    \n",
    "    end = datetime.now()\n",
    "    d2v_model.save(modelName)\n",
    "    print (\"Total running time: \", end-start)\n",
    "    return d2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감정 분석을 위한 rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491510, 2)\n"
     ]
    }
   ],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None,encoding='utf-8')\n",
    "print (rawdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Doc2Vec Using tagger Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "ct = ctwitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter\n",
    "def tokenize1(doc):\n",
    "    return ['/'.join(t) for t in ct.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5067/491510 [01:42<2:43:33, 49.57it/s]"
     ]
    }
   ],
   "source": [
    "# pickle로 저장된 파일이 없을 때\n",
    "raw_doc_ct = [(tokenize1(rawdata.loc[idx][0]), [idx], rawdata.loc[idx][1]) for idx in tqdm(rawdata.index)]\n",
    "pickle.dump(raw_doc_ct, open('./data/pre_data/tagged_data/pre_data_by_ct_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec 기본 포맷으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 없을 때\n",
    "\n",
    "tagged_ct = [TaggedDocument(b, c, [d]) for b, c, d in tqdm(raw_doc_ct)]\n",
    "pickle.dump(tagged_ct, open('./data/pre_data/tagged_data/pre_by_ct_data_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw_doc_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 있을 때\n",
    "tagged_ct = pickle.load(open('./data/pre_data/tagged_data/pre_by_ct_data_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dataset & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 없을 때\n",
    "train, test = train_test_split(tagged_ct, test_size=0.1, random_state=42)\n",
    "del tagged_ct\n",
    "pickle.dump(train, open('./data/pre_data/train_test_Data/pre_by_ct_train.pickled','wb'))\n",
    "pickle.dump(test, open('./data/pre_data/train_test_Data/pre_by_ct_test.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 있을 때\n",
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_by_ct_train.pickled','rb'))\n",
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_by_ct_test.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size : Dimensionality of the feature vectors \n",
    "* dm : 1 - distibuted memory (PV-DM)  \n",
    "* dm_concat : 1 - use concatenation of context vectors rather than sum/average  \n",
    "* dm_mean : 0 - don't use the sum of the context word vectors  \n",
    "> dm is used in non-concatenative mode.\n",
    "* negative : 7 - neative specifies how many 'noise words' should be drawn.\n",
    "* hs : 0 - hierarchical softmax 사용여부\n",
    "* window : 5 - The maximum distance between the current and predicted word within a sentence.  \n",
    "* alpha : the initial learning rate  \n",
    "* min_alpha : learning rate will linearly drop to min_alpha as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM W/\n",
    "d2v_model = Make_Doc2Vec_Model(data=train, size = 1000, dm = 1, dm_concat = 1,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = 5,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'ct')\n",
    "pprint(d2v_model.most_similar('문재인/Noun'))\n",
    "pprint(d2v_model.most_similar('노무현/Noun'))\n",
    "pprint(d2v_model.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size : Dimensionality of the feature vectors \n",
    "* dm : 1 - distibuted memory (PV-DM)  \n",
    "* dm_concat : 0 - don't use concatenation of context vectors rather than sum/average  \n",
    "* dm_mean : 1 - don't use the sum of the context word vectors  \n",
    "> dm is used in non-concatenative mode.\n",
    "* negative : 7 - neative specifies how many 'noise words' should be drawn.\n",
    "* hs : 0 - hierarchical softmax 사용여부\n",
    "* window : 10 - The maximum distance between the current and predicted word within a sentence.  \n",
    "* alpha : the initial learning rate  \n",
    "* min_alpha : learning rate will linearly drop to min_alpha as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM w/\n",
    "d2v_model = Make_Doc2Vec_Model(data=train, size = 1000, dm = 1, dm_concat = 0,\n",
    "                   dm_mean = 1, negative = 7, hs = 0, epoch = 20, window = 10,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'ct')\n",
    "pprint(d2v_model.most_similar('문재인/Noun'))\n",
    "pprint(d2v_model.most_similar('노무현/Noun'))\n",
    "pprint(d2v_model.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size : Dimensionality of the feature vectors \n",
    "* dm : 0 - distributed bag of words (PV-DBOW)\n",
    "* dm_concat : 0 - don't use concatenation of context vectors rather than sum/average  \n",
    "* dm_mean : 0 - don't use the sum of the context word vectors  \n",
    "> dm is used in non-concatenative mode.\n",
    "* negative : 7 - neative specifies how many 'noise words' should be drawn.\n",
    "* hs : 0 - hierarchical softmax 사용여부\n",
    "* window : 5 - The maximum distance between the current and predicted word within a sentence.  \n",
    "* alpha : the initial learning rate  \n",
    "* min_alpha : learning rate will linearly drop to min_alpha as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# PV - DBOW\n",
    "d2v_model = Make_Doc2Vec_Model(data=train, size = 1000, dm = 0, dm_concat = 0,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = None,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'ct')\n",
    "pprint(d2v_model.most_similar('문재인/Noun'))\n",
    "pprint(d2v_model.most_similar('노무현/Noun'))\n",
    "pprint(d2v_model.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test\n",
    "del d2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Doc2Vec Using tagger mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab\n",
    "def tokenize2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 없을 때\n",
    "raw_doc_mecab = [(tokenize2(rawdata.loc[idx][0]), [idx], rawdata.loc[idx][1]) for idx in tqdm(rawdata.index)]\n",
    "pickle.dump(raw_doc_mecab, open('./data/pre_data/tagged_data/pre_data_by_mecab_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec 기본 포맷으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 없을 때\n",
    "\n",
    "tagged_mecab = [TaggedDocument(b, c, [d]) for b, c, d in tqdm(raw_doc_ct)]\n",
    "pickle.dump(tagged_mecab, open('./data/pre_data/tagged_data/pre_by_mecab_data_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw_doc_mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 있을 때\n",
    "tagged_mecab = pickle.load(open('./data/pre_data/tagged_data/pre_by_mecab_data_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dataset & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 없을 때\n",
    "train2, test2 = train_test_split(tagged_mecab, test_size=0.1, random_state=42)\n",
    "del tagged_ct\n",
    "pickle.dump(train2, open('./data/pre_data/train_test_Data/pre_by_mecab_train.pickled','wb'))\n",
    "pickle.dump(test2, open('./data/pre_data/train_test_Data/pre_by_mecab_test.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장된 파일이 있을 때\n",
    "train2 = pickle.load(open('./data/pre_data/train_test_Data/pre_by_mecab_train.pickled','rb'))\n",
    "test2 = pickle.load(open('./data/pre_data/train_test_Data/pre_by_mecab_test.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size : Dimensionality of the feature vectors \n",
    "* dm : 1 - distibuted memory (PV-DM)  \n",
    "* dm_concat : 1 - use concatenation of context vectors rather than sum/average  \n",
    "* dm_mean : 0 - don't use the sum of the context word vectors  \n",
    "> dm is used in non-concatenative mode.\n",
    "* negative : 7 - neative specifies how many 'noise words' should be drawn.\n",
    "* hs : 0 - hierarchical softmax 사용여부\n",
    "* window : 5 - The maximum distance between the current and predicted word within a sentence.  \n",
    "* alpha : the initial learning rate  \n",
    "* min_alpha : learning rate will linearly drop to min_alpha as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM W/\n",
    "d2v_model = Make_Doc2Vec_Model(data=train2, size = 1000, dm = 1, dm_concat = 1,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = 5,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'mecab')\n",
    "pprint(d2v_model.most_similar('문재인/NNP'))\n",
    "pprint(d2v_model.most_similar('노무현/NNP'))\n",
    "pprint(d2v_model.most_similar('박근혜/NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size : Dimensionality of the feature vectors \n",
    "* dm : 1 - distibuted memory (PV-DM)  \n",
    "* dm_concat : 0 - don't use concatenation of context vectors rather than sum/average  \n",
    "* dm_mean : 1 - don't use the sum of the context word vectors  \n",
    "> dm is used in non-concatenative mode.\n",
    "* negative : 7 - neative specifies how many 'noise words' should be drawn.\n",
    "* hs : 0 - hierarchical softmax 사용여부\n",
    "* window : 10 - The maximum distance between the current and predicted word within a sentence.  \n",
    "* alpha : the initial learning rate  \n",
    "* min_alpha : learning rate will linearly drop to min_alpha as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DBOW\n",
    "d2v_model = Make_Doc2Vec_Model(data=train2, size = 1000, dm = 1, dm_concat = 0,\n",
    "                   dm_mean = 1, negative = 7, hs = 0, epoch = 20, window = 10,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'mecab')\n",
    "pprint(d2v_model.most_similar('문재인/NNP'))\n",
    "pprint(d2v_model.most_similar('노무현/NNP'))\n",
    "pprint(d2v_model.most_similar('박근혜/NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size : Dimensionality of the feature vectors \n",
    "* dm : 0 - distributed bag of words (PV-DBOW)\n",
    "* dm_concat : 0 - don't use concatenation of context vectors rather than sum/average  \n",
    "* dm_mean : 0 - don't use the sum of the context word vectors  \n",
    "> dm is used in non-concatenative mode.\n",
    "* negative : 7 - neative specifies how many 'noise words' should be drawn.\n",
    "* hs : 0 - hierarchical softmax 사용여부\n",
    "* window : 5 - The maximum distance between the current and predicted word within a sentence.  \n",
    "* alpha : the initial learning rate  \n",
    "* min_alpha : learning rate will linearly drop to min_alpha as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#PV-DM w/\n",
    "d2v_model = Make_Doc2Vec_Model(data=train2, size = 1000, dm = 0, dm_concat = 0,\n",
    "                   dm_mean = 0, negative = 7, hs = 0, epoch = 20, window = None,\n",
    "                   alpha = 0.025, min_alpha = 0.025, workers = cores, tagger = 'mecab')\n",
    "pprint(d2v_model.most_similar('문재인/NNP'))\n",
    "pprint(d2v_model.most_similar('노무현/NNP'))\n",
    "pprint(d2v_model.most_similar('박근혜/NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train2\n",
    "del test2\n",
    "del d2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
