{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 데이터에 대한 통계 분석\n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 1월 1일까지 32일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import sys\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import html\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import functools\n",
    "from konlpy.tag import Mecab\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    binList = []\n",
    "    collection = 'newsDaum'\n",
    "elif site.lower() == 'naver':\n",
    "    binList = []\n",
    "    collection = 'newsNaver'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "useCollection = dh.Use_Collection(useDb, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "def tokenize_pos2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None, encoding='utf-8')\n",
    "rawdata.head()\n",
    "X = list(rawdata[0])\n",
    "y = np.array(list(rawdata[1]), dtype=int)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mecab_log1 = Pipeline([\n",
    "            ('vect', TfidfVectorizer(tokenizer=tokenize_pos2, ngram_range=(1,2))), \n",
    "            ('log', LogisticRegression()),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model_mecab_log1.fit(train_X, train_y)\n",
    "print(classification_report(test_y, model_mecab_log1.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_mecab_log1, open('./data/pre_data/classifier_log_classifier_ngram12_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mecab_log1 = pickle.load(open('./data/pre_data/classifier_log_classifier_ngram12_by_mecab.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load('./model/doc2vec_size2000_epoch20_by_mecab.model')\n",
    "classifier = pickle.load(open('./data/pre_data/classifier_by_mecab_from_doc2vec_size2000_epoch20.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주의 \n",
    "> gensim의 doc2vec 모델의 infer_vector 함수를 이용해 벡터를 생성하면, 같은 단어열(문서)에 대해서도 여러번 호출하면 각기 다른 벡터가 나온다  \n",
    "> 이는 문서 벡터 추론 과정에서 랜덤하게 초기화하는 과정이 있기 때문인데, 문제는 결과로 나오는 벡터의 차이가 꽤 크다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cls(text, model, num):\n",
    "    mecab = Mecab()\n",
    "    def tokenize_pos2(doc):\n",
    "        return ['/'.join(t) for t in mecab.pos(doc)]\n",
    "    TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "    x2 = TaggedDocument(tokenize_pos2(text), 0.5)\n",
    "    out = list()\n",
    "    for idx in range(num):\n",
    "        x = model.infer_vector(x2.words)\n",
    "        cl = classifier.predict(x.reshape(1,-1))\n",
    "        out.append(cl)\n",
    "    out = list(itertools.chain.from_iterable(out))\n",
    "    cls2 = int(round(sum(out)/len(out)))\n",
    "    return cls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsDict = pickle.load(open('./data/pre_data/keywords_daum.pickled','rb'))\n",
    "#keywordsDict = pickle.load(open('./data/pre_data/keywords_Naver.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "CursorNotFound",
     "evalue": "Cursor not found, cursor id: 425730656959",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCursorNotFound\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8249b834170b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutputDict\u001b[0m  \u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcollection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0museCollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'site'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msite\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0midis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ObjectId__id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'. '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mainText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1132\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__manipulate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m                 \u001b[0m_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1073\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__codec_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1075\u001b[1;33m                                              self.__max_await_time_ms))\n\u001b[0m\u001b[0;32m   1076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Cursor id is zero nothing else to return\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36m__send_message\u001b[1;34m(self, operation)\u001b[0m\n\u001b[0;32m    945\u001b[0m                                            codec_options=self.__codec_options)\n\u001b[0;32m    946\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfrom_command\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                 \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_command_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__killed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymongo\\helpers.py\u001b[0m in \u001b[0;36m_check_command_response\u001b[1;34m(response, msg, allowable_errors, parse_write_concern_error)\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mExecutionTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m43\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mCursorNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"%s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCursorNotFound\u001b[0m: Cursor not found, cursor id: 425730656959"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "outputDict  =dict()\n",
    "collection = useCollection.find({'site':site})\n",
    "for data in collection:\n",
    "    idis = data['_id']._ObjectId__id.hex()\n",
    "    text = data['title']+'. '+data['mainText']    \n",
    "    commentCollection = dh.Use_Collection(useDb, 'comments')\n",
    "    comments = commentCollection.find({'site':site, 'category':data['category'], 'date':data['date'], 'rank':data['rank'] })\n",
    "    commEstimation = list()\n",
    "    for comment in comments:\n",
    "        comm = comment['comments']\n",
    "        commEstimation.append(Cls(comm, model, 30))\n",
    "    cls = sum(commEstimation)/ len(commEstimation)    \n",
    "    outputDict[idis] = tmpdict = dict()\n",
    "    tmpdict['title'] = data['title']\n",
    "    tmpdict['maintextestimation'] = Cls(text, model, 50)\n",
    "    tmpdict['keywordsindaum'] = data['keywords']\n",
    "    tmpdict['analyzedkeywords'] = keywordsDict[idis]\n",
    "    tmpdict['number_crawledcomments'] = comments.count()\n",
    "    tmpdict['commentsestimation'] = cls\n",
    "    tmpdict['press'] = data['press']\n",
    "    tmpdict['number_of_comment'] = data['number_of_comment']\n",
    "pickle.dump(outputDict, open('./outcome/outcome_daum_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용자 사전을 만들고, 사전작업을 하는데 너무 많은 시간을 소비하였다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(outputDict, open('./outcome/outcome_daum_by_mecab.pickled','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
