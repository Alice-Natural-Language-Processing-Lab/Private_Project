{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 1384828, 5532]\n",
      "[('이', 'Noun'), ('것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n",
      "best: [('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import html\n",
    "\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Twitter, Mecab,Kkma\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "\n",
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "mecab = Mecab()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "print (list(map(lambda x: len(xxxx[x]), xxxx)))\n",
    "\n",
    "score_weights = {\n",
    "    'num_nouns': -0.1,\n",
    "    'num_words': -0.2,\n",
    "    'no_noun': -1\n",
    "}\n",
    "\n",
    "def my_score(candidate):\n",
    "    num_nouns = len([w for w,t in candidate if t == 'Noun'])\n",
    "    num_words = len(candidate)\n",
    "    no_noun = 1 if num_nouns == 0 else 0\n",
    "    \n",
    "    score = (num_nouns * score_weights['num_nouns'] \n",
    "             + num_words * score_weights['num_words']\n",
    "             + no_noun * score_weights['no_noun'])\n",
    "    return score\n",
    "\n",
    "ct.set_selector(score_weights, my_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt','r',encoding='utf-8').readlines() + open('./data/newspress.txt','r',encoding = 'utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "[인용사이트](http://excelsior-cjh.tistory.com/entry/TextRank%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%AC%B8%EC%84%9C%EC%9A%94%EC%95%BD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.twitter = ot\n",
    "        self.ctwitter = ct\n",
    "        self.mecab = mecab\n",
    "        \n",
    "        #self.stopwords = []\n",
    "        self.stopwords = stopwords\n",
    "    def text2sentences(self, text):\n",
    "        sentences = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.ctwitter.phrases(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.ctwitter.nouns(str(sentence))\n",
    "                                                  if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.ctwitter.morphs(str(sentence))\n",
    "                                                   if noun not in self.stopwords and len(noun) > 1]))\n",
    "                #nouns.append(' '.join([noun for noun in self.mecab.nouns(str(sentence))\n",
    "                #                                  if noun not in self.stopwords and len(noun) > 1]))\n",
    "                #nouns.append(' '.join([noun for noun in self.mecab.phrases(str(sentence))\n",
    "                #                                  if noun not in self.stopwords and len(noun) > 1]))\n",
    "                #nouns.append(' '.join([noun for noun in self.mecab.morphs(str(sentence))\n",
    "                #                                   if noun not in self.stopwords and len(noun) > 1]))\n",
    "        nouns = list(set(nouns))\n",
    "        return nouns\n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(ngram_range=(1,5),sublinear_tf=True, lowercase=False,min_df=0.000001)\n",
    "        self.cnt_vec = CountVectorizer(ngram_range=(1,5), lowercase=False,min_df=0.000001)\n",
    "        self.graph_sentence = []\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        #tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = sknorm(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "class TextRankX(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "path = './data/Keyword_outcome/ckonlpy-naver.*'\n",
    "extracted_file = glob(path)\n",
    "extracted = dict()\n",
    "for file in extracted_file:\n",
    "    ex = pickle.load(open(file,'rb'))\n",
    "    extracted.update(ex)\n",
    "outlen = str(len(extracted_file))\n",
    "print (len(extracted))\n",
    "outname = './data/Keyword_outcome/ckonlpy-naver.pickled_'+outlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "extract_keywords_dict = dict()\n",
    "site = 'Naver'\n",
    "#site = 'daum'\n",
    "if site == 'daum':\n",
    "    binList = []\n",
    "    collection = 'newsDaum'\n",
    "elif site.lower() == 'naver':\n",
    "    binList = []\n",
    "    collection = 'newsNaver'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, collection)\n",
    "keywordList = list()\n",
    "collection = useCollection.find({'site':site})\n",
    "count = 0\n",
    "for data in collection:\n",
    "    idIs = data['_id']._ObjectId__id\n",
    "    tis2 = html.unescape(data['title'])+'. '+html.unescape(data['mainText'])\n",
    "    if data['keywords'] !='NaN':\n",
    "        keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "    else:\n",
    "        pass\n",
    "    if not idIs.hex() in extracted:\n",
    "        try:\n",
    "            workrank = TextRankX(tis2)\n",
    "        except:\n",
    "            try:\n",
    "                workrank = TextRankX(tis2)\n",
    "            except:\n",
    "                workrank = TextRankX(tis2)\n",
    "                break\n",
    "        extract_keywords_dict[idIs.hex()]= workrank.keywords()\n",
    "        if count == 1500:\n",
    "            break\n",
    "        print (count,)\n",
    "        count +=1\n",
    "pickle.dump(extract_keywords_dict, open(outname,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(extract_keywords_dict, open(outname,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
