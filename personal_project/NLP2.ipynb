{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = open('./data/wiki_pos_tokenizer_with_taginfo.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listis = []\n",
    "while 1:\n",
    "    line = x.readline()\n",
    "    if not line:break\n",
    "    c = line.split()\n",
    "    listis += list(map(lambda x: x.split('/')[0], c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = []\n",
    "for rawdata in useCollection.find({'site':'daum'}):\n",
    "    if rawdata['keywords'] != 'NaN':\n",
    "        keywords += rawdata['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reKeywords = []\n",
    "for i in keywords:\n",
    "    if '·' in i:\n",
    "        if '이슈' in i:\n",
    "            reKeywords.append(i.split('·')[1].strip())\n",
    "        else:\n",
    "            reKeywords.append(i.strip())\n",
    "    else:\n",
    "        reKeywords.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger.add_dictionary(reKeywords, 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "sys.path.append('/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/')\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import pickle\n",
    "import urllib3\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Mecab, Twitter\n",
    "from konlpy.utils import pprint\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from __future__ import print_function, unicode_literals\n",
    "from konlpy.utils import pprint\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import itertools\n",
    "def OpenAPI():\n",
    "    import pickle\n",
    "    openapi = pickle.load(open('./etri_open_api_access_key', 'rb'))\n",
    "    return openapi\n",
    "def USE_ETRI_ANALYSIS(analysisCode, text):    \n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    requestJson = {\n",
    "        'access_key' : accessKey,\n",
    "        'argument' : {\n",
    "            'text' : text,\n",
    "            'analysis_code' : analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "    return json.loads(str(response.data,'utf-8'))\n",
    "\n",
    "def USE_ETRI_INFO(fWord, fSenseId, sWord, sSenseId):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/WordRel\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    firstWord = fWord\n",
    "    firstSenseId = fSenseId\n",
    "    secondWord = sWord\n",
    "    secondSenseId = sSenseId\n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "        'first_word': firstWord,\n",
    "        'first_sense_id': firstSenseId,\n",
    "        'second_word': secondWord,\n",
    "        'second_sense_id': secondSenseId\n",
    "        }}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "    body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8'))\n",
    "\n",
    "def USE_ETRI_RELATION(word):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/Word\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    word = word\n",
    "    requestJson = {\n",
    "    \"access_key\": accessKey,\n",
    "    \"argument\": {\"word\": word}}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "                            body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8'))\n",
    "\n",
    "def Extract_Text_Info(idx):\n",
    "    morp = list(map(lambda x: (x['lemma'], x['type']),idx['morp']))\n",
    "    morp = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNB','SL','SH','NH'], morp))\n",
    "    wsd = list(map(lambda x: (x['text'], x['type']),idx['WSD']))\n",
    "    wsd = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], wsd))\n",
    "    word = list(map(lambda x: (x['text'], x['type']),idx['word']))\n",
    "    ne = list(map(lambda x: (x['text'], x['type']),idx['NE']))\n",
    "    return morp, wsd, word, ne\n",
    "\n",
    "def TfidFeatures(pos,stopwords):\n",
    "    posList = list(map(lambda x: x[0], pos))\n",
    "    vect = TfidfVectorizer(stop_words=stopwords).fit(posList)\n",
    "    count = vect.transform(posList).toarray().sum(axis = 0)\n",
    "    idx = np.argsort(-count)\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]\n",
    "    #feature_pos = np.array(list(map(lambda x:x[1], pos)))[idx]\n",
    "    x = list(zip(feature_name, count))[:3]\n",
    "    x = pd.DataFrame(x)\n",
    "    x.set_index(0, inplace=True)\n",
    "    return x\n",
    "def Analysis(stopwordsList, text):\n",
    "    #twitter_out = Twitter().pos(text)\n",
    "    #mecab_out = Mecab().pos(text)\n",
    "    #mecab_out = list(filter(lambda x: x[1] in ['NNB', 'NNP','NNG','SL','SH'], mecab_out))\n",
    "    #twitter_out = list(filter(lambda x: x[1] in ['Foreign','Unknown','Noun'], twitter_out))\n",
    "    etri = USE_ETRI_ANALYSIS('srl', text)['return_object']    \n",
    "    etri = etri['sentence']\n",
    "    morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = []\n",
    "    for i in etri:\n",
    "        x = Extract_Text_Info(i)\n",
    "        morp += x[0]\n",
    "        wsd += x[1]\n",
    "        word +=x[2]\n",
    "        ne += x[3]\n",
    "        \n",
    "    return morp, wsd, word, ne\n",
    "    #eriOut_srl = TfidFeatures(srl, stopwords)\n",
    "    #etriOut_morp = TfidFeatures(morp, stopwords)\n",
    "    #mecabOut = TfidFeatures(mecab_out, stopwords)\n",
    "    #twitterOut = TfidFeatures(twitter_out, stopwords)\n",
    "    #etriOut_ne = TfidFeatures(ne, stopwords)\n",
    "    #etriOut_wsd = TfidFeatures(wsd, stopwords)\n",
    "    #etriOut_word = TfidFeatures(word, stopwords)\n",
    "    #k = []\n",
    "    #for i in etri:\n",
    "    #    xx = i['dependency']\n",
    "    #    k +=list(map(lambda x: (x['text'], x['label']), xx))\n",
    "    #k = list(itertools.chain.from_iterable(list(map(lambda x: Twitter().pos(x[0]), k))))\n",
    "    #k = list(filter(lambda x: x[1] in ['Foreign','Unknown','Noun'], k))\n",
    "    #k = TfidFeatures(k, stopwords)\n",
    "    #z = pd.concat([etriOut_morp, etriOut_ne,etriOut_word, mecabOut, twitterOut, k],axis = 1)\n",
    "    #z1 = z.loc[:,z.columns.values==1].sum(axis = 1).sort_values()[-5:]\n",
    "    #return etriOut_morp,eriOut_srl, etriOut_ne, etriOut_wsd, etriOut_word, mecabOut, twitterOut, k, z1\n",
    "\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunLRNounExtractor(text):\n",
    "    verbose=False\n",
    "    left_max_length=8\n",
    "    right_max_length=4\n",
    "    min_count=5\n",
    "    word_extractor = WordExtractor()\n",
    "    noun_extractor = LRNounExtractor(None, \n",
    "                                 verbose, \n",
    "                                 left_max_length, \n",
    "                                 right_max_length, \n",
    "                                 min_count, \n",
    "                                 word_extractor)\n",
    "    sentences = Sentences(text)\n",
    "    nouns = noun_extractor.train_extract(sentences, minimum_noun_score=0.5, min_count=50)\n",
    "    return nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter as ctwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.hangle import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer, RegexTokenizer\n",
    "from soynlp.noun import LRNounExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sentences:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.length = 0\n",
    "    def __iter__(self):\n",
    "            for doc in self.text.split('. '):\n",
    "                doc = doc.strip()\n",
    "                if not doc:\n",
    "                    continue\n",
    "                for sent in doc.split(' '):\n",
    "                    yield sent\n",
    "    def __len__(self):\n",
    "        if self.length == 0:\n",
    "            for doc in self.text.split('. '):\n",
    "                doc = doc.strip()\n",
    "                if not doc:\n",
    "                    continue\n",
    "                self.length += len(doc.split(' '))\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = Stopwords('./data/koreanStopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, 'newsDaum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5a2af415588c1376f0d67afe'),\n",
       " 'category': '뉴스',\n",
       " 'date': '2017.12.06',\n",
       " 'keywords': ['당뇨병', '탄수화물', '식이섬유'],\n",
       " 'link': 'http://v.media.daum.net/v/20171206100855949',\n",
       " 'mainText': \" 사람들마다 식사를 할 때 밥과 반찬을 먹는 순서가 다르다. 밥부터 먼저 먹는 사람이 있는가 하면, 반찬을 먼저 먹은 후에 밥을 먹는 식이다. 그런데 식사를 할 때 무엇을 먼저 먹는지에 따라 살이 빠지고, 당뇨병까지 예방할 수 있다. 방법은 쉽다. 본인의 한 끼 식사를 영양소별로 나눠 먹는 순서를 정하고, 정한 순서대로 섭취해 살을 빼는 다이어트법이다. 우선 식단 내 영양소를 식이섬유, 단백질, 탄수화물, 크게 세 가지로 나눈다. 식이섬유가 많은 식품에는 채소·과일, 단백질이 많은 식품에는 고기·생선, 탄수화물이 많은 식품에는 쌀·보리 같은 곡류가 있다. 이때 순서를 식이섬유→단백질→탄수화물 순으로 음식을 섭취하면 된다. 예를 들어 일반적인 한식을 먹는다고 하면 나물과 김치를 먼저 먹은 후 다음에 국이나 찌개를 먹는다. 그런 후에 육류나 생선류를 먹고, 마지막으로 밥을 먹는다. 먹는 순서를 식이섬유→단백질→탄수화물 순으로 바꾸면 혈당이 급격하게 상승하는 것을 막고, 빠른 포만감을 준다. 또한 식이섬유부터 먹으면, 단백질·탄수화물이 천천히 흡수돼 혈당이 크게 상승하지 못하게 한다. 혈당이 높아지면 모두 에너지로 쓰이지 못하고 남으면서 지방으로 바뀌고, 비만으로 이어진다. 또 식이섬유를 먼저 먹으면 포만감이 잘 들어, 이후 먹는 단백질과 탄수화물 섭취량이 줄어든다. 특히 혈당을 많이 높이는 탄수화물 섭취를 막아 효과적이다. 실제로 쌀밥을 먹기 전에 생선이나 육류를 먼저 섭취하면 혈당이 급격히 높아지는 것을 억제할 수 있다는 연구결과도 있다. 간사이전력 의학연구소의 야베 다이스케 부소장 등 연구팀은 제2형 당뇨병 환자 12명과 건강한 사람 10명을 대상으로 쌀밥을 먼저 먹은 경우와 생선(고등어 졸임)이나 육류(소고기 석쇠 구이)를 쌀밥을 먹기 15분 전에 먹은 경우로 나누어 각각 4시간 후 혈당치를 조사했다. 그 결과, 당뇨병 환자나 건강한 사람 모두 혈당치 상승폭이 '쌀밥을 먼저' 먹은 경우보다 '생선을 먼저' 먹은 쪽이 약 30%, '육류를 먼저' 먹은 쪽은 약 40% 낮았다. '생선을 먼저' 먹은 쪽과 '육류를 먼저' 먹은 쪽 모두 소화에 관여하는 호르몬인 인르레틴(incretin)이 식사 30분 후에 약 2배 더 많이 분비된 것으로 나타났다. 인크레틴은 음식을 먹으면 췌장을 자극해 혈당을 낮추는 인슐린 분비 양을 증가시키며, 인슐린과 반대 작용하는 글루카곤은 억제하는 역할을 한다. 이러한 영향으로 위의 움직임이 느려져 쌀이 소장에서 흡수될 때까지 걸리는 시간이 약 3배 길어진 것으로 나타났다. 미국 코넬대에서도 먹는 순서 다이어트와 비슷한 '거꾸로 식사법'과 관련한 결과를 발표한 바 있다. 거꾸로 식사법은 후식→밥·반찬 순으로 식사를 하는 것인데, 역시 식이섬유를 먼저 섭취하고 이후 단백질·탄수화물을 섭취한다는 공통점이 있다. 브라이언 완싱크 교수가 이끌었던 코넬대 연구팀은 남녀 124명을 대상으로 과일을 먼저 먹는 그룹(1그룹)과 계란과 베이컨 등을 먼저 먹는 테이블(2그룹)과 나눠서 식사하게 했다. 실험 결과 1그룹이 2그룹보다 칼로리를 적게 섭취하고 지방이 많고 튀긴 음식에 대한 유혹을 덜 느꼈다.\",\n",
       " 'number_of_comment': 596,\n",
       " 'press': '헬스조선',\n",
       " 'rank': '10',\n",
       " 'real_number_of_comment': 485,\n",
       " 'site': 'daum',\n",
       " 'title': '반찬 먹는 순서만 바꿔도 살이 빠지고 당뇨병이 예방된다?'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = useCollection.find_one({'category':\"뉴스\"})\n",
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useCollection2 = dh.Use_Collection(useDb, 'newsNaver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5a303059588c135900450b14'),\n",
       " 'category': '정치',\n",
       " 'date': '2017.12.12',\n",
       " 'link': 'http://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=003&aid=0008336872&date=20171212&type=1&rankingSectionId=100&rankingSeq=29',\n",
       " 'mainText': '【서울=뉴시스】김진아 기자 = 삼성그룹의 한국동계스포츠영재센터(영재센터) 후원을 강요한 혐의 등으로 재판에 넘겨진 \\'비선실세\\' 최순실씨의 조카 장시호씨가 6일 오후 서울 서초구 서울중앙지방법원에서 열린 1심 선고 공판에서 징역 2년6월을 선고받은 후 법원을 나서고 있다. 2017.12.06. bluesoda@newsis.com   법정 구속후 첫 출석…감정 북받쳐 동요 \"崔, 영재센터 3차 후원·EBS 인사 개입도\"  【서울=뉴시스】이혜원 기자 = 최순실(61)씨가 지난해 10월 국정농단 사태가 불거지기 직전 삼성에 한국동계스포츠영재센터 3차 후원을 요구하려 했다는 법정 증언이 나왔다.  11일 서울고법 형사13부(부장판사 정형식) 심리로 열린 이재용(49) 삼성전자 부회장 등 삼성그룹 전·현직 임원 5명의 뇌물공여 등 혐의 항소심 12차 공판에서 최씨의 조카 장시호(38)씨가 증인으로 나와 이 같은 정황을 밝혔다.  장씨의 진술에 따르면 이규혁(39) 전 영재센터 전무는 지난해 10월 중순께 삼성전자에서 영재센터 추가 후원을 해줄 수 있는지 문의하기 위해 이영국 제일기획 상무에게 연락했다. 이에 이 상무는 일주일 뒤 만나자고 했지만, 결국 만남은 불발됐다.  장씨는 \"최씨의 지시에 따라 이 전 전무가 삼성에 연락한 것이냐\"는 질문에 \"그렇다\"라며 \"최씨가 다음연도 예산안을 보내라고 했다\"고 진술했다.  이어 특검이 \"국정농단 사태가 언론을 통해 드러나기 직전에 최씨가 영재센터 3차 후원을 받으려고 한 것이냐\"고 묻자 \"아무래도 내년도 예산안을 보내라고 했으니 (그러지 않았겠냐)\"라고 설명했다.  장씨는 최씨가 영재센터 자금 3억원을 추가로 빼돌리려 한 정황도 털어놨다.  앞서 장씨는 영재센터 자금 2억2000억원을 자신이 차명으로 운영한 스포츠마케팅 회사인 더스포츠엠으로 빼돌린 혐의 등으로 기소돼 1심에서 징역 2년6개월의 실형을 선고받았다.  장씨는 \"최씨의 지시로 영재센터 자금 2억2000만원을 더스포츠엠에 송금했다\"며 \"최씨가 영재센터 자금으로 직원들 급여를 얼마로 지출할지 지시하기도 했다\"고 증언했다.  이어 \"최씨가 영재센터 자금 3억원을 인출해 통장으로 만들어오라고 지시한 적도 있냐\"는 특검의 질문에 \"그렇다\"고 답하며 \"현금으로 3억원을 주면 어떻게 증빙할지 문제가 될 것 같아, 독일로 돈을 보낸 뒤 다시 돌려받는 방안을 최씨가 일러주기도 했다\"고 설명했다.  【서울=뉴시스】임태훈 기자 = \\'국정농단 정점\\' 최순실 씨가 8일 오전 서울 서초구 서울중앙지방법원에서 열린 \\'592억 뇌물\\' 관련 93회 공판에 출석하기 위해 법정으로 향하고 있다. 2017.12.08. taehoonlim@newsis.com   장씨는 또 최씨가 우종범 전 EBS 사장의 인사에 개입했으며, 이로 인해 EBS가 영재센터에 후원하게 됐다고도 증언했다.  장씨는 \"영재센터가 EBS에서 후원받은 경위에 대해 아는 게 있냐\"는 특검의 질문에 \"최씨의 추천을 받은 분이 EBS 사장이 된 것으로 안다\"며 \"추천받은 분이 어쩔 수 없이 후원사로 들어온 것으로 알고 있다\"고 떠올렸다.  이어 \"EBS가 (영재센터 관련) 방송도 단독으로 내보낸 것으로 안다\"고 답했다. 특검이 사장이 누구인지 묻자 장씨는 \"우종범 사장\"이라고 말했다.  한편 지난 6일 자신의 1심 재판에서 실형을 선고받은 이후 처음으로 법정에 나온 장씨는 증언 내내 북받치는 감정을 참아내지 못했다.  장씨는 증언을 시작하기에 앞서 선서를 하라는 재판부의 안내에도 고개를 떨군 채 1분여간 입을 떼지 못했다. 눈가를 손가락으로 닦아내기도 했다.  장씨는 이어 눈물에 잠긴 목소리로 \"양심에 따라 숨김과 보탬이 없이 사실 그대로 말하겠다\"며 선서를 겨우 마쳤다. 장씨는 이후 증언 중에도 눈물을 삼키며 신문에 답하는 모습을 보였다.  hey1@newsis.com  ▶ 뉴시스 빅데이터 MSI 주가시세표 바로가기 ▶ 네이버 채널에서 뉴시스를 구독해주세요  <저작권자 공감언론 뉴시스통신사. 무단전재-재배포 금지.>',\n",
       " 'number_of_comment': 1215,\n",
       " 'press': '뉴시스',\n",
       " 'rank': 29,\n",
       " 'real_number_of_comment': 993,\n",
       " 'site': 'Naver',\n",
       " 'title': \"[종합]장시호, 실형 후 첫 법정서 눈물…'최순실 혐의' 증언\"}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata2 = useCollection2.find_one({'category':\"정치\"})\n",
    "rawdata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "ct = ctwitter()\n",
    "ot = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used noun_predictor_sejong\n",
      "['이름'] ['노홍철', '한명회', '한혜진', '종영', '편성시간'] ['이름', '한명회', '시간', '회', '출연자']\n"
     ]
    }
   ],
   "source": [
    "for raw in useCollection.find({'site':'daum'}):\n",
    "    if len(raw['mainText'].strip()) !=0:\n",
    "        x = RunLRNounExtractor(raw['mainText'])\n",
    "        k = list(filter(lambda x: x[1]=='Noun',ct.pos(raw['mainText'])))\n",
    "        t =list(map(lambda x: x[0][0], Counter(k).most_common(n=5)))\n",
    "        print (list(x.keys()), raw['keywords'], t)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "etri = Analysis(stopwords, rawdata['mainText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "et0 = list(map(lambda x: x[0], etri[0]))\n",
    "et1 = list(map(lambda x: x[0], etri[1]))\n",
    "et2 = list(map(lambda x: x[0], etri[2]))\n",
    "et3 = list(map(lambda x: x[0], etri[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mcout = list(map(lambda x: x[0], mecab.pos(rawdata['mainText'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctout = list(map(lambda x: x[0], ct.pos(rawdata['mainText'])))\n",
    "otout = list(map(lambda x: x[0], ot.pos(rawdata['mainText'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer().fit(ctout+otout+mcout+et0+et1+et2+et3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [ctout, otout, mcout, et0, et1, et2, et3]\n",
    "outdict = dict()\n",
    "for i in y:\n",
    "    count = vect.transform(et3).toarray().sum(axis = 0)\n",
    "    idx = np.argsort(-count)\n",
    "    count = count[idx]\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]\n",
    "    out = dict(zip(feature_name, count))\n",
    "    for ii in out:\n",
    "        if not ii in outdict:\n",
    "            outdict[ii] = out[ii]\n",
    "        else:\n",
    "            outdict[ii] +=out[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('탄수화물', 56.0),\n",
       " ('단백질', 49.0),\n",
       " ('생선', 42.0),\n",
       " ('육류', 28.0),\n",
       " ('쌀밥', 28.0),\n",
       " ('당뇨병', 21.0),\n",
       " ('인슐린', 14.0),\n",
       " ('1그룹', 14.0),\n",
       " ('코넬대', 11.37555212392725),\n",
       " ('인크레틴', 7.0),\n",
       " ('고등어', 7.0),\n",
       " ('비만', 7.0),\n",
       " ('부소장', 7.0),\n",
       " ('교수', 7.0),\n",
       " ('구이', 7.0),\n",
       " ('베이컨', 7.0),\n",
       " ('찌개', 7.0),\n",
       " ('고기', 7.0),\n",
       " ('김치', 7.0),\n",
       " ('나물', 7.0),\n",
       " ('제2형', 7.0),\n",
       " ('미국', 7.0),\n",
       " ('인르레틴', 7.0),\n",
       " ('보리', 7.0),\n",
       " ('계란', 7.0),\n",
       " ('췌장', 7.0),\n",
       " ('30', 7.0),\n",
       " ('4시간', 7.0),\n",
       " ('2배', 7.0),\n",
       " ('가지', 7.0),\n",
       " ('2그룹', 7.0),\n",
       " ('40', 7.0),\n",
       " ('한식', 7.0),\n",
       " ('소고기', 7.0),\n",
       " ('124명', 7.0),\n",
       " ('소장', 7.0),\n",
       " ('15분', 7.0),\n",
       " ('12명', 7.0),\n",
       " ('후식', 7.0),\n",
       " ('30분', 7.0),\n",
       " ('10명', 7.0),\n",
       " ('3배', 7.0),\n",
       " ('환자', 7.0),\n",
       " ('연구팀', 5.4639311498952612),\n",
       " ('식사법', 5.1847658232184042),\n",
       " ('완싱크', 5.1288462926465961),\n",
       " ('야베', 5.0598861269831925),\n",
       " ('의학연구소', 4.9497474683058318),\n",
       " ('간사이전력', 4.9497474683058318),\n",
       " ('다이스케', 4.8371016509851241),\n",
       " ('브라이언', 4.7639202036143793),\n",
       " ('거꾸로', 4.7029994002111435),\n",
       " ('예방', 0.0),\n",
       " ('영양', 0.0),\n",
       " ('연구소', 0.0),\n",
       " ('영향', 0.0),\n",
       " ('영양소', 0.0),\n",
       " ('소화', 0.0),\n",
       " ('예방할', 0.0),\n",
       " ('연구', 0.0),\n",
       " ('우선', 0.0),\n",
       " ('움직임', 0.0),\n",
       " ('유혹', 0.0),\n",
       " ('섭취하면', 0.0),\n",
       " ('으로', 0.0),\n",
       " ('으면', 0.0),\n",
       " ('으면서', 0.0),\n",
       " ('음식', 0.0),\n",
       " ('의학', 0.0),\n",
       " ('예방하', 0.0),\n",
       " ('역할', 0.0),\n",
       " ('억제', 0.0),\n",
       " ('에서도', 0.0),\n",
       " ('순서', 0.0),\n",
       " ('순서대로', 0.0),\n",
       " ('시간', 0.0),\n",
       " ('시키', 0.0),\n",
       " ('식단', 0.0),\n",
       " ('식사', 0.0),\n",
       " ('소별', 0.0),\n",
       " ('식사하', 0.0),\n",
       " ('식사하게', 0.0),\n",
       " ('식이', 0.0),\n",
       " ('섭취해', 0.0),\n",
       " ('역시', 0.0),\n",
       " ('식이섬유', 0.0),\n",
       " ('실제', 0.0),\n",
       " ('실제로', 0.0),\n",
       " ('실험', 0.0),\n",
       " ('싱크', 0.0),\n",
       " ('쓰이', 0.0),\n",
       " ('어서', 0.0),\n",
       " ('억제하', 0.0),\n",
       " ('에너지', 0.0),\n",
       " ('섭취한', 0.0),\n",
       " ('에는', 0.0),\n",
       " ('에서', 0.0),\n",
       " ('식품', 0.0),\n",
       " ('이끌', 0.0),\n",
       " ('10', 0.0),\n",
       " ('이나', 0.0),\n",
       " ('줄어들', 0.0),\n",
       " ('증가', 0.0),\n",
       " ('지방', 0.0),\n",
       " ('채소', 0.0),\n",
       " ('천천히', 0.0),\n",
       " ('치를', 0.0),\n",
       " ('칼로리', 0.0),\n",
       " ('코넬', 0.0),\n",
       " ('크게', 0.0),\n",
       " ('크레틴', 0.0),\n",
       " ('테이블', 0.0),\n",
       " ('튀기', 0.0),\n",
       " ('튀긴', 0.0),\n",
       " ('줄어든다', 0.0),\n",
       " ('특히', 0.0),\n",
       " ('포만감', 0.0),\n",
       " ('하게', 0.0),\n",
       " ('하고', 0.0),\n",
       " ('하는', 0.0),\n",
       " ('하면', 0.0),\n",
       " ('하지', 0.0),\n",
       " ('한다', 0.0),\n",
       " ('한다는', 0.0),\n",
       " ('혈당', 0.0),\n",
       " ('혈당치', 0.0),\n",
       " ('호르몬', 0.0),\n",
       " ('효과', 0.0),\n",
       " ('효과적', 0.0),\n",
       " ('포만', 0.0),\n",
       " ('줄어든', 0.0),\n",
       " ('준다', 0.0),\n",
       " ('졸임', 0.0),\n",
       " ('이다', 0.0),\n",
       " ('이때', 0.0),\n",
       " ('이러', 0.0),\n",
       " ('이러하', 0.0),\n",
       " ('이러한', 0.0),\n",
       " ('이어지', 0.0),\n",
       " ('이어진', 0.0),\n",
       " ('이어진다', 0.0),\n",
       " ('이후', 0.0),\n",
       " ('인데', 0.0),\n",
       " ('섭취하고', 0.0),\n",
       " ('일반', 0.0),\n",
       " ('일반적', 0.0),\n",
       " ('있는', 0.0),\n",
       " ('있다', 0.0),\n",
       " ('자극', 0.0),\n",
       " ('자극하', 0.0),\n",
       " ('자극해', 0.0),\n",
       " ('작용', 0.0),\n",
       " ('작용하', 0.0),\n",
       " ('작용하는', 0.0),\n",
       " ('적게', 0.0),\n",
       " ('전력', 0.0),\n",
       " ('정하', 0.0),\n",
       " ('정하고', 0.0),\n",
       " ('정한', 0.0),\n",
       " ('조사', 0.0),\n",
       " ('조사하', 0.0),\n",
       " ('조사했', 0.0),\n",
       " ('이끌었', 0.0),\n",
       " ('섭취하', 0.0),\n",
       " ('비슷하', 0.0),\n",
       " ('섭취', 0.0),\n",
       " ('길어지', 0.0),\n",
       " ('길어진', 0.0),\n",
       " ('까지', 0.0),\n",
       " ('나누', 0.0),\n",
       " ('나누어', 0.0),\n",
       " ('나눈', 0.0),\n",
       " ('나눈다', 0.0),\n",
       " ('나눠', 0.0),\n",
       " ('나눠서', 0.0),\n",
       " ('나타나', 0.0),\n",
       " ('나타났', 0.0),\n",
       " ('남녀', 0.0),\n",
       " ('남으', 0.0),\n",
       " ('낮았', 0.0),\n",
       " ('낮추', 0.0),\n",
       " ('낮추는', 0.0),\n",
       " ('높아지', 0.0),\n",
       " ('높아지는', 0.0),\n",
       " ('높이', 0.0),\n",
       " ('높이는', 0.0),\n",
       " ('느꼈', 0.0),\n",
       " ('느끼', 0.0),\n",
       " ('느려', 0.0),\n",
       " ('느려져', 0.0),\n",
       " ('느려지', 0.0),\n",
       " ('는가', 0.0),\n",
       " ('는다', 0.0),\n",
       " ('는다고', 0.0),\n",
       " ('는지', 0.0),\n",
       " ('급격히', 0.0),\n",
       " ('는지에', 0.0),\n",
       " ('급격하', 0.0),\n",
       " ('글루카곤', 0.0),\n",
       " ('12', 0.0),\n",
       " ('124', 0.0),\n",
       " ('15', 0.0),\n",
       " ('incretin', 0.0),\n",
       " ('ㄴ다', 0.0),\n",
       " ('ㄴ다는', 0.0),\n",
       " ('ㄴ데', 0.0),\n",
       " ('각각', 0.0),\n",
       " ('간사이', 0.0),\n",
       " ('같은', 0.0),\n",
       " ('건강', 0.0),\n",
       " ('건강하', 0.0),\n",
       " ('건강한', 0.0),\n",
       " ('걸리', 0.0),\n",
       " ('걸리는', 0.0),\n",
       " ('결과', 0.0),\n",
       " ('경우', 0.0),\n",
       " ('곡류', 0.0),\n",
       " ('공통', 0.0),\n",
       " ('공통점', 0.0),\n",
       " ('과일', 0.0),\n",
       " ('관련', 0.0),\n",
       " ('관련하', 0.0),\n",
       " ('관련한', 0.0),\n",
       " ('관여', 0.0),\n",
       " ('관여하', 0.0),\n",
       " ('그런', 0.0),\n",
       " ('그런데', 0.0),\n",
       " ('그룹', 0.0),\n",
       " ('급격', 0.0),\n",
       " ('다고', 0.0),\n",
       " ('다는', 0.0),\n",
       " ('다르', 0.0),\n",
       " ('바꾸면', 0.0),\n",
       " ('바뀌', 0.0),\n",
       " ('반대', 0.0),\n",
       " ('반찬', 0.0),\n",
       " ('발표', 0.0),\n",
       " ('발표하', 0.0),\n",
       " ('발표한', 0.0),\n",
       " ('방법', 0.0),\n",
       " ('별로', 0.0),\n",
       " ('보다', 0.0),\n",
       " ('본인', 0.0),\n",
       " ('부소', 0.0),\n",
       " ('부터', 0.0),\n",
       " ('분비', 0.0),\n",
       " ('분비되', 0.0),\n",
       " ('비슷', 0.0),\n",
       " ('흡수', 0.0),\n",
       " ('비슷한', 0.0),\n",
       " ('빠르', 0.0),\n",
       " ('빠른', 0.0),\n",
       " ('빠지', 0.0),\n",
       " ('빼는', 0.0),\n",
       " ('사람', 0.0),\n",
       " ('살이', 0.0),\n",
       " ('상승', 0.0),\n",
       " ('상승하', 0.0),\n",
       " ('서도', 0.0),\n",
       " ('석쇠', 0.0),\n",
       " ('섬유', 0.0),\n",
       " ('바꾸', 0.0),\n",
       " ('무엇', 0.0),\n",
       " ('못하', 0.0),\n",
       " ('모두', 0.0),\n",
       " ('다음', 0.0),\n",
       " ('다이어트', 0.0),\n",
       " ('다이어트법', 0.0),\n",
       " ('단백', 0.0),\n",
       " ('당뇨', 0.0),\n",
       " ('대로', 0.0),\n",
       " ('대상', 0.0),\n",
       " ('대하', 0.0),\n",
       " ('대한', 0.0),\n",
       " ('된다', 0.0),\n",
       " ('들어', 0.0),\n",
       " ('따라', 0.0),\n",
       " ('따르', 0.0),\n",
       " ('또한', 0.0),\n",
       " ('섭취량', 0.0),\n",
       " ('르레', 0.0),\n",
       " ('마지막', 0.0),\n",
       " ('막고', 0.0),\n",
       " ('막아', 0.0),\n",
       " ('많고', 0.0),\n",
       " ('많은', 0.0),\n",
       " ('많이', 0.0),\n",
       " ('먹고', 0.0),\n",
       " ('먹기', 0.0),\n",
       " ('먹는', 0.0),\n",
       " ('먹으', 0.0),\n",
       " ('먹은', 0.0),\n",
       " ('먼저', 0.0),\n",
       " ('면서', 0.0),\n",
       " ('명과', 0.0),\n",
       " ('마다', 0.0),\n",
       " ('흡수되', 0.0)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "sorted(outdict.items(), key = itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noun_extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noun_extractor' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nouns = noun_extractor.train_extract(sentences, minimum_noun_score=0.3, min_count=100)\n",
    "print('num nouns = %d' % len(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('./data/wikidumps/text/AA/wiki_*'):\n",
    "    sentences = Sentences(file)\n",
    "    word_extractor.train(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['식이섬유']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CountFeatures(pos):\n",
    "    posList = list(map(lambda x: x[0], pos))\n",
    "    vect = CountVectorizer().fit(posList)\n",
    "    count = vect.transform(posList).toarray().sum(axis = 0)\n",
    "    idx = np.argsort(-count)\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]    \n",
    "    x = list(zip(feature_name, count))[:5]     \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in useCollection.find({'category':\"뉴스\"}):\n",
    "    f = idx['mainText'].strip()\n",
    "    if len(f) ==0:\n",
    "        t = f\n",
    "    else:\n",
    "        t = CountFeatures(Twitter().pos(f))\n",
    "    print (t,idx['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaa = etri = USE_ETRI_ANALYSIS('srl', idx['mainText'])['return_object']    \n",
    "etri = aaa['sentence']\n",
    "morp = [] ; wsd = [] ; word = [] ; ne = []; srl =[]\n",
    "for i in etri:\n",
    "    x = Extract_Text_Info(i)\n",
    "    morp += x[0]\n",
    "    wsd += x[1]\n",
    "    word +=x[2]\n",
    "    ne += x[3]\n",
    "    srl += x[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(np.array(list(map(lambda x:x[1], srl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eriOut_srl = TfidFeatures(srl, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iii3 = TfidFeatures(iii2, stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "import sys\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "word_min_count = 2\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 1 #0 = dbow; 1 = dmpv\n",
    "worker_count = cores #number of parallel processes\n",
    "modelfile = \"./model/doc2vec.model\"\n",
    "word2vec_file = modelfile + \".word2vec_format\"\n",
    "#sentences=doc2vec.TaggedLineDocument()\n",
    "#build voca \n",
    "doc_vectorizer = doc2vec.Doc2Vec(min_count=word_min_count, size=vector_size, alpha=0.025, min_alpha=0.025, seed=1234, workers=worker_count)\n",
    "doc_vectorizer.build_vocab(rawdata['mainText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etriOut_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx2 = list(set(map(lambda x: x[0], etriOut_ne)) & set(map(lambda x: x[0], mecabOut)))\n",
    "xx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x: x[0] in xx, twitterOut ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(map(lambda x: x[0], etriOut_word)) & set(map(lambda x: x[0], twitterOut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitterOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set1 = Counter(list(filter(lambda x: ('NNG' or 'NNP' or 'NNB') ==x[1] , morp))).most_common(n= 20)\n",
    "set1 = set(map(lambda x: x[0][0], set1))\n",
    "set2 = Counter(list(filter(lambda x: ('NNG' or 'NNP' or 'NNB') ==x[1] , mecab_out))).most_common(n= 20)\n",
    "set2 = set(map(lambda x: x[0][0], set2))\n",
    "set3 = Counter(list(filter(lambda x: ('Noun') ==x[1] , twitter_out))).most_common(n= 20)\n",
    "set3 = set(map(lambda x: x[0][0], set3))\n",
    "set1 & set2 & set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from gensim.models import doc2vec\n",
    "import sys\n",
    "import multiprocessing\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "cores = multiprocessing.cpu_count()\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "word_min_count = 2\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 1 #0 = dbow; 1 = dmpv\n",
    "worker_count = cores #number of parallel processes\n",
    "\n",
    "\n",
    "inputfile = \"./data/\"\n",
    "modelfile = \"./model/doc2vec.model\"\n",
    "word2vec_file = modelfile + \".word2vec_format\"\n",
    "sentences=doc2vec.TaggedLineDocument(inputfile)\n",
    "#build voca \n",
    "doc_vectorizer = doc2vec.Doc2Vec(min_count=word_min_count, size=vector_size, alpha=0.025, min_alpha=0.025, seed=1234, workers=worker_count)\n",
    "doc_vectorizer.build_vocab(sentences)\n",
    "# Train document vectors!\n",
    "for epoch in range(10):\n",
    "\tdoc_vectorizer.train(sentences)\n",
    "\tdoc_vectorizer.alpha -= 0.002 # decrease the learning rate\n",
    "\tdoc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n",
    "# To save\n",
    "doc_vectorizer.save(modelfile)\n",
    "doc_vectorizer.save_word2vec_format(word2vec_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding = 'utf-8', mode='r') as f:\n",
    "        data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
