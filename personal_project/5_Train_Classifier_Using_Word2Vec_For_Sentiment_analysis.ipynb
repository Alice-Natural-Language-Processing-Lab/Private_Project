{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 만들어진 Word2Vec model을 통한 감정분석 실시\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10419045285149508714\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4984510873\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 4320047186308386534\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:09:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Roc_Curve(x, y, model1, model2, model3, model4):\n",
    "    print ('Logistic Regression')\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y, model1.predict(x))\n",
    "    print ('Random Forest')\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y, model2.predict(x))\n",
    "    print ('Kernel SVM')\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(y, model3.predict(x))\n",
    "    print ('XGBoost')\n",
    "    import xgboost as xgb\n",
    "    fpr4, tpr4, thresholds4 = roc_curve(y, model4.predict(xgb.DMatrix(x)))\n",
    "    plt.plot(fpr1, tpr1, label=\"Logistic Regression\")\n",
    "    plt.plot(fpr2, tpr2, label=\"RandomForest\")\n",
    "    plt.plot(fpr3, tpr3, label=\"Kernel SVM\")\n",
    "    plt.plot(fpr4, tpr4, label='XGBoost')\n",
    "    plt.legend()\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot model history after `fit()`.\n",
    "    \"\"\"\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_TSNE1(n_component, model, wv, limit):\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\")\n",
    "    wv = wv[:limit]\n",
    "    tsne_model = TSNE(n_components=n_component,\n",
    "                       verbose = 1, random_state = 0)\n",
    "    tsne_w2v = tsne_model.fit_transform(wv)\n",
    "    tsne_df = pd.DataFrame(tsne_w2v, columns = ['x', 'y'])\n",
    "    tsne_df['words'] = list(model.wv.vocab.keys())[:limit]\n",
    "    i = 0\n",
    "    for i in tqdm(range(tsne_df['words'].size)):\n",
    "        plt.scatter(tsne_df['x'][i], tsne_df['y'][i])\n",
    "        plt.annotate(tsne_df['words'][i], \n",
    "                xy = (tsne_df['x'][i], tsne_df['y'][i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_TSNE2(n_component, model, wv, limit):\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    import bokeh.plotting as bp\n",
    "    from bokeh.models import HoverTool, BoxSelectTool\n",
    "    from bokeh.plotting import figure, show, output_notebook\n",
    "    \n",
    "    output_notebook()\n",
    "    plot_tfidf = bp.figure(plot_width=500, plot_height=500, title=\"A map of word vectors\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "    word_vectors = [model[w] for w in tqdm(list(model.wv.vocab.keys())[:limit])]\n",
    "\n",
    "    tsne_model = TSNE(n_components=n_component, verbose=1, random_state=0)\n",
    "    tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "    # putting everything in a dataframe\n",
    "    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "    tsne_df['words'] = list(model.wv.vocab.keys())[:limit]\n",
    "\n",
    "    # plotting. the corresponding word appears when you hover on the data point.\n",
    "    plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "    hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "    hover.tooltips={\"word\": \"@words\"}\n",
    "    show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_tfidf(data):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer = lambda x: x, min_df = 2)\n",
    "    matrix = vectorizer.fit_transform([x.words for x in tqdm(data)])\n",
    "    print (matrix.shape)\n",
    "    tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    print ('vocab size : {}'.format(len(tfidf)))    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, model, size, tfidf):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Pre_Data(model, tfidf, size, train, test):\n",
    "    from datetime import datetime\n",
    "    start = datetime.now()\n",
    "    print (str(model))\n",
    "    wv = [model[w] for w in tqdm(model.wv.vocab.keys())]\n",
    "    process1 = datetime.now()\n",
    "    print ('running time : {}'.format(process1 - start))\n",
    "    \n",
    "    print ('Vectorizing Train Data')\n",
    "    train_vecs_w2v = np.concatenate([buildWordVector(z, model, size, tfidf) for z in tqdm(map(lambda x: x.words, train))])\n",
    "    print ('scaling Train Data')\n",
    "    train_vecs_w2v = scale(train_vecs_w2v)\n",
    "    process2 = datetime.now()\n",
    "    print ('running time : {}'.format(process2 - process1))\n",
    "    \n",
    "    print ('Vectorizing Test Data')\n",
    "    test_vecs_w2v = np.concatenate([buildWordVector(z, model, size, tfidf) for z in tqdm(map(lambda x: x.words, test))])\n",
    "    print ('scaling Test Data')\n",
    "    test_vecs_w2v = scale(test_vecs_w2v)\n",
    "    process3 = datetime.now()\n",
    "    print ('running time : {}'.format(process3 - process2))\n",
    "    \n",
    "    print ('total running time : {}'.format(process3 - start))\n",
    "    return wv, train_vecs_w2v, test_vecs_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "    #loadModelPath = './model/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "saveTrainPath = './data/pre_data/train_test_Data2/'\n",
    "saveClassifierPath = './data/pre_data/classifier/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReMake_Outcome(train_y, test_y):\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\") \n",
    "    train_y = [y[0] for y in tqdm(train_y)]\n",
    "    test_y = [y[0] for y in tqdm(test_y)]\n",
    "    return train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Return_ModelName(model, tagger):\n",
    "    size = model.vector_size\n",
    "    epochs = model.epochs\n",
    "    window = model.window\n",
    "    negative = model.negative \n",
    "    hs = model.hs \n",
    "    sg = model.sg \n",
    "    cbow_mean = model.cbow_mean \n",
    "    min_count = model.min_count \n",
    "    min_alpha = model.min_alpha\n",
    "    alpha = model.alpha\n",
    "    modelName = 'word2vec_size-{}_epochs-{}_window-{}_negative-{}_hs-{}_sg-{}_cbow_mean-{}_min_count-{}_min_alpha-{}_alpha-{}_by-{}'.format(\n",
    "    size, epochs, window, negative, hs, sg, cbow_mean, min_count, \n",
    "    min_alpha, alpha, tagger)\n",
    "    return modelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "cores = int(multiprocessing.cpu_count() )\n",
    "print (cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load Word2Vec model을 만들기 위해 사용한 사용된 train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1512694.70it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_ct.pickled','rb'))\n",
    "y_train = np.array([doc.sentiment for doc in tqdm(train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set을 사용하여 Tf-Idf vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1502356.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442359, 159113)\n",
      "vocab size : 159113\n"
     ]
    }
   ],
   "source": [
    "tfidf = Build_tfidf(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec model을 만들기 위해 사용한 사용된 testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49151/49151 [00:00<00:00, 1321425.28it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_ct.pickled','rb'))\n",
    "y_test = np.array([doc.sentiment for doc in tqdm(test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1111958.35it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1066675.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_y2, test_y2 = ReMake_Outcome(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-ct.model')\n",
    "model2 = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-ct.model')\n",
    "model3 = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-ct.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = Make_Pre_Data(model1, tfidf, 1000, train, test)\n",
    "modelName = Return_ModelName(model1,'ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_TSNE2(2, model1, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C - Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_vecs_w2v)\n",
    "train_vecs_w2v2 = scaling.transform(train_vecs_w2v)\n",
    "test_vecs_w2v2 = scaling.transform(test_vecs_w2v)\n",
    "classifier3 =  SVC(kernel = 'linear',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v2)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree= 0.7\n",
    "params ={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\" : max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\":1,\n",
    "    'eta':0.125,\n",
    "    'nthread' : cores\n",
    "    \n",
    "    }\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist, early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_class = np.round(test_prediction)\n",
    "print (accuracy_score(y_test, test_class))\n",
    "print (classification_report(y_test, test_class))\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_Roc_Curve(test_vecs_w2v, y_test, classifier, classifier2, classifier3, gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = Make_Pre_Data(model2, tfidf, 1000, train, test)\n",
    "modelName = Return_ModelName(model2,'ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_TSNE2(2, model2, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C - Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_vecs_w2v)\n",
    "train_vecs_w2v2 = scaling.transform(train_vecs_w2v)\n",
    "test_vecs_w2v2 = scaling.transform(test_vecs_w2v)\n",
    "classifier3 =  SVC(kernel = 'linear',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v2)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree= 0.7\n",
    "params ={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\" : max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\":1,\n",
    "    'eta':0.125,\n",
    "    'nthread' : cores\n",
    "    \n",
    "    }\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist, early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_class = np.round(test_prediction)\n",
    "print (accuracy_score(y_test, test_class))\n",
    "print (classification_report(y_test, test_class))\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_Roc_Curve(test_vecs_w2v, y_test, classifier, classifier2, classifier3, gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = Make_Pre_Data(model3, tfidf, 1000, train, test)\n",
    "modelName = Return_ModelName(model3,'ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_TSNE2(2, model3, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C - Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_vecs_w2v)\n",
    "train_vecs_w2v2 = scaling.transform(train_vecs_w2v)\n",
    "test_vecs_w2v2 = scaling.transform(test_vecs_w2v)\n",
    "classifier3 =  SVC(kernel = 'linear',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v2)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree= 0.7\n",
    "params ={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\" : max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\":1,\n",
    "    'eta':0.125,\n",
    "    'nthread' : cores\n",
    "    \n",
    "    }\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist, early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_class = np.round(test_prediction)\n",
    "print (accuracy_score(y_test, test_class))\n",
    "print (classification_report(y_test, test_class))\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_Roc_Curve(test_vecs_w2v, y_test, classifier, classifier2, classifier3, gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load Word2Vec model을 만들기 위해 사용한 사용된 train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_mecab.pickled','rb'))\n",
    "y_train = np.array([doc.sentiment for doc in tqdm(train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set을 사용하여 Tf-Idf vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = Build_tfidf(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec model을 만들기 위해 사용한 사용된 testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_mecab.pickled','rb'))\n",
    "y_test = np.array([doc.sentiment for doc in tqdm(test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y2, test_y2 = ReMake_Outcome(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "model2 = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-mecab.model')\n",
    "model3 = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-mecab.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = Make_Pre_Data(model1, tfidf, 1000, train, test)\n",
    "modelName = Return_ModelName(model1,'mecab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_TSNE2(2, model1, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C - Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_vecs_w2v)\n",
    "train_vecs_w2v2 = scaling.transform(train_vecs_w2v)\n",
    "test_vecs_w2v2 = scaling.transform(test_vecs_w2v)\n",
    "classifier3 =  SVC(kernel = 'linear',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v2)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree= 0.7\n",
    "params ={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\" : max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\":1,\n",
    "    'eta':0.125,\n",
    "    'nthread' : cores\n",
    "    \n",
    "    }\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist, early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_class = np.round(test_prediction)\n",
    "print (accuracy_score(y_test, test_class))\n",
    "print (classification_report(y_test, test_class))\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_Roc_Curve(test_vecs_w2v, y_test, classifier, classifier2, classifier3, gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = Make_Pre_Data(model2, tfidf, 1000, train, test)\n",
    "modelName = Return_ModelName(model2,'mecab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_TSNE2(2, model2, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C - Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_vecs_w2v)\n",
    "train_vecs_w2v2 = scaling.transform(train_vecs_w2v)\n",
    "test_vecs_w2v2 = scaling.transform(test_vecs_w2v)\n",
    "classifier3 =  SVC(kernel = 'linear',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v2)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree= 0.7\n",
    "params ={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\" : max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\":1,\n",
    "    'eta':0.125,\n",
    "    'nthread' : cores\n",
    "    \n",
    "    }\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist, early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_class = np.round(test_prediction)\n",
    "print (accuracy_score(y_test, test_class))\n",
    "print (classification_report(y_test, test_class))\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_Roc_Curve(test_vecs_w2v, y_test, classifier, classifier2, classifier3, gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1, train_vecs_w2v, test_vecs_w2v = Make_Pre_Data(model3, tfidf, 1000, train, test)\n",
    "modelName = Return_ModelName(model3,'mecab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "> * t-분포 확률적 임베딩\n",
    "> * 데이터의 차원 축소에 사용되는 기계 학습 알고리즘\n",
    "> * 비선형 차원 축소 기법으로 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용\n",
    "> * 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑\n",
    "##### word : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_TSNE2(2, model3, wv1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(max_iter = 250, n_jobs = cores)\n",
    "classifier.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(n_estimators = 75, n_jobs = cores)\n",
    "classifier2.fit(train_vecs_w2v, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_vecs_w2v, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_vecs_w2v)))\n",
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C - Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_vecs_w2v)\n",
    "train_vecs_w2v2 = scaling.transform(train_vecs_w2v)\n",
    "test_vecs_w2v2 = scaling.transform(test_vecs_w2v)\n",
    "classifier3 =  SVC(kernel = 'linear',\n",
    "        cache_size= 1024, max_iter = 1500, verbose = True) \n",
    "classifier3.fit(train_vecs_w2v2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_vecs_w2v2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_vecs_w2v2)))\n",
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+modelName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "subsample = 0.7\n",
    "colsample_bytree= 0.7\n",
    "params ={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\" : max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\":1,\n",
    "    'eta':0.125,\n",
    "    'nthread' : cores\n",
    "    \n",
    "    }\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_vecs_w2v, y_train)\n",
    "dvalid = xgb.DMatrix(test_vecs_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist, early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "test_prediction = gbm.predict(xgb.DMatrix(test_vecs_w2v))\n",
    "test_class = np.round(test_prediction)\n",
    "print (accuracy_score(y_test, test_class))\n",
    "print (classification_report(y_test, test_class))\n",
    "gbm.save_model(saveClassifierPath+'XGBoost_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Make_Roc_Curve(test_vecs_w2v, y_test, classifier, classifier2, classifier3, gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3\n",
    "del gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)\n",
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
