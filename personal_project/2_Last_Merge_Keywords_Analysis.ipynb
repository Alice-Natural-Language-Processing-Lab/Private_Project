{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 1385489, 5532]\n",
      "[('이', 'Noun'), ('것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n",
      "best: [('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import html\n",
    "\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Twitter, Mecab,Kkma\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "\n",
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "mecab = Mecab()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "print (list(map(lambda x: len(xxxx[x]), xxxx)))\n",
    "\n",
    "score_weights = {\n",
    "    'num_nouns': -0.1,\n",
    "    'num_words': -0.2,\n",
    "    'no_noun': -1\n",
    "}\n",
    "\n",
    "def my_score(candidate):\n",
    "    num_nouns = len([w for w,t in candidate if t == 'Noun'])\n",
    "    num_words = len(candidate)\n",
    "    no_noun = 1 if num_nouns == 0 else 0\n",
    "    \n",
    "    score = (num_nouns * score_weights['num_nouns'] \n",
    "             + num_words * score_weights['num_words']\n",
    "             + no_noun * score_weights['no_noun'])\n",
    "    return score\n",
    "\n",
    "ct.set_selector(score_weights, my_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check(list1, list2):\n",
    "    first_keywords = list(filter(lambda x: x[0] == x[1], list(itertools.product(list1, list2))))\n",
    "    first_keywords = set(map(lambda x: x[0], first_keywords))\n",
    "    second_keywords = list(filter(lambda x: x[0] != x[1], list(itertools.product(list1, list2))))\n",
    "    second_keywords = list(filter(lambda x: abs(len(max(x, key = len)) - len(''.join(re.findall('['+x[0]+']', x[1])).strip())) == 1,\n",
    "                                  second_keywords))\n",
    "    second_keywords = set(map(lambda x: max(x, key = len), second_keywords))\n",
    "    keywords = first_keywords|second_keywords\n",
    "    k2 = list(filter(lambda idx: abs(len(''.join(re.findall('['+CompareWords(*idx)[0]+']', CompareWords(*idx)[1])).strip())-len(min(idx, key = len))) == 0,\n",
    "                     list(itertools.combinations(keywords, 2))))\n",
    "    k2 = set(map(lambda x: min(x, key = len), k2))\n",
    "    list(map(lambda x: keywords.remove(x), k2))\n",
    "    return keywords\n",
    "\n",
    "def CompareWords(word1, word2):\n",
    "    if len(word1) == len(word2):\n",
    "        if re.sub(' ','', word1) > re.sub(' ','', word2):\n",
    "            return word2, word1\n",
    "        elif re.sub(' ','', word1) < re.sub(' ','', word2):\n",
    "            return word1, word2\n",
    "        else:\n",
    "            return word1, word2\n",
    "    else:\n",
    "        return min([word1, word2], key = len), max([word1, word2], key = len)\n",
    "\n",
    "def Check2(list1, list2):\n",
    "    first_keywords = list(filter(lambda x: x[0] == x[1], list(itertools.product(list1, list2))))\n",
    "    first_keywords = set(map(lambda x: x[0], first_keywords))\n",
    "    second_keywords = list(filter(lambda x: x[0] != x[1], list(itertools.product(list1, list2))))\n",
    "    second_keywords = list(filter(lambda x: abs(len(CompareWords(*x)[1]) - len(''.join(re.findall('('+CompareWords(*x)[0]+')?', CompareWords(*x)[1])).strip())) == 1,\n",
    "                                  second_keywords))\n",
    "    second_keywords = set(map(lambda x: CompareWords(*x)[1], second_keywords))\n",
    "    keywords = first_keywords|second_keywords\n",
    "    k2 = list(filter(lambda idx: abs(len(''.join(re.findall('['+CompareWords(*idx)[1]+']',CompareWords(*idx)[0])).strip())-len(min(idx, key = len))) == 0,\n",
    "                     list(itertools.combinations(keywords, 2))))\n",
    "    k2 = set(map(lambda x: min(x, key = len), k2))\n",
    "    list(map(lambda x: keywords.remove(x), k2))\n",
    "    return keywords\n",
    "\n",
    "def Compare_Keywords(list1, list2, list3):\n",
    "    list1 = list(map(lambda x: ' '.join(list(dict.fromkeys(x.split(' ')))).strip(), list1))\n",
    "    list1 = list(dict.fromkeys(list1 + list(map(lambda y: ''.join(y), list(map(lambda x: x.split(' '), list1))))))\n",
    "    list2 = list(map(lambda x: ' '.join(list(dict.fromkeys(x.split(' ')))).strip(), list2))\n",
    "    list2 = list(dict.fromkeys(list2 + list(map(lambda y: ''.join(y), list(map(lambda x: x.split(' '), list2))))))\n",
    "    list3 = list(map(lambda x: ' '.join(list(dict.fromkeys(x.split(' ')))).strip(), list3))\n",
    "    list3 = list(dict.fromkeys(list3 + list(map(lambda y: ''.join(y), list(map(lambda x: x.split(' '), list3))))))\n",
    "    m1 = Check2(Check(list1, list2), Check(list2, list1))\n",
    "    m2 = Check2(Check(list1, list3), Check(list3, list1))\n",
    "    m3 = Check2(Check(list2, list3), Check(list3, list2))\n",
    "    total =m1 | m2 | m3\n",
    "    xyz2 = list(itertools.combinations(total, 2))\n",
    "    xyz3 = list(filter(lambda x: abs(len(''.join(re.findall('('+CompareWords(*x)[0]+')',CompareWords(*x)[1])).strip()) - len(CompareWords(*x)[0]))==0, xyz2))\n",
    "    xyz3_1 = set(map(lambda x:CompareWords(*x), xyz3))\n",
    "    xyz3 = set(map(lambda x:CompareWords(*x)[0], xyz3))\n",
    "    xyz3 = set(filter(lambda x: x!='', xyz3))\n",
    "    list(map(lambda x: total.remove(x), xyz3))\n",
    "    xyz2 = list(itertools.combinations(total, 2))\n",
    "    xyz4 = list(filter(lambda x: abs(len(''.join(re.findall('['+CompareWords(*x)[0]+']',CompareWords(*x)[1])).strip()) - len(CompareWords(*x)[0]))==0, xyz2))\n",
    "    xyz4_1 = set(map(lambda x: CompareWords(*x), xyz4))\n",
    "    xyz4 = set(map(lambda x: CompareWords(*x)[0], xyz4))\n",
    "    xyz4 = set(filter(lambda x:x!='', xyz4))\n",
    "    list(map(lambda x: total.remove(x), xyz4))\n",
    "    xyz2 = list(itertools.combinations(total, 2))\n",
    "    xyz5 = set(filter(lambda x: re.findall('('+re.sub(' ','',CompareWords(*x)[0])+')',CompareWords(*x)[1]),xyz2)) \n",
    "    lstCheck = list(map(lambda x: (CompareWords(*x)[1], CheckDuplicate1(*x)), xyz3_1 | xyz4_1 | xyz5))\n",
    "    lstCheck = list(filter(lambda x: x[1]!=None, lstCheck))\n",
    "    total = list(total)\n",
    "    if lstCheck:\n",
    "        for check in lstCheck:\n",
    "            if check[0] in total:\n",
    "                total[total.index(check[0])] = check[1]\n",
    "            else:pass\n",
    "    total = list(set(total))\n",
    "    xyz6 = list(itertools.combinations(total, 2))\n",
    "    xyz6 = list(filter(lambda x: re.findall('('+re.sub(' ', '', CompareWords(*x)[0])+')', CompareWords(*x)[1]), xyz6))\n",
    "    if xyz6:\n",
    "        for check in xyz6:\n",
    "            if check[0] in total:\n",
    "                total[total.index(check[0])]=check[1]\n",
    "    return set(total)\n",
    "\n",
    "def CheckDuplicate1(x1,x2):\n",
    "    lessWord = CompareWords(x1,x2)[0]\n",
    "    moreWord = CompareWords(x1,x2)[1]\n",
    "    out1 = re.findall(lessWord, moreWord)\n",
    "    if len(out1) > 1:\n",
    "        return out1[0]\n",
    "    elif len(out1) == 1:\n",
    "        if re.search(lessWord, moreWord):\n",
    "            out2 = re.sub(re.search(lessWord, moreWord).group(), '', moreWord).strip()\n",
    "            lessWord2 = CompareWords(out2, lessWord)[0]\n",
    "            moreWord2 = CompareWords(out2, lessWord)[1]\n",
    "            if re.search(lessWord2, moreWord2):\n",
    "                return moreWord2\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Run(site):\n",
    "    if site == 'daum':\n",
    "        collection = 'newsDaum'\n",
    "    elif site.lower() == 'naver':\n",
    "        collection = 'newsNaver'\n",
    "    mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "    dbname = 'hy_db'\n",
    "    useDb = dh.Use_Database(mongodb, dbname)\n",
    "    useCollection = dh.Use_Collection(useDb, collection)\n",
    "\n",
    "    ext_Keyword_Daum_mecab = glob('./data/Keyword_outcome/mecab-'+site.lower()+'*')\n",
    "    ext_Keyword_Daum_ckonlpy = glob('./data/Keyword_outcome/ckonlpy-'+site.lower()+'*')\n",
    "    ext_Keyword_Daum_both = glob('./data/Keyword_outcome/mecab_ckonlpy-'+site.lower()+'*')\n",
    "\n",
    "    ckonlpyDict = dict()\n",
    "    for file in ext_Keyword_Daum_ckonlpy:\n",
    "        ckonlpyDict.update(pickle.load(open(file,'rb')))\n",
    "    mecabDict = dict()\n",
    "    for file in ext_Keyword_Daum_mecab:\n",
    "        mecabDict.update(pickle.load(open(file,'rb')))\n",
    "    bothDict = dict()\n",
    "    for file in ext_Keyword_Daum_both:\n",
    "        bothDict.update(pickle.load(open(file,'rb')))\n",
    "    print (len(ckonlpyDict))\n",
    "    print (len(mecabDict))\n",
    "    print (len(bothDict))\n",
    "\n",
    "    dataList = useCollection.find({'site':site})\n",
    "    print (dataList.count())\n",
    "    dataDict = dict()\n",
    "    i = 0 \n",
    "    for idx, data in enumerate(dataList):\n",
    "        idIs = data['_id']._ObjectId__id.hex()\n",
    "        if idIs in ckonlpyDict.keys() and idIs in mecabDict.keys() and idIs in bothDict.keys():\n",
    "            keywords = Compare_Keywords(ckonlpyDict[idIs], mecabDict[idIs], bothDict[idIs])\n",
    "            dataDict[idIs] = keywords\n",
    "            i+=1\n",
    "    pickle.dump(dataDict, open('./data/pre_data/keywords/keywords_'+site+'.pickled','wb'))\n",
    "\n",
    "    print (len(dataDict))\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15121\n",
      "15121\n",
      "15121\n",
      "15120\n",
      "15120\n",
      "9372\n",
      "9372\n",
      "9372\n",
      "9372\n",
      "9372\n"
     ]
    }
   ],
   "source": [
    "naver_out = Run('Naver')\n",
    "daum_out = Run('daum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stastics(site, keywordsdict):\n",
    "    if site == 'daum':\n",
    "        binList = []\n",
    "        collection = 'newsDaum'\n",
    "    elif site.lower() == 'naver':\n",
    "        binList = []\n",
    "        collection = 'newsNaver'\n",
    "    mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "    dbname = 'hy_db'\n",
    "    useDb = dh.Use_Database(mongodb, dbname)\n",
    "    useCollection = dh.Use_Collection(useDb, collection)\n",
    "    collection = useCollection.find({'site':site})\n",
    "\n",
    "    dataDict = dict()\n",
    "    for data in collection:\n",
    "        dataDict[data['_id']._ObjectId__id.hex()] = idData = dict()\n",
    "        idData['category'] = data['category']\n",
    "        idData['date'] = data['date']\n",
    "        idData['press'] = data['press']\n",
    "        idData['number_of_comment'] = data['number_of_comment']\n",
    "        idData['number_of_crawled_comment'] = data['real_number_of_comment']\n",
    "        idData['rank'] = data['rank']\n",
    "        idData['title'] = data['title']\n",
    "        idData['mainText'] = data['mainText']\n",
    "        idData['keywords'] = data['keywords']\n",
    "        idData['extracted_keywords'] = keywordsdict[data['_id']._ObjectId__id.hex()]\n",
    "    pickle.dump(dataDict, open('./data/pre_data/stastics/for_statistics_'+site+'_from_mongodb.pickled','wb'))\n",
    "    print (len(dataDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120\n",
      "9372\n"
     ]
    }
   ],
   "source": [
    "Stastics('Naver', naver_out)\n",
    "Stastics('daum', daum_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "* 완벽하지 않다. \n",
    "> 플라세보 효과 -> 세보 효과 등으로 단어를 정확히 추출해내지 못하는 경우가 존재함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
