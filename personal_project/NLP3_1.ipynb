{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 빅데이터 분석 데이터 추출 & 사용자 사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbpath = '/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/ckonlpy/data/twitter/noun/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = './data/news/high_frequency_noun/'\n",
    "path2 = './data/news/mainissue/'\n",
    "path3 = './data/news/newstopic/'\n",
    "path4 = './data/news/people/'\n",
    "path5 = './data/news/4th_industry/'\n",
    "path6 = './data/news/have_negative_positive/constitution/'\n",
    "path7 = './data/news/have_negative_positive/household_debt/'\n",
    "path8 = './data/news/have_negative_positive/olymphic/'\n",
    "path9 = './data/news/have_negative_positive/'\n",
    "path10 = './data/news/'\n",
    "\n",
    "pathlist = [path1, path2, path3,\n",
    "            path4, path5, path6,\n",
    "            path7, path8, path9,\n",
    "           path10]\n",
    "\n",
    "with open(dbpath+'from_news.txt','w') as f:\n",
    "    for path in pathlist:\n",
    "        out = Extract_Keyword_from_news_bigdata(path)\n",
    "        f.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Mecab, Twitter, Kkma\n",
    "from konlpy.utils import pprint\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwords = Stopwords('./data/koreanStopwords.txt') +Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512, 5068471, 5532]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "list(map(lambda x: len(xxxx[x]), xxxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "sys.path.append('/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/')\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import time, re, pickle, itertools\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from multiprocessing import Pool\n",
    "import urllib3, json\n",
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import os\n",
    "\n",
    "def OpenAPI():\n",
    "    import pickle\n",
    "    openapi = pickle.load(open('./etri_open_api_access_key', 'rb'))\n",
    "    return openapi\n",
    "def USE_ETRI_ANALYSIS(analysisCode, text):    \n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    requestJson = {\n",
    "        'access_key' : accessKey,\n",
    "        'argument' : {\n",
    "            'text' : text,\n",
    "            'analysis_code' : analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "    return json.loads(str(response.data,'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_INFO(fWord, sWord):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/WordRel\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    firstWord = fWord\n",
    "    #firstSenseId = fSenseId\n",
    "    secondWord = sWord\n",
    "    #secondSenseId = sSenseId\n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "        'first_word': firstWord,\n",
    "        #'first_sense_id': firstSenseId,\n",
    "        'second_word': secondWord,\n",
    "        #'second_sense_id': secondSenseId\n",
    "        }}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "    body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_RELATION(word):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/Word\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    word = word\n",
    "    requestJson = {\n",
    "    \"access_key\": accessKey,\n",
    "    \"argument\": {\"word\": word}}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "                            body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def Extract_Text_Info(idx):\n",
    "    morp = list(map(lambda x: (x['lemma'], x['type']),idx['morp']))\n",
    "    wsd = list(map(lambda x: (x['text'], x['type']),idx['WSD']))\n",
    "    word = list(map(lambda x: (x['text'], x['type']),idx['word']))\n",
    "    ne = list(map(lambda x: (x['text'], x['type']),idx['NE']))\n",
    "    return morp, wsd, word, ne\n",
    "\n",
    "def Run_ETRI_Analysis(stopwordsList, text):\n",
    "    etri = USE_ETRI_ANALYSIS('srl', text)['return_object']    \n",
    "    etri = etri['sentence']\n",
    "    morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "    for i in etri:\n",
    "        x = Extract_Text_Info(i)\n",
    "        morp += x[0]\n",
    "        wsd += x[1]\n",
    "        word +=x[2]\n",
    "        ne += x[3]\n",
    "        for ii in i['SRL']:\n",
    "            srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "        dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "    return morp, wsd, word, ne, srl, dependency, etri\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    collection = 'newsDaum'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/daum/'\n",
    "elif site.lower() == 'naver':\n",
    "    collection = 'newsNaver'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/naver/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywordList = list()\n",
    "pressList = list()\n",
    "#etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/'\n",
    "for data in useCollection.find({'site':site}):\n",
    "    pressList.append(data['press'])\n",
    "    if len(data['mainText'].strip()) !=0:\n",
    "        if site=='daum':\n",
    "            if data['keywords'] !='NaN':\n",
    "                keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "        else:\n",
    "            pass\n",
    "        idIs = data['_id']._ObjectId__id\n",
    "        if not os.path.isfile(etri_outcome+idIs.hex()+'.picked.txt'):\n",
    "            print (idIs.hex())\n",
    "            try:\n",
    "                etri = USE_ETRI_ANALYSIS('srl', re.sub('\\. ','.\\n', data['mainText']))\n",
    "            except:\n",
    "                print (etri[1])\n",
    "                break\n",
    "            else:\n",
    "                pickle.dump(etri[0], open(etri_outcome+idIs.hex()+'.picked.txt','wb'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5a2b0eac588c13738825a5cd'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['_id']._ObjectId__id.hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer, RegexTokenizer\n",
    "from soynlp.noun import LRNounExtractor\n",
    "from newspaper import Article\n",
    "class Sentences:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.length = 0\n",
    "    def __iter__(self):\n",
    "            for doc in kkma.sentences(self.text):\n",
    "                doc = doc.strip()\n",
    "                if not doc:\n",
    "                    continue\n",
    "                for sent in doc.split(' '):\n",
    "                    yield sent\n",
    "    def __len__(self):\n",
    "        if self.length == 0:\n",
    "            for doc in kkma.sentences(self.text):\n",
    "                doc = doc.strip()\n",
    "                if not doc:\n",
    "                    continue\n",
    "                self.length += len(doc.split(' '))\n",
    "        return self.length\n",
    "def RunWordKRwordRank(text, min_count):\n",
    "    from krwordrank.hangle import normalize\n",
    "    #min_count = 2   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "    max_length = 10 # 단어의 최대 길이\n",
    "    wordrank_extractor = KRWordRank(min_count, max_length)\n",
    "    beta = 0.85   # PageRank의 decaying factor beta\n",
    "    max_iter = 300\n",
    "    verbose = False\n",
    "    re.sub('\\. ','.\\n', data['mainText']).split('\\n')\n",
    "    textIs = list(map(lambda x: normalize(x, english = True, number = True), textIs))\n",
    "    keywords, rank, graph = wordrank_extractor.extract(textIs, beta, max_iter, verbose)\n",
    "    return keywords, rank, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keywords2(text, stopwords):\n",
    "    vect = TfidfVectorizer(norm='l2', \n",
    "                           ngram_range=(1,4),\n",
    "                           use_idf = True,sublinear_tf = True, \n",
    "                           stop_words=stopwords)\n",
    "    vect.fit(list(map(lambda x: x[0], ct.pos(text))))\n",
    "    #vect.fit(ct.phrases(text))\n",
    "    #vect.fit(ct.nouns(text))\n",
    "    #vect.fit(ct.morphs(text))\n",
    "    #vect.fit(list(map(lambda x: x[0], ot.pos(text,stem = True, norm = True))))\n",
    "    count = vect.transform(kkma.sentences(text)).toarray().sum(axis = 0)\n",
    "    idx = np.argsort(-count)\n",
    "    count = count[idx]\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]\n",
    "    out = dict(zip(feature_name, count))\n",
    "    return sorted(out.items(), key = itemgetter(1), reverse=True)\n",
    "\n",
    "def CombinationAndCheckInclude(dfidfDict):\n",
    "    outset = set()\n",
    "    for x1, x2 in list(itertools.combinations(list(dfidfDict.keys()), 2)):\n",
    "        if x1 in x2:\n",
    "            if dfidfDict[x1] <= dfidfDict[x2]:\n",
    "                outset.add(x1)\n",
    "            else:\n",
    "                outset.add(x2)\n",
    "        elif x2 in x1:\n",
    "            if dfidfDict[x1] >= dfidfDict[x2]:\n",
    "                outset.add(x2)\n",
    "            else:\n",
    "                outset.add(x1)\n",
    "        else:\n",
    "            pass\n",
    "    return outset\n",
    "\n",
    "def Out_Keywords(text, stopwords):\n",
    "    dfidfOut = dict(Extract_Keywords2(text, stopwords))\n",
    "    forFilterSet = CombinationAndCheckInclude(dfidfOut)\n",
    "    reout = list(filter(lambda x: not x in forFilterSet, dfidfOut.keys()))\n",
    "    return reout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5a2b0eac588c13738825a5cd'),\n",
       " 'category': '연예',\n",
       " 'date': '2017.12.05',\n",
       " 'keywords': ['노홍철', '한명회', '한혜진', '종영', '편성시간'],\n",
       " 'link': 'http://v.media.daum.net/v/20171205151743503',\n",
       " 'mainText': '영상 바로보기 [마이데일리 = 이승길 기자] JTBC \\'내 이름을 불러줘-한명(名)회\\'(이하 \\'한명회\\')가 막을 내린다. 최근 진행된 \\'한명회\\' 9회 녹화는 별난 이름 특집으로 꾸며졌다. 이날은 특집 명에 맞게 녹화 전까지 철저히 출연자들이 서로 이름을 공유하지 못하도록 조치를 취했다. 출연자들은 서로의 이름을 추측하며 자신의 이름보다 특이 한 지 궁금해 했다. 이날 8명의 출연자는 모두 \"출석 부르는 시간, 자기소개 시간을 가장 싫어한다\"며 다른 이름이지만 같은 심정으로 공감 대화를 나눴다. 그중 유독 눈에 띄는 한 남성 출연자의 기상천외한 이름이 밝혀져, 나머지 출연진은 물론 MC들도 당황하며 이름을 재차 물어 웃음을 자아냈다. 하지만 후에 밝혀진 이름 탄생 배경에는 마냥 웃을 수만은 없는 인생 스토리가 담겨있어 눈길을 끌기도 했다. 한편, 지난 10월 10일부터 방송된 \\'한명회\\'는 9회로 막을 내린다. 오는 12일부터 \\'한명회\\' 방송 시간에는 편성 시간이 변경된 \\'패키지로 세계일주-뭉쳐야 뜬다\\'가 전파를 탄다. \\'한명회\\' 마지막 회는 5일 오후 9시 30분에 방송된다. [사진 = JTBC 제공]- ⓒ마이데일리(www.mydaily.co.kr). 무단전재&재배포 금지 -',\n",
       " 'number_of_comment': 143,\n",
       " 'press': '마이데일리',\n",
       " 'rank': '33',\n",
       " 'real_number_of_comment': 121,\n",
       " 'site': 'daum',\n",
       " 'title': \"김국진·노홍철·한혜진 '한명회', 오늘(5일) 9회만에 종영\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['김국진', '노홍철', '한혜진', '한명회', '오늘', '5일', '9회', '종영']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.phrases(data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1240efb40a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vect1 = CountVectorizer(tokenizer=ct.phrases, ngram_range=(1,1),\n\u001b[0m\u001b[1;32m      2\u001b[0m                         stop_words=stopwords)\n\u001b[1;32m      3\u001b[0m \u001b[0mcount1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\. '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mainText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0midx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcount1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcount1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vect1 = CountVectorizer(tokenizer=ct.phrases, ngram_range=(1,1),\n",
    "                        stop_words=stopwords)\n",
    "count1 = vect1.fit_transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "idx1 = np.argsort(-count1)\n",
    "count1 = count1[idx1]\n",
    "feature_name1 = np.array(vect1.get_feature_names())[idx1]\n",
    "out1 = dict(zip(feature_name1, count1))\n",
    "sorted(out1.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect3 = CountVectorizer(tokenizer=ct.nouns, ngram_range=(1,2),\n",
    "                    stop_words=stopwords)\n",
    "count3 = vect3.fit_transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "idx3 = np.argsort(-count3)\n",
    "count3 = count3[idx3]\n",
    "feature_name3 = np.array(vect3.get_feature_names())[idx3]\n",
    "out3 = dict(zip(feature_name3, count3))\n",
    "sorted(out3.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect4 = CountVectorizer(tokenizer=ct.morphs, ngram_range=(1,3),\n",
    "                    stop_words=stopwords)\n",
    "count4 = vect4.fit_transform(re.sub('\\. ','.\\n', data['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "idx4 = np.argsort(-count4)\n",
    "count4 = count4[idx4]\n",
    "feature_name4 = np.array(vect4.get_feature_names())[idx4]\n",
    "out4 = dict(zip(feature_name4, count4))\n",
    "sorted(out4.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = Twitter()\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [(tokenize(row[1].decode('utf-8')), row[2]) for row in train_data[:3000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Extract_Maintext_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['본문'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = './data/news/high_frequency_noun/'\n",
    "path2 = './data/news/mainissue/'\n",
    "path3 = './data/news/newstopic/'\n",
    "path4 = './data/news/people/'\n",
    "path5 = './data/news/4th_industry/'\n",
    "path6 = './data/news/have_negative_positive/constitution/'\n",
    "path7 = './data/news/have_negative_positive/household_debt/'\n",
    "path8 = './data/news/have_negative_positive/olymphic/'\n",
    "path9 = './data/news/have_negative_positive/'\n",
    "path10 = './data/news/'\n",
    "path11 = './data/news/speaking/'\n",
    "path12 = './data/news/quote/'\n",
    "\n",
    "pathlist = [path1, path2, path3,\n",
    "            path4, path5, path6,\n",
    "            path7, path8, path9,\n",
    "           path10, path11, path12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                                 주소        일자  \\\n",
      "0           0  http://www.bigkinds.or.kr/news/newsDetailView....  20170701   \n",
      "1           1  http://www.bigkinds.or.kr/news/newsDetailView....  20170701   \n",
      "2           2  http://www.bigkinds.or.kr/news/newsDetailView....  20170701   \n",
      "3           3  http://www.bigkinds.or.kr/news/newsDetailView....  20170701   \n",
      "4           4  http://www.bigkinds.or.kr/news/newsDetailView....  20170701   \n",
      "\n",
      "     언론사     기고자                                        제목   통합 분류1  \\\n",
      "0   서울경제  고병기 기자    [부동산시장 뒤집어보기]같은 전략, 다른 결과에 웃다가 운 경매 시장   경제>부동산   \n",
      "1  헤럴드경제  김유진 기자               트럼프 인도 부동산, 모디 총리 세제 개편에 타격  경제>국제경제   \n",
      "2   중도일보  임붕순 기자               서산준법지원센터, 서산평생학습센터 시민로스쿨 실시       문화   \n",
      "3   매일경제     NaN  현대산업개발, 초역세권 아파트 ‘인덕 아이파크’ 견본주택에 실수요자 북적   경제>부동산   \n",
      "4  헤럴드경제  김우영 기자                     새로 생긴 길따라 아파트 단지 살펴볼까      NaN   \n",
      "\n",
      "          통합 분류2  통합 분류3 사건/사고 분류1 사건/사고 분류2 사건/사고 분류3  \\\n",
      "0      경제>금융_재테크     NaN       NaN       NaN       NaN   \n",
      "1         국제>아시아  경제>부동산       NaN       NaN       NaN   \n",
      "2  IT_과학>IT_과학일반     NaN       NaN       NaN       NaN   \n",
      "3      경제>서비스_쇼핑     NaN       NaN       NaN       NaN   \n",
      "4            NaN     NaN       NaN       NaN       NaN   \n",
      "\n",
      "                                                  본문  \\\n",
      "0  #작년 9월 거듭된 유찰로 매각에 난항을 겪던 서울 서대문구 동교동에 위치한 ‘옛 ...   \n",
      "1  -부동산 세제개편 우려에 분양가 1억 이상 낮춰 -불법ㆍ비리 연루 가능성 높은 부동...   \n",
      "2  [중도일보] 법무부 서산준법지원센터(서산보호관찰소, 소장 박현배)는 지난달 9일부터...   \n",
      "3  지난달 30일 현대산업개발이 서울 노원구 월계동에서 문을 연 ‘인덕 아이파크’ 모델...   \n",
      "4  ［헤럴드경제=김우영 기자］구리~포천 고속도로와 동서 고속도로 등 새 길이 잇달아 뚫...   \n",
      "\n",
      "                            개체명(인물)                              개체명(기업/기관)  \\\n",
      "0                               NaN         마스터자동차관리지지옥션THAAD린나이전원이앤씨아카시아호텔   \n",
      "1  라지브밀란 비슈나브도널드 트럼프나렌드라 모디모디카네기트럼프                  인도파이낸셜타임스로다그룹인디아FTAP연합   \n",
      "2                            박현배권준경                             법무부서산준법지원센터   \n",
      "3                               NaN  이마트2001아울렛홈플러스하나로마트중·고교현대산업개발월계로서울시광운대   \n",
      "4                               NaN                         의정부돈암동동해선부동산114   \n",
      "\n",
      "                                             개체명(지역)  \\\n",
      "0                             서울중국을지로이창동서대문구동교동서울 중구   \n",
      "1                                               미국인도   \n",
      "2                                              하나라서산   \n",
      "3  서울창동역강남구노원구월계동초안산근인공원중계동경기도삼성역중랑천수변공원하계역아시아퍼시픽...   \n",
      "4  신북면강릉서울경기망우역여주정릉대관령경기도성남성북구양주우이동포천시강북포항남양주광주인천...   \n",
      "\n",
      "                                               중요키워드  \n",
      "0  대성프라퍼티,린나이빌딩,경매_시장,경매_입찰,아카시아호텔_경매,채권자,린나이빌딩_매...  \n",
      "1  부동산,인도_부동산,트럼프_인도_부동산,트럼프_인도,현지_부동산,인도_현지,인도_부...  \n",
      "2  서산,시민로스쿨,박현배_소장,서산평생학습센터_시민로스쿨,서산평생학습센터,권준경,시민...  \n",
      "3  노원구,아이파크,서울시_노원구,현대산업개발,월계역,서울시,1호,지하철_1호,인덕_아...  \n",
      "4  청량리,9월_개통,고속도로,서울,성남,구리,동해선,우이동,개통_예정,강릉,동해선_영...  \n",
      "[(12, '본문')]\n"
     ]
    }
   ],
   "source": [
    "for i in pathlist:\n",
    "    fileList = glob(i+'*.csv')\n",
    "    fileList.extend(glob(i+'*.CSV'))\n",
    "    for ii in fileList:\n",
    "        df = pd.read_csv(ii, encoding='utf-8')\n",
    "        if '본문' in list(df.columns.values):\n",
    "            dff = df.copy()\n",
    "            print (dff.head())\n",
    "            f = list(filter(lambda x: x[1] in ['본문'], enumerate(dff.columns.values)))\n",
    "            print (f)\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywordList = []\n",
    "press = []\n",
    "ii = 0\n",
    "for data in useCollection.find({'site':'daum'}):\n",
    "    if len(data['mainText'].strip()) !=0:\n",
    "        ii +=1\n",
    "        if data['keywords'] !='NaN':\n",
    "            keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "        press.append(data['press'])\n",
    "        f = Out_Keywords(data['mainText'],stopwords)\n",
    "        print (f, data['keywords'])\n",
    "        if ii== 50:break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = list(map(lambda x: x[0], etri[0]))\n",
    "x2 = list(map(lambda x: x[0], etri[1]))\n",
    "x3 = list(map(lambda x: x[0], etri[2]))\n",
    "x4 = list(map(lambda x: x[0], etri[3]))\n",
    "x5 = list(map(lambda x: x[0], etri[4]))\n",
    "x6 = list(map(lambda x: x[0], etri[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('먼저', 3.1991445890560875),\n",
       " ('탄수화물', 2.6543833270054069),\n",
       " ('혈당', 2.1660505751638253),\n",
       " ('섭취', 2.0656146575606704),\n",
       " ('식사', 1.9807014416068316),\n",
       " ('밥', 1.8165329867679629),\n",
       " ('순서', 1.4554532208612323),\n",
       " ('때', 1.3101122459112935),\n",
       " ('육류', 1.2925200347613497),\n",
       " ('결과', 1.1068065846000599),\n",
       " ('반찬', 1.0941912368203977),\n",
       " ('음식', 1.0550180680448702),\n",
       " ('생선', 1.0275869795587738),\n",
       " ('방법', 1.0),\n",
       " ('약', 0.922033192262804),\n",
       " ('포만', 0.91666074702894129),\n",
       " ('쪽', 0.82471343292651123),\n",
       " ('모두', 0.82245878295543684),\n",
       " ('쌀밥', 0.81505517859491472),\n",
       " ('못하', 0.80727771423877104),\n",
       " ('상승', 0.79691245868892668),\n",
       " ('이후', 0.7686047624863166),\n",
       " ('지방', 0.74692960827456634),\n",
       " ('흡수', 0.74021466432853034),\n",
       " ('수', 0.7355702248518694),\n",
       " ('법', 0.72510846817730723),\n",
       " ('식품', 0.7002158190954183),\n",
       " ('거꾸로', 0.65147782389149533),\n",
       " ('많이', 0.63374184535807143),\n",
       " ('쌀', 0.58993722319706565),\n",
       " ('억제', 0.56217834627829988),\n",
       " ('그룹', 0.5481970489143585),\n",
       " ('그런', 0.54048114891124832),\n",
       " ('마지막', 0.54048114891124832),\n",
       " ('보다', 0.54032471679982597),\n",
       " ('인슐린', 0.52762868635072235),\n",
       " ('또', 0.50709710107334005),\n",
       " ('마다', 0.506522911916374),\n",
       " ('분비', 0.50427918191082832),\n",
       " ('이나', 0.46640917988434327),\n",
       " ('급격', 0.46572486272268732),\n",
       " ('세', 0.46536506753958468),\n",
       " ('가지', 0.46536506753958468),\n",
       " ('우선', 0.46536506753958468),\n",
       " ('식단', 0.46536506753958468),\n",
       " ('무엇', 0.46399261297506772),\n",
       " ('후', 0.46388319296605757),\n",
       " ('연구', 0.44996822227022204),\n",
       " ('과일', 0.44812266231956221),\n",
       " ('쓰이', 0.43829043885028612),\n",
       " ('비만', 0.43829043885028612),\n",
       " ('까지', 0.43720622174893059),\n",
       " ('천천히', 0.41844690246158567),\n",
       " ('특히', 0.41737197882473076),\n",
       " ('효과', 0.41737197882473076),\n",
       " ('높이', 0.41737197882473076),\n",
       " ('환자', 0.39562420572493134),\n",
       " ('건강', 0.39562420572493134),\n",
       " ('부터', 0.39428987469161214),\n",
       " ('대상', 0.39335148641192752),\n",
       " ('다이어트', 0.38673969192805063),\n",
       " ('본인', 0.38673969192805063),\n",
       " ('끼', 0.38673969192805063),\n",
       " ('미국', 0.38279412553245201),\n",
       " ('바', 0.38279412553245201),\n",
       " ('관련', 0.38279412553245201),\n",
       " ('경우', 0.37842651871449673),\n",
       " ('영향', 0.36711862515226645),\n",
       " ('움직임', 0.36711862515226645),\n",
       " ('걸리', 0.36711862515226645),\n",
       " ('위', 0.36711862515226645),\n",
       " ('사람', 0.36508695632469279),\n",
       " ('살', 0.36441312809713639),\n",
       " ('2', 0.36298966256824905),\n",
       " ('덜', 0.35440143837277066),\n",
       " ('실험', 0.35440143837277066),\n",
       " ('칼로리', 0.35440143837277066),\n",
       " ('유혹', 0.35440143837277066),\n",
       " ('김치', 0.34906317532869574),\n",
       " ('다음', 0.34906317532869574),\n",
       " ('국', 0.34906317532869574),\n",
       " ('나물', 0.34906317532869574),\n",
       " ('찌개', 0.34906317532869574),\n",
       " ('한식', 0.34906317532869574),\n",
       " ('예', 0.34906317532869574),\n",
       " ('소장', 0.34592478963691825),\n",
       " ('식이', 0.32014933440417631),\n",
       " ('실제로', 0.31664392346395603),\n",
       " ('급격히', 0.31664392346395603),\n",
       " ('역시', 0.30859790588359076),\n",
       " ('후식', 0.30859790588359076),\n",
       " ('공통', 0.30859790588359076),\n",
       " ('글루카곤', 0.27997750338633298),\n",
       " ('반대', 0.27997750338633298),\n",
       " ('췌장', 0.27997750338633298),\n",
       " ('인크레틴', 0.27997750338633298),\n",
       " ('증가', 0.27997750338633298),\n",
       " ('자극', 0.27997750338633298),\n",
       " ('보리', 0.25896238665833871),\n",
       " ('곡류', 0.25896238665833871),\n",
       " ('채소', 0.25896238665833871),\n",
       " ('소화', 0.25519744071155964),\n",
       " ('호르몬', 0.25519744071155964),\n",
       " ('관여', 0.25519744071155964),\n",
       " ('더', 0.25519744071155964),\n",
       " ('인르레틴', 0.25519744071155964),\n",
       " ('고기', 0.24401243356014746),\n",
       " ('계란', 0.21661548639032221),\n",
       " ('베이컨', 0.21661548639032221),\n",
       " ('완', 0.21661548639032221),\n",
       " ('싱크', 0.21661548639032221),\n",
       " ('남녀', 0.21661548639032221),\n",
       " ('브라이언', 0.21661548639032221),\n",
       " ('완 싱크', 0.21661548639032221),\n",
       " ('테이블', 0.21661548639032221),\n",
       " ('단백', 0.20784815940527351),\n",
       " ('30', 0.20638294807241858),\n",
       " ('치', 0.20638294807241858),\n",
       " ('코넬대', 0.20411022875941473),\n",
       " ('1', 0.20411022875941473),\n",
       " ('제', 0.20083553539021018),\n",
       " ('베', 0.20083553539021018),\n",
       " ('다이스케', 0.20083553539021018),\n",
       " ('졸임', 0.20083553539021018),\n",
       " ('각각', 0.20083553539021018),\n",
       " ('12', 0.20083553539021018),\n",
       " ('의학', 0.20083553539021018),\n",
       " ('고등어', 0.20083553539021018),\n",
       " ('형', 0.20083553539021018),\n",
       " ('석쇠', 0.20083553539021018),\n",
       " ('구이', 0.20083553539021018),\n",
       " ('10', 0.20083553539021018),\n",
       " ('정', 0.0),\n",
       " ('15', 0.0),\n",
       " ('점', 0.0),\n",
       " ('조사', 0.0),\n",
       " ('전력', 0.0),\n",
       " ('주', 0.0),\n",
       " ('량', 0.0),\n",
       " ('기', 0.0),\n",
       " ('전', 0.0),\n",
       " ('질', 0.0),\n",
       " ('식', 0.0),\n",
       " ('던', 0.0),\n",
       " ('데', 0.0),\n",
       " ('나타', 0.0),\n",
       " ('남', 0.0),\n",
       " ('낮', 0.0),\n",
       " ('느끼', 0.0),\n",
       " ('는가', 0.0),\n",
       " ('폭', 0.0),\n",
       " ('도', 0.0),\n",
       " ('팀', 0.0),\n",
       " ('는지', 0.0),\n",
       " ('124', 0.0),\n",
       " ('다이어트법', 0.0),\n",
       " ('당뇨', 0.0),\n",
       " ('대로', 0.0),\n",
       " ('적', 0.0),\n",
       " ('는다', 0.0),\n",
       " ('작용', 0.0),\n",
       " ('3', 0.0),\n",
       " ('일반', 0.0),\n",
       " ('에너지', 0.0),\n",
       " ('빠르', 0.0),\n",
       " ('어서', 0.0),\n",
       " ('양', 0.0),\n",
       " ('았', 0.0),\n",
       " ('빠지', 0.0),\n",
       " ('에서', 0.0),\n",
       " ('간사이', 0.0),\n",
       " ('빼', 0.0),\n",
       " ('고', 0.0),\n",
       " ('섬유', 0.0),\n",
       " ('소', 0.0),\n",
       " ('순', 0.0),\n",
       " ('게', 0.0),\n",
       " ('감', 0.0),\n",
       " ('분', 0.0),\n",
       " ('역할', 0.0),\n",
       " ('부', 0.0),\n",
       " ('막', 0.0),\n",
       " ('시키', 0.0),\n",
       " ('며', 0.0),\n",
       " ('이어지', 0.0),\n",
       " ('면', 0.0),\n",
       " ('4', 0.0),\n",
       " ('40', 0.0),\n",
       " ('명', 0.0),\n",
       " ('교수', 0.0),\n",
       " ('발표', 0.0),\n",
       " ('배', 0.0),\n",
       " ('별', 0.0),\n",
       " ('예방', 0.0),\n",
       " ('병', 0.0),\n",
       " ('영양', 0.0),\n",
       " ('류', 0.0),\n",
       " ('시간', 0.0)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect4 = TfidfVectorizer(tokenizer=ct.nouns, ngram_range=(1,4),\n",
    "                    stop_words=stopwords)\n",
    "vect4.fit(x1)\n",
    "#vect4.fit(x2)\n",
    "#vect4.fit(x3)\n",
    "#vect4.fit(x4)\n",
    "#vect4.fit(x5)\n",
    "count4 = vect4.transform(re.sub('\\. ','.\\n', rawdata['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "idx4 = np.argsort(-count4)\n",
    "count4 = count4[idx4]\n",
    "feature_name4 = np.array(vect4.get_feature_names())[idx4]\n",
    "out4 = dict(zip(feature_name4, count4))\n",
    "sorted(out4.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('먼저', 2.647840006657419),\n",
       " ('탄수화물', 2.1459401777922396),\n",
       " ('혈당', 1.9388114653496136),\n",
       " ('섭취', 1.8544457171625832),\n",
       " ('식이섬유', 1.7589750770325798),\n",
       " ('식사', 1.7216141555297726),\n",
       " ('밥', 1.6599826826000881),\n",
       " ('단백질 탄수화물', 1.6486512379756904),\n",
       " ('단백질', 1.5430532660345244),\n",
       " ('때', 1.1868717714773676),\n",
       " ('육류', 1.136065104807138),\n",
       " ('생선', 1.0704668272380691),\n",
       " ('반찬', 1.0386737408354931),\n",
       " ('결과', 1.0359573708415231),\n",
       " ('방법', 1.0),\n",
       " ('순서', 0.98891256179281828),\n",
       " ('식이섬유 단백질', 0.96231817371702055),\n",
       " ('식이섬유 단백질 탄수화물', 0.96231817371702055),\n",
       " ('약', 0.90940513474627638),\n",
       " ('음식', 0.89823743937253098),\n",
       " ('모두', 0.82078470549335925),\n",
       " ('당뇨병', 0.7917063814422356),\n",
       " ('많이', 0.78857451588652427),\n",
       " ('탄수화물 섭취', 0.77492111968885835),\n",
       " ('하면', 0.75217301280355808),\n",
       " ('지방', 0.70692934631864146),\n",
       " ('못하', 0.69871967978438132),\n",
       " ('흡수', 0.68864377975647906),\n",
       " ('밥 반찬', 0.6818659951352265),\n",
       " ('크게', 0.67329333214650322),\n",
       " ('식품', 0.66834153043089328),\n",
       " ('영양소', 0.63232628428393345),\n",
       " ('쌀밥', 0.62360273900643592),\n",
       " ('법', 0.60297893531210578),\n",
       " ('분비', 0.59386631041142401),\n",
       " ('인슐린', 0.5933342060893616),\n",
       " ('억제', 0.58958266547967209),\n",
       " ('이후', 0.57796166472595423),\n",
       " ('쌀', 0.54219390853477367),\n",
       " ('거꾸로', 0.53692716768577231),\n",
       " ('무엇', 0.53027882501072654),\n",
       " ('섭취 하면', 0.51792114573982451),\n",
       " ('하게', 0.48117522115953504),\n",
       " ('과일', 0.45287548327189647),\n",
       " ('사람들', 0.4404049267518213),\n",
       " ('마지막', 0.42981654104953515),\n",
       " ('육류 생선류', 0.42981654104953515),\n",
       " ('생선류', 0.42981654104953515),\n",
       " ('그런', 0.42981654104953515),\n",
       " ('그룹', 0.41573886344228445),\n",
       " ('쓰이 못하', 0.40644745208031119),\n",
       " ('쓰이', 0.40644745208031119),\n",
       " ('비만', 0.40644745208031119),\n",
       " ('천천히', 0.39117452294501059),\n",
       " ('탄수화물 섭취량', 0.36281689382743904),\n",
       " ('단백질 탄수화물 섭취량', 0.36281689382743904),\n",
       " ('또', 0.36281689382743904),\n",
       " ('섭취량', 0.36281689382743904),\n",
       " ('이나 육류', 0.3603475379750789),\n",
       " ('이나', 0.3603475379750789),\n",
       " ('연구', 0.3603475379750789),\n",
       " ('2그룹', 0.35576786751958445),\n",
       " ('칼로리', 0.35576786751958445),\n",
       " ('1그룹', 0.35576786751958445),\n",
       " ('덜', 0.35576786751958445),\n",
       " ('급격 하게', 0.35305778850118852),\n",
       " ('급격', 0.35305778850118852),\n",
       " ('바', 0.35258335069052299),\n",
       " ('미국', 0.35258335069052299),\n",
       " ('세', 0.33477425117082332),\n",
       " ('가지', 0.33477425117082332),\n",
       " ('세 가지', 0.33477425117082332),\n",
       " ('유혹', 0.3299628589973978),\n",
       " ('거꾸로 식사 법', 0.32700932560280421),\n",
       " ('거꾸로 식사', 0.32700932560280421),\n",
       " ('식사 법', 0.32700932560280421),\n",
       " ('환자', 0.32334358535144142),\n",
       " ('건강 사람', 0.32334358535144142),\n",
       " ('당뇨병 환자', 0.32334358535144142),\n",
       " ('건강', 0.32334358535144142),\n",
       " ('후', 0.32057758608316378),\n",
       " ('인슐린 분비', 0.31986819042033837),\n",
       " ('췌장', 0.31986819042033837),\n",
       " ('글루카곤', 0.31986819042033837),\n",
       " ('정한', 0.29755203311311013),\n",
       " ('본인 끼', 0.29755203311311013),\n",
       " ('본인', 0.29755203311311013),\n",
       " ('본인 끼 식사', 0.29755203311311013),\n",
       " ('끼 식사', 0.29755203311311013),\n",
       " ('끼', 0.29755203311311013),\n",
       " ('약 3배', 0.29746925681146846),\n",
       " ('3배', 0.29746925681146846),\n",
       " ('위 움직임', 0.29746925681146846),\n",
       " ('위', 0.29746925681146846),\n",
       " ('소장', 0.29746925681146846),\n",
       " ('움직임', 0.29746925681146846),\n",
       " ('보리', 0.28712735872169454),\n",
       " ('쌀 보리', 0.28712735872169454),\n",
       " ('대상', 0.28456245403834396),\n",
       " ('경우', 0.28325012786905795),\n",
       " ('국', 0.2801768926535822),\n",
       " ('일반적인 한식', 0.2801768926535822),\n",
       " ('일반적인', 0.2801768926535822),\n",
       " ('다음', 0.2801768926535822),\n",
       " ('찌개', 0.2801768926535822),\n",
       " ('나물', 0.2801768926535822),\n",
       " ('김치', 0.2801768926535822),\n",
       " ('국 찌개', 0.2801768926535822),\n",
       " ('한식', 0.2801768926535822),\n",
       " ('예', 0.2801768926535822),\n",
       " ('나물 김치', 0.2801768926535822),\n",
       " ('다이어트', 0.27596960970930151),\n",
       " ('살', 0.27596960970930151),\n",
       " ('영향', 0.27589283744389898),\n",
       " ('이러한 영향', 0.27589283744389898),\n",
       " ('이러한', 0.27589283744389898),\n",
       " ('30분', 0.27399811999108564),\n",
       " ('식사 30분', 0.27399811999108564),\n",
       " ('2배', 0.27399811999108564),\n",
       " ('소화', 0.27399811999108564),\n",
       " ('약 2배', 0.27399811999108564),\n",
       " ('사람', 0.27034291748430678),\n",
       " ('급격히', 0.26971447505933377),\n",
       " ('생선 이나', 0.26971447505933377),\n",
       " ('생선 이나 육류', 0.26971447505933377),\n",
       " ('연구 결과', 0.26971447505933377),\n",
       " ('곡류', 0.2663010710908747),\n",
       " ('2', 0.2639222073439188),\n",
       " ('후식 밥 반찬', 0.26034472379341966),\n",
       " ('후식 밥', 0.26034472379341966),\n",
       " ('역시', 0.26034472379341966),\n",
       " ('후식', 0.26034472379341966),\n",
       " ('인르레틴', 0.25412413904185377),\n",
       " ('연구팀', 0.24927771925794456),\n",
       " ('당뇨병 환자 건강 사람', 0.20452925586329937),\n",
       " ('당뇨병 환자 건강', 0.20452925586329937),\n",
       " ('혈당 치', 0.20452925586329937),\n",
       " ('치', 0.20452925586329937),\n",
       " ('환자 건강 사람', 0.20452925586329937),\n",
       " ('환자 건강', 0.20452925586329937),\n",
       " ('브라이언 완', 0.16574812455020194),\n",
       " ('1 그룹 계란', 0.16574812455020194),\n",
       " ('싱크', 0.16574812455020194),\n",
       " ('1 그룹 계란 베이컨', 0.16574812455020194),\n",
       " ('그룹 1 그룹 계란', 0.16574812455020194),\n",
       " ('식사 하게', 0.16574812455020194),\n",
       " ('브라이언 완 싱크', 0.16574812455020194),\n",
       " ('그룹 계란', 0.16574812455020194),\n",
       " ('그룹 계란 베이컨', 0.16574812455020194),\n",
       " ('계란', 0.16574812455020194),\n",
       " ('완', 0.16574812455020194),\n",
       " ('완 싱크', 0.16574812455020194),\n",
       " ('베이컨', 0.16574812455020194),\n",
       " ('브라이언', 0.16574812455020194),\n",
       " ('계란 베이컨', 0.16574812455020194),\n",
       " ('테이블', 0.1537258702741916),\n",
       " ('테이블 2', 0.1537258702741916),\n",
       " ('테이블 2 그룹', 0.1537258702741916),\n",
       " ('코넬대 연구팀', 0.1537258702741916),\n",
       " ('1 그룹', 0.1537258702741916),\n",
       " ('2 그룹', 0.1537258702741916),\n",
       " ('그룹 1', 0.1537258702741916),\n",
       " ('1', 0.1537258702741916),\n",
       " ('그룹 1 그룹', 0.1537258702741916),\n",
       " ('코넬대', 0.14519594511786393),\n",
       " ('졸임 이나 육류', 0.11881432948814204),\n",
       " ('졸임 이나', 0.11881432948814204),\n",
       " ('졸임', 0.11881432948814204),\n",
       " ('경우 생선 고등어', 0.11881432948814204),\n",
       " ('경우 생선', 0.11881432948814204),\n",
       " ('생선 고등어', 0.11881432948814204),\n",
       " ('4시간 후', 0.11881432948814204),\n",
       " ('생선 고등어 졸임 이나', 0.11881432948814204),\n",
       " ('석쇠', 0.11881432948814204),\n",
       " ('이나 육류 소고기 석쇠', 0.11881432948814204),\n",
       " ('이나 육류 소고기', 0.11881432948814204),\n",
       " ('석쇠 구이', 0.11881432948814204),\n",
       " ('졸임 이나 육류 소고기', 0.11881432948814204),\n",
       " ('부소장 연구팀', 0.11881432948814204),\n",
       " ('베 다이스케 부소장 연구팀', 0.11881432948814204),\n",
       " ('소고기', 0.11881432948814204),\n",
       " ('베 다이스케 부소장', 0.11881432948814204),\n",
       " ('베 다이스케', 0.11881432948814204),\n",
       " ('베', 0.11881432948814204),\n",
       " ('경우 생선 고등어 졸임', 0.11881432948814204),\n",
       " ('15분', 0.11881432948814204),\n",
       " ('고등어', 0.11881432948814204),\n",
       " ('고등어 졸임', 0.11881432948814204),\n",
       " ('고등어 졸임 이나', 0.11881432948814204),\n",
       " ('고등어 졸임 이나 육류', 0.11881432948814204),\n",
       " ('구이', 0.11881432948814204),\n",
       " ('다이스케 부소장 연구팀', 0.11881432948814204),\n",
       " ('다이스케 부소장', 0.11881432948814204),\n",
       " ('다이스케', 0.11881432948814204),\n",
       " ('부소장', 0.11881432948814204),\n",
       " ('의학 연구 소의 베', 0.11881432948814204),\n",
       " ('각각', 0.11881432948814204),\n",
       " ('의학 연구', 0.11881432948814204),\n",
       " ('4시간', 0.11881432948814204),\n",
       " ('간사이전력', 0.11881432948814204),\n",
       " ('간사이전력 의학', 0.11881432948814204),\n",
       " ('연구 소의', 0.11881432948814204),\n",
       " ('연구 소의 베', 0.11881432948814204),\n",
       " ('연구 소의 베 다이스케', 0.11881432948814204),\n",
       " ('간사이전력 의학 연구', 0.11881432948814204),\n",
       " ('의학 연구 소의', 0.11881432948814204),\n",
       " ('소의 베 다이스케 부소장', 0.11881432948814204),\n",
       " ('소의 베 다이스케', 0.11881432948814204),\n",
       " ('소의 베', 0.11881432948814204),\n",
       " ('간사이전력 의학 연구 소의', 0.11881432948814204),\n",
       " ('생선 고등어 졸임', 0.11881432948814204),\n",
       " ('육류 소고기 석쇠', 0.11881432948814204),\n",
       " ('육류 소고기', 0.11881432948814204),\n",
       " ('소고기 석쇠 구이', 0.11881432948814204),\n",
       " ('소고기 석쇠', 0.11881432948814204),\n",
       " ('육류 소고기 석쇠 구이', 0.11881432948814204),\n",
       " ('소의', 0.11881432948814204),\n",
       " ('의학', 0.11881432948814204),\n",
       " ('그런 후', 0.0),\n",
       " ('단백질 탄수화물 순', 0.0),\n",
       " ('에너지', 0.0),\n",
       " ('교수', 0.0),\n",
       " ('반찬 순', 0.0),\n",
       " ('역할', 0.0),\n",
       " ('양', 0.0),\n",
       " ('미국 코넬대', 0.0),\n",
       " ('포만감', 0.0),\n",
       " ('싱크 교수', 0.0),\n",
       " ('식이섬유 단백질 탄수화물 순', 0.0),\n",
       " ('억제 역할', 0.0),\n",
       " ('공통점', 0.0),\n",
       " ('전', 0.0),\n",
       " ('분비 양', 0.0),\n",
       " ('15분 전', 0.0),\n",
       " ('후식 밥 반찬 순', 0.0),\n",
       " ('30분 후', 0.0),\n",
       " ('영양소 별', 0.0),\n",
       " ('식사 30분 후', 0.0),\n",
       " ('별', 0.0),\n",
       " ('시간', 0.0),\n",
       " ('순서 다이어트', 0.0),\n",
       " ('순', 0.0),\n",
       " ('완 싱크 교수', 0.0),\n",
       " ('인슐린 분비 양', 0.0),\n",
       " ('정한 순서', 0.0),\n",
       " ('브라이언 완 싱크 교수', 0.0),\n",
       " ('탄수화물 순', 0.0),\n",
       " ('밥 반찬 순', 0.0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect4 = TfidfVectorizer(tokenizer=ct.nouns, ngram_range=(1,4),\n",
    "                    stop_words=stopwords)\n",
    "vect4.fit(x1)\n",
    "vect4.fit(x2)\n",
    "vect4.fit(x3)\n",
    "vect4.fit(x4)\n",
    "vect4.fit(x5)\n",
    "count4 = vect4.transform(re.sub('\\. ','.\\n', rawdata['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "idx4 = np.argsort(-count4)\n",
    "count4 = count4[idx4]\n",
    "feature_name4 = np.array(vect4.get_feature_names())[idx4]\n",
    "out4 = dict(zip(feature_name4, count4))\n",
    "sorted(out4.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('탄수화물', 2.652335239011482),\n",
       " ('먼저', 2.4179824299316746),\n",
       " ('혈당', 2.3602571608616172),\n",
       " ('식이섬유', 2.2175272857255952),\n",
       " ('단백질', 2.2175272857255952),\n",
       " ('식사', 1.8872972053190649),\n",
       " ('생선', 1.8636578729641378),\n",
       " ('순서', 1.6025836028749274),\n",
       " ('육류', 1.5038089788191193),\n",
       " ('반찬', 1.3741627744056613),\n",
       " ('결과', 1.2912052314802238),\n",
       " ('음식', 1.0897177618204064),\n",
       " ('모두', 1.0303623867958787),\n",
       " ('사람', 1.0177645503207056),\n",
       " ('당뇨병', 1.0013885040260471),\n",
       " ('방법', 1.0),\n",
       " ('흡수', 0.88096878833416103),\n",
       " ('크게', 0.85025408269105274),\n",
       " ('탄수화물 순', 0.84490825272125814),\n",
       " ('지방', 0.83906911620484981),\n",
       " ('영양소', 0.83383561820569918),\n",
       " ('쌀밥', 0.76189358774877203),\n",
       " ('다이어트', 0.7081738331720937),\n",
       " ('억제', 0.69873344549256999),\n",
       " ('포만', 0.69602909827030424),\n",
       " ('포만감', 0.69602909827030424),\n",
       " ('연구', 0.6588459372987524),\n",
       " ('이후', 0.65877316918563777),\n",
       " ('과일', 0.64192227117676137),\n",
       " ('분비', 0.6282627118996873),\n",
       " ('30', 0.62790748288999576),\n",
       " ('거꾸로 식사법', 0.62286355415526962),\n",
       " ('무엇', 0.59548008297116284),\n",
       " ('거꾸로', 0.58830216582113803),\n",
       " ('탄수화물 섭취', 0.57819661015758628),\n",
       " ('2그룹', 0.5718476658751952),\n",
       " ('섭취', 0.53625811650822774),\n",
       " ('마지막', 0.53188747294697825),\n",
       " ('생선류', 0.53188747294697825),\n",
       " ('밥과 반찬', 0.51796368496099432),\n",
       " ('코넬대', 0.51253035579157924),\n",
       " ('그룹', 0.50649726665524031),\n",
       " ('시간', 0.49038557002686756),\n",
       " ('환자', 0.48422280758593195),\n",
       " ('에너지', 0.48303521604464378),\n",
       " ('비만', 0.48303521604464378),\n",
       " ('사람들', 0.48039408263071681),\n",
       " ('가지', 0.42421890511782295),\n",
       " ('세 가지', 0.42421890511782295),\n",
       " ('경우', 0.42418089728532804),\n",
       " ('칼로리', 0.42165396952243844),\n",
       " ('보리', 0.41078474524604824),\n",
       " ('본인의 한 끼 식사', 0.40961671308787623),\n",
       " ('본인', 0.40961671308787623),\n",
       " ('끼 식사', 0.40961671308787623),\n",
       " ('대상', 0.40249732571043018),\n",
       " ('급격', 0.40189217281660883),\n",
       " ('찌개', 0.3954578395542771),\n",
       " ('다음', 0.3954578395542771),\n",
       " ('한식', 0.3954578395542771),\n",
       " ('김치', 0.3954578395542771),\n",
       " ('나물', 0.3954578395542771),\n",
       " ('나물과 김치', 0.3954578395542771),\n",
       " ('탄수화물 섭취량', 0.392658374562812),\n",
       " ('단백질과 탄수화물 섭취량', 0.392658374562812),\n",
       " ('유혹', 0.39107002625403364),\n",
       " ('움직임', 0.38843881217501031),\n",
       " ('소장', 0.38843881217501031),\n",
       " ('약 3배', 0.38843881217501031),\n",
       " ('위의 움직임', 0.38843881217501031),\n",
       " ('연구결과', 0.38551520867666361),\n",
       " ('곡류', 0.38098918240962937),\n",
       " ('영향', 0.36026407304410102),\n",
       " ('미국', 0.35394041945228227),\n",
       " ('미국 코넬대', 0.35394041945228227),\n",
       " ('순서 다이어트', 0.32826796172305894),\n",
       " ('식품', 0.31872558404940871),\n",
       " ('공통점', 0.31763467566845993),\n",
       " ('후식', 0.31763467566845993),\n",
       " ('반찬 순', 0.31763467566845993),\n",
       " ('역시', 0.31763467566845993),\n",
       " ('30분 후', 0.31504447508378092),\n",
       " ('소화', 0.31504447508378092),\n",
       " ('식사 30분 후', 0.31504447508378092),\n",
       " ('억제하는 역할', 0.31321823681590633),\n",
       " ('췌장', 0.31321823681590633),\n",
       " ('글루카곤', 0.31321823681590633),\n",
       " ('분비 양', 0.31321823681590633),\n",
       " ('인슐린 분비 양', 0.31321823681590633),\n",
       " ('역할', 0.31321823681590633),\n",
       " ('당뇨병 환자', 0.31286300780621484),\n",
       " ('incretin', 0.29219326757849029),\n",
       " ('인르레틴', 0.29219326757849029),\n",
       " ('인슐린', 0.29049949235286454),\n",
       " ('계란과 베이컨 등', 0.23113752593071307),\n",
       " ('완싱크 교수', 0.23113752593071307),\n",
       " ('베이컨 등', 0.23113752593071307),\n",
       " ('싱크', 0.23113752593071307),\n",
       " ('베이컨', 0.23113752593071307),\n",
       " ('교수', 0.23113752593071307),\n",
       " ('과 계란과 베이컨 등', 0.23113752593071307),\n",
       " ('브라이언 완싱크 교수', 0.23113752593071307),\n",
       " ('브라이언', 0.23113752593071307),\n",
       " ('과 계란', 0.23113752593071307),\n",
       " ('계란', 0.23113752593071307),\n",
       " ('테이블', 0.21437236423124945),\n",
       " ('코넬대 연구팀', 0.21437236423124945),\n",
       " ('1그룹', 0.2024772927041471),\n",
       " ('졸임', 0.17135979977971713),\n",
       " ('전력', 0.17135979977971713),\n",
       " ('15분 전', 0.17135979977971713),\n",
       " ('소고기', 0.17135979977971713),\n",
       " ('이나', 0.17135979977971713),\n",
       " ('고등어', 0.17135979977971713),\n",
       " ('고등어 졸임', 0.17135979977971713),\n",
       " ('구이', 0.17135979977971713),\n",
       " ('다이스케', 0.17135979977971713),\n",
       " ('다이스케 부소장 등 연구팀', 0.17135979977971713),\n",
       " ('등 연구팀', 0.17135979977971713),\n",
       " ('간사이전력 의학연구소', 0.17135979977971713),\n",
       " ('간사이', 0.17135979977971713),\n",
       " ('각각', 0.17135979977971713),\n",
       " ('이나 육류', 0.17135979977971713),\n",
       " ('베 다이스케 부소장 등 연구팀', 0.17135979977971713),\n",
       " ('부소장 등 연구팀', 0.17135979977971713),\n",
       " ('석쇠', 0.17135979977971713),\n",
       " ('석쇠 구이', 0.17135979977971713),\n",
       " ('경우와 생선', 0.17135979977971713),\n",
       " ('소고기 석쇠 구이', 0.17135979977971713),\n",
       " ('연구소', 0.17135979977971713),\n",
       " ('의학', 0.17135979977971713),\n",
       " ('부소장', 0.17135979977971713),\n",
       " ('15', 0.17135979977971713),\n",
       " ('4시간 후', 0.0),\n",
       " ('약 2배', 0.0),\n",
       " ('영양소별', 0.0),\n",
       " ('혈당치', 0.0)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect4 = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1,1),\n",
    "                    stop_words=stopwords)\n",
    "vect4.fit(x1)\n",
    "vect4.fit(x2)\n",
    "vect4.fit(x3)\n",
    "vect4.fit(x4)\n",
    "vect4.fit(x5)\n",
    "count4 = vect4.transform(re.sub('\\. ','.\\n', rawdata['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "idx4 = np.argsort(-count4)\n",
    "count4 = count4[idx4]\n",
    "feature_name4 = np.array(vect4.get_feature_names())[idx4]\n",
    "out4 = dict(zip(feature_name4, count4))\n",
    "sorted(out4.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('탄수화물', 2.652335239011482),\n",
       " ('먼저', 2.4179824299316746),\n",
       " ('혈당', 2.3602571608616172),\n",
       " ('식이섬유', 2.2175272857255952),\n",
       " ('단백질', 2.2175272857255952),\n",
       " ('식사', 1.8872972053190649),\n",
       " ('생선', 1.8636578729641378),\n",
       " ('순서', 1.6025836028749274),\n",
       " ('육류', 1.5038089788191193),\n",
       " ('반찬', 1.3741627744056613),\n",
       " ('결과', 1.2912052314802238),\n",
       " ('음식', 1.0897177618204064),\n",
       " ('모두', 1.0303623867958787),\n",
       " ('사람', 1.0177645503207056),\n",
       " ('당뇨병', 1.0013885040260471),\n",
       " ('방법', 1.0),\n",
       " ('흡수', 0.88096878833416103),\n",
       " ('크게', 0.85025408269105274),\n",
       " ('탄수화물 순', 0.84490825272125814),\n",
       " ('지방', 0.83906911620484981),\n",
       " ('영양소', 0.83383561820569918),\n",
       " ('쌀밥', 0.76189358774877203),\n",
       " ('다이어트', 0.7081738331720937),\n",
       " ('억제', 0.69873344549256999),\n",
       " ('포만', 0.69602909827030424),\n",
       " ('포만감', 0.69602909827030424),\n",
       " ('연구', 0.6588459372987524),\n",
       " ('이후', 0.65877316918563777),\n",
       " ('과일', 0.64192227117676137),\n",
       " ('분비', 0.6282627118996873),\n",
       " ('30', 0.62790748288999576),\n",
       " ('거꾸로 식사법', 0.62286355415526962),\n",
       " ('무엇', 0.59548008297116284),\n",
       " ('거꾸로', 0.58830216582113803),\n",
       " ('탄수화물 섭취', 0.57819661015758628),\n",
       " ('2그룹', 0.5718476658751952),\n",
       " ('섭취', 0.53625811650822774),\n",
       " ('마지막', 0.53188747294697825),\n",
       " ('생선류', 0.53188747294697825),\n",
       " ('밥과 반찬', 0.51796368496099432),\n",
       " ('코넬대', 0.51253035579157924),\n",
       " ('그룹', 0.50649726665524031),\n",
       " ('시간', 0.49038557002686756),\n",
       " ('환자', 0.48422280758593195),\n",
       " ('에너지', 0.48303521604464378),\n",
       " ('비만', 0.48303521604464378),\n",
       " ('사람들', 0.48039408263071681),\n",
       " ('가지', 0.42421890511782295),\n",
       " ('세 가지', 0.42421890511782295),\n",
       " ('경우', 0.42418089728532804),\n",
       " ('칼로리', 0.42165396952243844),\n",
       " ('보리', 0.41078474524604824),\n",
       " ('본인의 한 끼 식사', 0.40961671308787623),\n",
       " ('본인', 0.40961671308787623),\n",
       " ('끼 식사', 0.40961671308787623),\n",
       " ('대상', 0.40249732571043018),\n",
       " ('급격', 0.40189217281660883),\n",
       " ('찌개', 0.3954578395542771),\n",
       " ('다음', 0.3954578395542771),\n",
       " ('한식', 0.3954578395542771),\n",
       " ('김치', 0.3954578395542771),\n",
       " ('나물', 0.3954578395542771),\n",
       " ('나물과 김치', 0.3954578395542771),\n",
       " ('탄수화물 섭취량', 0.392658374562812),\n",
       " ('단백질과 탄수화물 섭취량', 0.392658374562812),\n",
       " ('유혹', 0.39107002625403364),\n",
       " ('움직임', 0.38843881217501031),\n",
       " ('소장', 0.38843881217501031),\n",
       " ('약 3배', 0.38843881217501031),\n",
       " ('위의 움직임', 0.38843881217501031),\n",
       " ('연구결과', 0.38551520867666361),\n",
       " ('곡류', 0.38098918240962937),\n",
       " ('영향', 0.36026407304410102),\n",
       " ('미국', 0.35394041945228227),\n",
       " ('미국 코넬대', 0.35394041945228227),\n",
       " ('순서 다이어트', 0.32826796172305894),\n",
       " ('식품', 0.31872558404940871),\n",
       " ('공통점', 0.31763467566845993),\n",
       " ('후식', 0.31763467566845993),\n",
       " ('반찬 순', 0.31763467566845993),\n",
       " ('역시', 0.31763467566845993),\n",
       " ('30분 후', 0.31504447508378092),\n",
       " ('소화', 0.31504447508378092),\n",
       " ('식사 30분 후', 0.31504447508378092),\n",
       " ('억제하는 역할', 0.31321823681590633),\n",
       " ('췌장', 0.31321823681590633),\n",
       " ('글루카곤', 0.31321823681590633),\n",
       " ('분비 양', 0.31321823681590633),\n",
       " ('인슐린 분비 양', 0.31321823681590633),\n",
       " ('역할', 0.31321823681590633),\n",
       " ('당뇨병 환자', 0.31286300780621484),\n",
       " ('incretin', 0.29219326757849029),\n",
       " ('인르레틴', 0.29219326757849029),\n",
       " ('인슐린', 0.29049949235286454),\n",
       " ('계란과 베이컨 등', 0.23113752593071307),\n",
       " ('완싱크 교수', 0.23113752593071307),\n",
       " ('베이컨 등', 0.23113752593071307),\n",
       " ('싱크', 0.23113752593071307),\n",
       " ('베이컨', 0.23113752593071307),\n",
       " ('교수', 0.23113752593071307),\n",
       " ('과 계란과 베이컨 등', 0.23113752593071307),\n",
       " ('브라이언 완싱크 교수', 0.23113752593071307),\n",
       " ('브라이언', 0.23113752593071307),\n",
       " ('과 계란', 0.23113752593071307),\n",
       " ('계란', 0.23113752593071307),\n",
       " ('테이블', 0.21437236423124945),\n",
       " ('코넬대 연구팀', 0.21437236423124945),\n",
       " ('1그룹', 0.2024772927041471),\n",
       " ('졸임', 0.17135979977971713),\n",
       " ('전력', 0.17135979977971713),\n",
       " ('15분 전', 0.17135979977971713),\n",
       " ('소고기', 0.17135979977971713),\n",
       " ('이나', 0.17135979977971713),\n",
       " ('고등어', 0.17135979977971713),\n",
       " ('고등어 졸임', 0.17135979977971713),\n",
       " ('구이', 0.17135979977971713),\n",
       " ('다이스케', 0.17135979977971713),\n",
       " ('다이스케 부소장 등 연구팀', 0.17135979977971713),\n",
       " ('등 연구팀', 0.17135979977971713),\n",
       " ('간사이전력 의학연구소', 0.17135979977971713),\n",
       " ('간사이', 0.17135979977971713),\n",
       " ('각각', 0.17135979977971713),\n",
       " ('이나 육류', 0.17135979977971713),\n",
       " ('베 다이스케 부소장 등 연구팀', 0.17135979977971713),\n",
       " ('부소장 등 연구팀', 0.17135979977971713),\n",
       " ('석쇠', 0.17135979977971713),\n",
       " ('석쇠 구이', 0.17135979977971713),\n",
       " ('경우와 생선', 0.17135979977971713),\n",
       " ('소고기 석쇠 구이', 0.17135979977971713),\n",
       " ('연구소', 0.17135979977971713),\n",
       " ('의학', 0.17135979977971713),\n",
       " ('부소장', 0.17135979977971713),\n",
       " ('15', 0.17135979977971713),\n",
       " ('4시간 후', 0.0),\n",
       " ('약 2배', 0.0),\n",
       " ('영양소별', 0.0),\n",
       " ('혈당치', 0.0)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect4 = TfidfVectorizer(tokenizer=ct.phrases, ngram_range=(1,1),\n",
    "                    stop_words=stopwords)\n",
    "vect4.fit(x1)\n",
    "vect4.fit(x2)\n",
    "vect4.fit(x3)\n",
    "vect4.fit(x4)\n",
    "vect4.fit(x5)\n",
    "count4 = vect4.transform(re.sub('\\. ','.\\n', rawdata['mainText']).split('\\n')).toarray().sum(axis = 0)\n",
    "idx4 = np.argsort(-count4)\n",
    "count4 = count4[idx4]\n",
    "feature_name4 = np.array(vect4.get_feature_names())[idx4]\n",
    "out4 = dict(zip(feature_name4, count4))\n",
    "sorted(out4.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('을', 'JKO'), 29),\n",
       " (('먹', 'VV'), 28),\n",
       " (('.', 'SF'), 25),\n",
       " (('를', 'JKO'), 22),\n",
       " (('이', 'JKS'), 20),\n",
       " (('는', 'ETM'), 18),\n",
       " ((',', 'SP'), 17),\n",
       " (('다', 'EF'), 16),\n",
       " (('먼저', 'MAG'), 15),\n",
       " (('ㄴ', 'ETM'), 14)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(etri[1]).most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('먼저', ''), 10),\n",
       " (('먹는', ''), 9),\n",
       " (('먹은', ''), 9),\n",
       " (('있다.', ''), 5),\n",
       " ((\"먼저'\", ''), 5),\n",
       " (('식사를', ''), 4),\n",
       " (('혈당이', ''), 4),\n",
       " (('약', ''), 4),\n",
       " (('후에', ''), 3),\n",
       " (('순서를', ''), 3)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(etri[2]).most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('탄수화물', 'MT_CHEMICAL'), 8),\n",
       " (('단백질', 'MT_CHEMICAL'), 7),\n",
       " (('생선', 'CV_FOOD'), 6),\n",
       " (('밥', 'CV_FOOD'), 5),\n",
       " (('육류', 'CV_FOOD'), 4),\n",
       " (('쌀밥', 'CV_FOOD'), 4),\n",
       " (('당뇨병', 'TMM_DISEASE'), 3),\n",
       " (('쌀', 'CV_FOOD'), 2),\n",
       " (('인슐린', 'MT_CHEMICAL'), 2),\n",
       " (('한 끼', 'QT_COUNT'), 1)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(etri[3]).most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('먼저', 'ARGM-ADV'), 10),\n",
       " (('식품', 'ARG2'), 6),\n",
       " (('혈당', 'ARG1'), 6),\n",
       " (('식이섬유', 'ARG1'), 5),\n",
       " ((\"먼저'\", 'ARGM-TMP'), 5),\n",
       " (('식사', 'ARG1'), 3),\n",
       " (('순서', 'ARG1'), 3),\n",
       " (('밥', 'ARG1'), 3),\n",
       " (('것', 'ARG1'), 3),\n",
       " (('쌀밥', 'ARG1'), 3)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(etri[4]).most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Extract_Keywords(text, stopwords):\n",
    "    etri = Run_ETRI_Analysis(stopwords, text)\n",
    "    soy1 = RunLRNounExtractor(text)\n",
    "    soy2 = RunWordExtractor(text)\n",
    "    et0 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[0]))\n",
    "    et0 = list(map(lambda x: x[0], et0))\n",
    "    et1 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[1]))\n",
    "    et1 = list(map(lambda x: x[0], et1))\n",
    "    et2 = list(map(lambda x: x[0], etri[2]))\n",
    "    et3 = list(map(lambda x: x[0], etri[3]))\n",
    "    et4 = list(filter(lambda x: x[1] in ['ARG0','ARG1','ARG2','ARG3','ARG4'], etri[4]))\n",
    "    et4 = list(map(lambda x: x[0], et4))\n",
    "    et5 = list(filter(lambda x: x[1] in ['NP','NP_SBJ','NP_OBJ','NP_AJT','VNP'], etri[5]))\n",
    "    et5 = list(map(lambda x: x[0], et5))\n",
    "    mecabout = list(filter(lambda x: x[1] in ['NNG','NNB','NNP'], mecab.pos(text)))\n",
    "    ctout = list(filter(lambda x: x[1] == 'Noun' , ct.pos(text)))\n",
    "    otout = list(filter(lambda x: x[1] == 'Noun' , ot.pos(text)))\n",
    "    mcout = list(map(lambda x: x[0], mecabout))\n",
    "    ctout = list(map(lambda x: x[0], ctout))\n",
    "    otout = list(map(lambda x: x[0], otout))\n",
    "    out = list(soy1.keys())+list(soy2.keys())+ctout+otout+mcout+et3+et4+et0+et1+et2+et5\n",
    "    vect = TfidfVectorizer().fit(out)\n",
    "    y = [list(soy1.keys()), list(soy2.keys()),et3, et4, ctout, otout, mcout,et0, et1, et2, et5]\n",
    "    y = list(filter(lambda x: len(x) !=0, y))\n",
    "    outdict = dict()\n",
    "    for i in y:\n",
    "        count = vect.transform(i).toarray().sum(axis = 0)\n",
    "        idx = np.argsort(-count)\n",
    "        count = count[idx]\n",
    "        feature_name = np.array(vect.get_feature_names())[idx]\n",
    "        out = dict(zip(feature_name, count))\n",
    "        for ii in out:\n",
    "            if not ii in outdict:\n",
    "                outdict[ii] = out[ii]\n",
    "            else:\n",
    "                outdict[ii] +=out[ii]\n",
    "    x = sorted(outdict.items(), key = itemgetter(1), reverse=True)[:5]\n",
    "    output = list(map(lambda x: x[0], x))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(map(lambda x: x[0], f)) & set(out.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "#keywords(rawdata['mainText'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'탄수화물\\n혈당이\\n먹으면\\n단백질\\n식사를\\n거꾸로\\n쌀밥을\\n순으로 음식을\\n것으로\\n나눈다\\n그룹보다\\n결과를\\n생선을\\n당뇨병까지\\n다르다\\n환자나\\n한식을\\n식이섬유\\n글루카곤은 억제하는\\n국이나 찌개를\\n인르레틴\\n관여하는\\n걸리는\\n흡수될\\n비슷한\\n코넬대'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords(rawdata['mainText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때 순서를 식이섬유→단백질→탄수화물 순으로 음식을 섭취하면 된다.\n",
      "먹는 순서를 식이섬유→단백질→탄수화물 순으로 바꾸면 혈당이 급격하게 상승하는 것을 막고, 빠른 포만감을 준다.\n",
      "거꾸로 식사법은 후식→밥·반찬 순으로 식사를 하는 것인데, 역시 식이섬유를 먼저 섭취하고 이후 단백질·탄수화물을 섭취한다는 공통점이 있다.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from textrankr import TextRank\n",
    "\n",
    "textrank = TextRank(rawdata['mainText'])\n",
    "print(textrank.summarize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data in useCollection.find({'site':'daum'}):\n",
    "    if len(data['mainText'].strip()) !=0:\n",
    "        if data['keywords'] !='NaN':\n",
    "            ct.add_dictionary(list(map(lambda x: x.split(' · ')[-1], data['keywords'])), 'Noun')\n",
    "        y = Extract_Keywords2(data['mainText'],stopwords)\n",
    "        z = TextRank(data['mainText'])\n",
    "        p = RunLRNounExtractor(data['mainText'])\n",
    "        q = RunWordExtractor(data['mainText'])\n",
    "        #print (y)\n",
    "        print (data['keywords'])\n",
    "        print (z.keywords())\n",
    "        #print (p.keys())\n",
    "        #print (q.keys())\n",
    "        print ()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lexrankr import LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexrank = LexRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.cohesion_backward, m.cohesion_forward, m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding = 'utf-8', mode='r') as f:\n",
    "        data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
