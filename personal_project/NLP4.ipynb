{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 빅데이터 분석 데이터 추출 & 사용자 사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath = 'C:/Users/pc/Anaconda3/lib/site-packages/ckonlpy/data/twitter/noun/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './data/nnews/high_frequency_noun/'\n",
    "path2 = './data/news/mainissue/'\n",
    "path3 = './data/news/newstopic/'\n",
    "path4 = './data/news/people/'\n",
    "path5 = './data/news/4th_industry/'\n",
    "path6 = './data/news/have_negative_positive/constitution/'\n",
    "path7 = './data/news/have_negative_positive/household_debt/'\n",
    "path8 = './data/news/have_negative_positive/olymphic/'\n",
    "path9 = './data/news/have_negative_positive/'\n",
    "path10 = './data/news/'\n",
    "\n",
    "pathlist = [path1, path2, path3,\n",
    "            path4, path5, path6,\n",
    "            path7, path8, path9,\n",
    "           path10]\n",
    "if not os.path.isfile(dbpath+'from_news.txt'):\n",
    "    with open(dbpath+'from_news.txt','w') as f:\n",
    "        for path in pathlist:\n",
    "            out = Extract_Keyword_from_news_bigdata(path)\n",
    "            f.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Mecab, Twitter\n",
    "from konlpy.utils import pprint\n",
    "import os\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwords = Stopwords('./data/koreanStopwords.txt') +Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512, 5068471, 5532]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "list(map(lambda x: len(xxxx[x]), xxxx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "sys.path.append('/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/')\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import time, re, pickle, itertools\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from multiprocessing import Pool\n",
    "import urllib3, json\n",
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.hangle import normalize\n",
    "import os\n",
    "\n",
    "def OpenAPI():\n",
    "    import pickle\n",
    "    openapi = pickle.load(open('./etri_open_api_access_key', 'rb'))\n",
    "    return openapi\n",
    "def USE_ETRI_ANALYSIS(analysisCode, text):    \n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    requestJson = {\n",
    "        'access_key' : accessKey,\n",
    "        'argument' : {\n",
    "            'text' : text,\n",
    "            'analysis_code' : analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "    return json.loads(str(response.data,'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_INFO(fWord, sWord):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/WordRel\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    firstWord = fWord\n",
    "    #firstSenseId = fSenseId\n",
    "    secondWord = sWord\n",
    "    #secondSenseId = sSenseId\n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "        'first_word': firstWord,\n",
    "        #'first_sense_id': firstSenseId,\n",
    "        'second_word': secondWord,\n",
    "        #'second_sense_id': secondSenseId\n",
    "        }}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "    body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_RELATION(word):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/Word\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    word = word\n",
    "    requestJson = {\n",
    "    \"access_key\": accessKey,\n",
    "    \"argument\": {\"word\": word}}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "                            body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def Extract_Text_Info(idx):\n",
    "    morp = list(map(lambda x: (x['lemma'], x['type']),idx['morp']))\n",
    "    wsd = list(map(lambda x: (x['text'], x['type']),idx['WSD']))\n",
    "    word = list(map(lambda x: (x['text'], x['type']),idx['word']))\n",
    "    ne = list(map(lambda x: (x['text'], x['type']),idx['NE']))\n",
    "    return morp, wsd, word, ne\n",
    "\n",
    "def Run_ETRI_Analysis(stopwordsList, text):\n",
    "    etri = USE_ETRI_ANALYSIS('srl', text)['return_object']    \n",
    "    etri = etri['sentence']\n",
    "    morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "    for i in etri:\n",
    "        x = Extract_Text_Info(i)\n",
    "        morp += x[0]\n",
    "        wsd += x[1]\n",
    "        word +=x[2]\n",
    "        ne += x[3]\n",
    "        for ii in i['SRL']:\n",
    "            srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "        dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "    return morp, wsd, word, ne, srl, dependency, etri\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexrankr import LexRank\n",
    "lexrank = LexRank()\n",
    "lexrank.summarize()\n",
    "lr = lexrank.probe(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.twitter = ct\n",
    "        #self.twitter = ot\n",
    "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "        ,\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]    \n",
    "    def text2sentences(self, text):\n",
    "        sentences = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                #nouns.append(' '.join([noun for noun in self.twitter.phrases(str(sentence))\n",
    "                #                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = sknorm(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=4):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keyword(etri_outcome, data):\n",
    "    testdata = pickle.load(open(etri_outcome+data['_id']._ObjectId__id.hex()+'.picked.txt','rb'))\n",
    "    etri = testdata['return_object']['sentence']\n",
    "    morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "    for i in etri:\n",
    "        x = Extract_Text_Info(i)\n",
    "        morp += x[0]\n",
    "        wsd += x[1]\n",
    "        word +=x[2]\n",
    "        ne += x[3]\n",
    "        for ii in i['SRL']:\n",
    "            srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "        dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "    morp1 = list(map(lambda x: x[0], morp))\n",
    "    wsd1 = list(map(lambda x: x[0], wsd))\n",
    "    word1 = list(map(lambda x: x[0], word))\n",
    "    ne1 = list(map(lambda x: x[0], ne))\n",
    "    srl1 = list(map(lambda x: x[0], srl))\n",
    "    dependency1 = list(map(lambda x: x[0], dependency))\n",
    "    outlist = []\n",
    "    for idx in range(len(etri)):\n",
    "        y = pd.DataFrame(etri[idx]['dependency'])\n",
    "        y2 = y[y['mod'].apply(lambda x: len(x)) ==y['mod'].apply(lambda x: len(x)).max()]\n",
    "        y3 = y2[y2['weight'] == y2['weight'].max()]\n",
    "        y4 = y[y['id'].isin(y3['mod'][y3['mod'].index[0]])]\n",
    "        y5 = y[y['weight'] ==y4['weight'].max()]\n",
    "        y6 = y2.text.values.tolist() + y5.text.values.tolist()\n",
    "        y7 = y[y.text.isin(y6)]    \n",
    "        out = ' '.join(y7.text.tolist())\n",
    "        outlist.append(out)\n",
    "    sText = data['title']+'. '+data['mainText']\n",
    "    sText = '.\\n'.join(sText.split('. '))\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2),\n",
    "                      norm='l2',\n",
    "                      stop_words=stopwords)\n",
    "    vect.fit(morp1)\n",
    "    vect.fit(wsd1)\n",
    "    vect.fit(word1)\n",
    "    vect.fit(ne1)\n",
    "    vect.fit(srl1)\n",
    "    vect.fit(outlist)\n",
    "    vect.fit(dependency1)\n",
    "    #vect.fit(ct.nouns(sText))\n",
    "    count1 = vect.fit_transform([sText]).toarray().sum(axis = 0)\n",
    "    ch = ct.nouns(data['title'])+ct.phrases(data['title'])+outlist\n",
    "    ch2 = list(map(lambda x: re.sub('[\\W]',' ', x), ch))\n",
    "    ch2 = list(map(lambda x: x.strip(), ch2))\n",
    "    ch3 = list(filter(lambda x: x[1] in ch2, enumerate(vect.get_feature_names())))\n",
    "    y =  [1] * len(vect.get_feature_names())\n",
    "    for ix in ch3:\n",
    "        y[ix[0]] = 1.5\n",
    "    count1 = count1 * y\n",
    "    idx1 = np.argsort(-count1)\n",
    "    count1 = count1[idx1]\n",
    "    feature_name1 = np.array(vect.get_feature_names())[idx1]\n",
    "    out1 = dict(zip(feature_name1, count1))\n",
    "    return sorted(out1.items(), key=lambda x: x[1], reverse=True)[:7]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한명회', '이름', '이름을', '방송'] [('한명회', 0.4845437118523489), ('이름을', 0.21535276082326621), ('막을 내린다', 0.16151457061744967), ('5일', 0.16151457061744967), ('이름', 0.1076763804116331), ('했다', 0.1076763804116331), ('막을', 0.1076763804116331)] ['노홍철', '한명회', '한혜진', '종영', '편성시간']\n",
      "\n",
      "['캠프', '선수', '신인', '류중일'] [('류중일', 0.30292214689575725), ('있다', 0.24233771751660582), ('신인', 0.24233771751660582), ('캠프', 0.24233771751660582), ('했다', 0.20194809793050483), ('2차', 0.12116885875830291), ('없다', 0.12116885875830291)] ['류중일']\n",
      "\n",
      "['좌윤', '상무', '비서', '으로'] [('좌윤이는', 0.26711222531767115), ('봉상무', 0.23372319715296225), ('보스', 0.20033416898825335), ('봉상무의', 0.16694514082354445), ('저글러스', 0.15025062674119), ('좌윤이를', 0.10016708449412667), ('좌윤이가', 0.10016708449412667)] ['불륜', '비', '첫방', '최다니엘', '최대철']\n",
      "\n",
      "['기억', '장항준', '영화', '스릴러'] [('장항준', 0.35097629199405966), ('기억의', 0.26741050818595025), ('걱정을', 0.13370525409297512), ('감독이', 0.13370525409297512), ('감독은', 0.13370525409297512), ('장항준 감독이', 0.13370525409297512), ('제가', 0.10027894056973134)] ['기억의밤', '시나리오', '리얼리티', '강하늘', '김무열']\n",
      "\n",
      "['정상', '강호동', '훈은', '낚시'] [('정상훈은', 0.25615775978927996), ('낚시', 0.23054198381035196), ('섬총사', 0.23054198381035196), ('강호동과', 0.20492620783142398), ('정상훈이', 0.10246310391571199), ('농어를', 0.10246310391571199), ('강호동과 정상훈은', 0.10246310391571199)] ['강호동', '정상훈', '조세호', '어청도', '달타냥']\n",
      "\n",
      "['불법', '영화', '유포', '범죄'] [('불법', 0.45514956368175619), ('범죄도시', 0.37929130306813019), ('불법 행위에', 0.1517165212272521), ('대한', 0.15171652122725207), ('아니라', 0.10114434748483472), ('밝혔다', 0.10114434748483472), ('키위미디어그룹은', 0.10114434748483472)] ['윤계상', '마동석', '강경대응', 'sns']\n",
      "\n",
      "['박지헌', '당신', '이제', '사랑'] [('박지헌', 0.23816525581123343), ('정말', 0.23816525581123343), ('여섯째', 0.23816525581123343), ('당신이', 0.21170244960998524), ('당신을', 0.15877683720748895), ('정말 고마워', 0.15877683720748892), ('앞둔', 0.15877683720748892)] ['출산', '혼인신고', '해시태그', '인스타그램', '결혼식']\n",
      "\n",
      "['광고', '다니엘', '전광판', '팬들'] [('타임스퀘어', 0.27944785969957342), ('광고', 0.27944785969957336), ('뉴욕', 0.20958589477468004), ('전광판', 0.20958589477468004), ('아이돌', 0.18629857313304893), ('강다니엘의', 0.18629857313304893), ('팬들이', 0.13972392984978668)] ['타임스스퀘어', '아이돌', '뉴욕', '워너원', '연말시상식']\n",
      "\n",
      "['혈당', '먼저', '식사', '생선'] [('먼저', 0.39847982127139125), ('먹는', 0.26565321418092752), ('먹은', 0.23908789276283474), ('먼저 먹은', 0.21252257134474201), ('단백질', 0.13282660709046376), ('있다', 0.13282660709046376), ('탄수화물', 0.13282660709046376)] ['당뇨병', '탄수화물', '식이섬유']\n",
      "\n",
      "['김가연', '임요환', '부부', '사진을'] [('임요환', 0.35464968280759529), ('김가연과', 0.15762208124782012), ('김가연과 임요환', 0.15762208124782012), ('김가연은', 0.15762208124782012), ('지금에', 0.15762208124782012), ('지금에 감사하자', 0.15762208124782012), ('사진을', 0.15762208124782012)] ['이지현', '눈길', '혼인신고', '인스타그램', '결혼식']\n",
      "\n",
      "['이창', '명은', '상고', '항소심'] [('너무', 0.23693955110363693), ('이창명은', 0.23693955110363693), ('가족들', 0.17770466332772769), ('이창명', 0.17770466332772769), ('지난', 0.11846977555181847), ('정말', 0.11846977555181847), ('박아름', 0.11846977555181847)] ['이창명', '본격연예한밤', '무죄판결', '음주운전', '교통사고']\n",
      "\n",
      "['식당', '박서준', '시즌', '리포트'] [('윤식당2', 0.6092717958449424), ('tv리포트', 0.17407765595569785), ('배우', 0.17407765595569785), ('5일', 0.17407765595569782), ('촬영을 마치고', 0.11605177063713189), ('때문에', 0.11605177063713189), ('동안', 0.11605177063713189)] ['이서진', '정유미', '윤여정', '알쓸신잡2', '윤식당']\n",
      "\n",
      "['가희', '소식', '둘째', '임신'] [('둘째', 0.41731116305599292), ('격려', 0.25038669783359579), ('건강히', 0.22256595362986289), ('있다', 0.16692446522239718), ('임신 소식을', 0.16692446522239718), ('크고', 0.16692446522239718), ('둘째 임신', 0.16692446522239718)] ['임신', '애프터스쿨', '인스타그램']\n",
      "\n",
      "['온유', '샤이니', '으로', '시즌'] [('샤이니', 0.55844039082349051), ('그리팅', 0.16922436085560316), ('시즌', 0.16922436085560316), ('시즌 그리팅', 0.16922436085560316), ('불매', 0.13537948868448255), ('온유는', 0.13537948868448255), ('팬들은', 0.13537948868448255)] ['샤이니', 'sm', '사과문', 'sm엔터테인먼트', '아이돌그룹']\n",
      "\n",
      "['김지민', '어머니', '웃음', '하고'] [('라고', 0.23947373603569985), ('비행소녀', 0.17960530202677488), ('김지민은', 0.17960530202677488), ('어머니는', 0.11973686801784993), ('웃음을', 0.11973686801784993), ('김지민이', 0.11973686801784993), ('mkculture', 0.11973686801784993)] ['김지민', '돌직구', '데이트', '양세찬', '허지웅']\n",
      "\n",
      "['결혼', '정인영', '하차', '빅리그'] [('정인영의', 0.19405386820594528), ('빅리그', 0.19405386820594528), ('코미디', 0.19405386820594528), ('코미디 빅리그', 0.19405386820594528), ('결혼', 0.19405386820594525), ('개인', 0.19405386820594525), ('없다', 0.12936924547063017)] ['결혼', '코빅', '키', '결혼식', 'tvn']\n",
      "\n",
      "['영입', '추진', '투수', '김근'] [('삼성', 0.36380343755449945), ('추진', 0.36380343755449945), ('김근한의 골든크로스', 0.36380343755449945), ('린드블럼은 보험', 0.36380343755449945), ('보험', 0.36380343755449945), ('투수', 0.36380343755449945), ('좋은 투수', 0.24253562503633297)] ['조쉬 린드블럼', '삼성 라이온즈']\n",
      "\n",
      "['판청', '엔터테인먼트', '판빙빙', '연습생'] [('연습생', 0.29867549021998741), ('소속 연습생', 0.19911699347999162), ('동생', 0.19911699347999162), ('판빙빙', 0.19911699347999162), ('소속', 0.19911699347999162), ('판청청이', 0.19911699347999159), ('위에화', 0.19911699347999159)] ['판빙빙', '연습생', '프로듀스101', '우주소녀', '키']\n",
      "\n",
      "['자이언티', '겨울', '이문세', '모든'] [('신곡', 0.27705425792376615), ('자이언티의', 0.18470283861584408), ('이문세가', 0.18470283861584408), ('자이언티', 0.13852712896188307), ('이문세', 0.13852712896188307), ('위해', 0.13852712896188307), ('있는', 0.13852712896188304)] ['자이언티', '발라드곡', '음원차트', '피처링', '엠넷']\n",
      "\n",
      "['정글', '박세리', '법칙', '제작'] [('정글', 0.31222908534136001), ('박세리는', 0.16652217884872533), ('박세리가', 0.124891634136544), ('제작진의', 0.124891634136544), ('관심을 보인', 0.124891634136544), ('모습에', 0.124891634136544), ('박세리', 0.124891634136544)] ['박세리', '닐슨코리아', '김병만', '리얼', '승부욕']\n",
      "\n",
      "['자신', '둥지', '탈출', '김가연'] [('둥지탈출2', 0.24743582965269675), ('44세에', 0.1649572197684645), ('된다', 0.1649572197684645), ('자신의', 0.1649572197684645), ('포기하지', 0.1649572197684645), ('tvn', 0.1649572197684645), ('임산부', 0.12371791482634838)] ['선우재덕', '임신', '출산', '배낭여행', '폴란드']\n",
      "\n",
      "['해양플랜트', '싱가포르', '중국', '한국'] [('한국', 0.3173088897381372), ('있다', 0.30219894260774971), ('해양플랜트', 0.15109947130387485), ('세계', 0.12087957704309989), ('국내', 0.090659682782324919), ('25달러', 0.090659682782324919), ('가격', 0.090659682782324919)] ['해양플랜트', '침몰', '수주']\n",
      "\n",
      "['시즌', '보상', '시장', 'fa'] [('없는', 0.21473125083800759), ('최준석', 0.16104843812850569), ('팀의', 0.16104843812850569), ('보상선수', 0.16104843812850569), ('채태인', 0.16104843812850569), ('있다', 0.16104843812850569), ('크게', 0.1073656254190038)] ['채태인', '최준석']\n",
      "\n",
      "['핀란드', '친구들', '한국', '한국에'] [('핀란드', 0.23200591622629663), ('페트리', 0.21750554646215309), ('한국', 0.21750554646215309), ('빌레', 0.17400443716972247), ('한국에', 0.17400443716972247), ('딘딘', 0.11600295811314831), ('대해', 0.11600295811314831)] ['핀란드', '어서와', '딘딘', '김준현', '한국어']\n",
      "\n",
      "['예산', '여야', '조원', '확대'] [('예산', 0.3480826751385972), ('복지예산', 0.31643879558054289), ('soc', 0.29534287587517338), ('확대', 0.18986327734832575), ('복지', 0.18986327734832575), ('여야의', 0.16876735764295622), ('내년도', 0.12657551823221716)] ['이슈 · 2018 예산안 본회의 통과', '복지예산', '예산', 'soc']\n",
      "\n",
      "['가스', '지역', '공급', '중국'] [('가스', 0.57822504282643272), ('중국', 0.2753452584887775), ('있다', 0.1652071550932665), ('지역', 0.13767262924438875), ('석탄', 0.13767262924438875), ('난방', 0.110138103395511), ('것이다', 0.110138103395511)] ['lng', '석탄', '스모그']\n",
      "\n",
      "['정근', 'fa', '한화', '번즈'] [('있다', 0.21821789023599239), ('fa', 0.17457431218879391), ('않는다', 0.17457431218879391), ('정근우가', 0.13093073414159545), ('없다', 0.13093073414159545), ('번즈의', 0.087287156094396953), ('sk', 0.087287156094396953)] ['정근우', '한화 이글스']\n",
      "\n",
      "['귀순', 'cnn', '교수', '병사'] [('긴박했던', 0.16342041321085299), ('담겼습니다', 0.16342041321085299), ('이국종', 0.10894694214056866), ('있습니다 이국종', 0.10894694214056866), ('귀순병은', 0.10894694214056866), ('귀순병을', 0.10894694214056866), ('의료진이', 0.10894694214056866)] ['아주대병원', '혈압', '이국종교수']\n",
      "\n",
      "['이렇게', '미국', '북한', '이런'] [('이렇게', 0.3539938602808404), ('임상훈', 0.27070118727358383), ('정관용', 0.2498780190217697), ('있다', 0.21864326664404848), ('이런', 0.20823168251814142), ('문재인', 0.20302589045518787), ('미국', 0.18740851426632729)] ['이슈 · 북한 핵·미사일 도발', '문재인', '정관용', '트럼프']\n",
      "\n",
      "['원장', '국정원', '으로', '이자'] [('국정원', 0.58593217998845892), ('것으로', 0.24857728847995225), ('원장이', 0.14204416484568699), ('원장과', 0.10653312363426524), ('검찰은', 0.10653312363426524), ('것으로 알려졌다', 0.10653312363426524), ('댓글부대', 0.071022082422843497)] ['이슈 · 국정원·군 정치개입 의혹', '원세훈', '국가정보원', '원전']\n",
      "\n",
      "['석현준', '리그', '트루아', '프랑스'] [('즐라탄', 0.26017211742598512), ('석현준은', 0.2081376939407881), ('석현준의', 0.17344807828399006), ('골을', 0.17344807828399006), ('석현준이', 0.17344807828399006), ('한국의 즐라탄', 0.15610327045559108), ('리그', 0.13875846262719205)] NaN\n",
      "\n",
      "['수색', '해상', '작업', '선창1호'] [('영흥도', 0.22815905695352723), ('선창1호', 0.22815905695352723), ('수색', 0.20280805062535753), ('선장', 0.15210603796901814), ('인근', 0.15210603796901814), ('인천', 0.15210603796901814), ('이날', 0.15210603796901814)] ['이슈 · 영흥도 낚싯배 전복사고', '갯벌', '영흥도', '선창1호']\n",
      "\n",
      "['a씨', '갑자기', '바다', '어떻게'] [('어떻게', 0.28291257605786913), ('배가', 0.25147784538477258), ('묻자', 0.18860838403857944), ('a씨는', 0.18860838403857944), ('갑자기', 0.12573892269238629), ('있는', 0.12573892269238629), ('대답했다', 0.12573892269238629)] ['생존자', '낚싯배', '유조선']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['변호', '이영', '이영학', '사람'] [('그는', 0.23398215222964938), ('이영학이', 0.20473438320094323), ('이영학의', 0.17548661417223704), ('있다', 0.17548661417223704), ('이영학', 0.13161496062917777), ('말했다', 0.087743307086118522), ('약을', 0.087743307086118522)] [\"이슈 · '어금니 아빠' 살인사건\", '이영학', '여중생', '서울구치소']\n",
      "\n",
      "['구조', '으로', '대통령', '세월호'] [('박근혜', 0.21972288386821306), ('대통령은', 0.17903346092965508), ('대통령의', 0.16275769175423188), ('사고', 0.14648192257880871), ('대통령', 0.14648192257880871), ('세월호', 0.14648192257880871), ('지난', 0.13020615340338551)] ['이슈 · 영흥도 낚싯배 전복사고', '문재인', '박근혜', '세월호']\n",
      "\n",
      "['복귀', '폭격', '가능성', '관건'] [('도미니카', 0.36380343755449945), ('복귀 가능성', 0.36380343755449945), ('가능성', 0.36380343755449945), ('복귀', 0.36380343755449945), ('몸값', 0.36380343755449945), ('리즈', 0.36380343755449945), ('높은 몸값', 0.36380343755449945)] NaN\n",
      "\n",
      "['음식', '하루', '다이어트', '섭취'] [('하루', 0.32114446885847342), ('12시간', 0.1835111250619848), ('먹는', 0.15292593755165401), ('먹고', 0.12234075004132321), ('섭취', 0.12234075004132321), ('하는', 0.12234075004132321), ('음식 섭취', 0.12234075004132321)] ['비만', '체중', '다이어트']\n",
      "\n",
      "['사기', '원을', '혐의로', '1억'] [('사기', 0.33384893044479436), ('사기 혐의로', 0.22256595362986289), ('혐의로', 0.22256595362986289), ('가수', 0.16692446522239718), ('내가', 0.11128297681493145), ('집행유예', 0.11128297681493145), ('지난', 0.11128297681493145)] ['김동현', '인순이', '투자사기', '무죄판결', '이승현']\n",
      "\n",
      "['최다빈', '랭킹', '피겨스케이팅', '유영'] [('트리플', 0.29677162009769875), ('더블', 0.22257871507327406), ('최다빈은', 0.19784774673179917), ('토루프', 0.14838581004884938), ('유영', 0.14838581004884938), ('유영은', 0.12365484170737448), ('평창', 0.11128935753663703)] ['김연아', '유영']\n",
      "\n",
      "['선수', '보류선수', '명단', '시즌'] [('보류선수', 0.28478588590558696), ('있다', 0.19935012013391087), ('자유계약선수', 0.11391435436223479), ('선수는', 0.11391435436223479), ('넥센', 0.11391435436223479), ('한다', 0.11391435436223479), ('명단', 0.11391435436223479)] NaN\n",
      "\n",
      "['부장판사', '구속적부심', '결과', '면서'] [('부장판사는', 0.17102764992360511), ('있다', 0.12827073744270384), ('국방부', 0.12827073744270384), ('공개', 0.12827073744270384), ('현직', 0.12827073744270384), ('구속', 0.12827073744270384), ('김관진', 0.085513824961802556)] ['구속적부심', '대법원장', '인천지법']\n",
      "\n",
      "['부장', '그룹', '해성', '인물'] [('황금빛', 0.24935149047701477), ('궁금증을', 0.16623432698467652), ('인물이', 0.16623432698467652), ('민부장', 0.16623432698467652), ('해성그룹', 0.1246757452385074), ('민부장이', 0.1246757452385074), ('민부장의', 0.1246757452385074)] ['서지수', '노명희', '유인영', '신혜선', '나영희']\n",
      "\n",
      "['북한', '주민', '평양', '존스'] [('북한', 0.47104120318731579), ('있는', 0.15701373439577193), ('사진', 0.11776030079682895), ('같은', 0.11776030079682893), ('존스', 0.11776030079682893), ('모습', 0.11776030079682893), ('찍은', 0.078506867197885966)] ['존스', '조선중앙통신', '배추']\n",
      "\n",
      "['도르트문트', '박주호', '계약을', '해지'] [('키커', 0.33593550657351257), ('이에', 0.16796775328675631), ('박주호는', 0.16796775328675631), ('계약을', 0.16796775328675631), ('박주호', 0.16796775328675628), ('도르트문트', 0.16796775328675628), ('있다', 0.11197850219117086)] NaN\n",
      "\n",
      "['러시아', '올림픽', '안은', '빅토르'] [('올림픽', 0.27589195979164111), ('러시아', 0.2574991624721984), ('빅토르', 0.22071356783331292), ('안은', 0.18392797319442741), ('빅토르 안은', 0.18392797319442741), ('출전', 0.14714237855554194), ('있다', 0.14714237855554194)] ['빅토르 안']\n",
      "\n",
      "['동탁', '수창', '지안', '수사'] [('동탁 수창', 0.29884009507691373), ('동탁', 0.29884009507691373), ('수창', 0.29884009507691373), ('투깝스', 0.19922673005127584), ('기어코', 0.099613365025637918), ('조정석', 0.099613365025637918), ('제대로', 0.099613365025637918)] ['조정석', '오현종', '조항준', '살인누명', '닐슨코리아']\n",
      "\n",
      "['토트넘', '감독', '포체티노', '으로'] [('포체티노', 0.41602514716892186), ('감독은', 0.19810721293758182), ('일레븐', 0.15848577035006547), ('베스트 일레븐', 0.15848577035006547), ('베스트', 0.15848577035006547), ('포체티노 감독은', 0.15848577035006547), ('토트넘은', 0.15848577035006547)] ['포체티노']\n",
      "\n",
      "['때문', '앵커', '사고', '인터뷰'] [('때문에', 0.36398055336191365), ('인터뷰', 0.3006795875598417), ('앵커', 0.28485434610932375), ('지금', 0.12660193160414387), ('가장', 0.12660193160414387), ('것으로', 0.12660193160414387), ('이렇게', 0.1107766901536259)] ['이슈 · 영흥도 낚싯배 전복사고', '낚싯배', '영흥도', '급유선']\n",
      "\n",
      "['리튬', '중국', '배터리', '전기차'] [('리튬', 0.66452147071914014), ('중국', 0.37972655469665151), ('있다', 0.25315103646443432), ('전기차', 0.16876735764295622), ('최대', 0.16876735764295622), ('세계', 0.14767143793758669), ('배터리', 0.14767143793758669)] ['리튬', '전기차', '배터리']\n",
      "\n",
      "['부부', '남편', '주말', '으로'] [('주말', 0.27727604922646965), ('남편은', 0.1848506994843131), ('우리는', 0.13863802461323482), ('지옥', 0.13863802461323482), ('집에', 0.13863802461323482), ('했다', 0.11553168717769569), ('집을', 0.11553168717769569)] ['대중교통', '빌라', '월세']\n",
      "\n",
      "['성은', '최대', '지난', '두산'] [('같다', 0.28848737838984756), ('항상', 0.14424368919492378), ('모습을', 0.14424368919492378), ('팀에', 0.10818276689619283), ('없을', 0.10818276689619283), ('하는', 0.10818276689619283), ('최대성은', 0.10818276689619283)] NaN\n",
      "\n",
      "['직장', '119', '갑질', '가면'] [('갑질', 0.21148042285817176), ('라고', 0.18504537000090029), ('직장갑질119', 0.18504537000090029), ('다른', 0.13217526428635734), ('때까지', 0.13217526428635734), ('제가', 0.10574021142908588), ('라는', 0.10574021142908588)] ['갑질', '공유', '폭언']\n",
      "\n",
      "['시즌', 'lg', '올해', '허프'] [('일본', 0.23994625805661898), ('투수', 0.15996417203774599), ('시즌', 0.15996417203774599), ('올해', 0.15996417203774599), ('허프', 0.1439677548339714), ('계약을', 0.12797133763019677), ('좋은', 0.12797133763019677)] ['데이비드 허프']\n",
      "\n",
      "['선발', '한화', '가운데', '올해'] [('있다', 0.21524880100025257), ('선발', 0.21524880100025257), ('한화', 0.16143660075018942), ('있는', 0.14349920066683505), ('올해', 0.14349920066683505), ('10승', 0.10762440050012628), ('평균자책', 0.10762440050012628)] NaN\n",
      "\n",
      "['승격', '면서', '울산', '우승'] [('이승엽', 0.29304924943671135), ('fa컵', 0.23443939954936907), ('부산은', 0.15629293303291272), ('울산', 0.11721969977468455), ('부산', 0.11721969977468455), ('마리', 0.11721969977468455), ('시즌', 0.11721969977468454)] NaN\n",
      "\n",
      "['아스널', '가드', '맨유', '제세'] [('기록한', 0.21580500037968192), ('아스널', 0.21580500037968192), ('맨유', 0.16185375028476146), ('받았다', 0.14387000025312127), ('린가드', 0.14387000025312127), ('멀티골을', 0.10790250018984096), ('다비드', 0.10790250018984096)] ['박지성', '린가드', '데 헤아']\n",
      "\n",
      "['라며', '경은', '최도', '도경'] [('최도경은', 0.3965873843153086), ('황금빛', 0.1982936921576543), ('장소라는', 0.1762610596956927), ('최도경', 0.13219579477176954), ('서지안은', 0.13219579477176954), ('유인영', 0.13219579477176951), ('박시후', 0.13219579477176951)] ['신혜선', '유인영', '박시후', '장소라', '결혼']\n",
      "\n",
      "['해지', '방어', '콜센터', '그런데'] [('해지', 0.63443240165672032), ('방어', 0.25377296066268812), ('해지 방어', 0.25377296066268812), ('통신사', 0.19032972049701607), ('방통위는', 0.084590986887562711), ('겁니다', 0.084590986887562711), ('요금을', 0.084590986887562711)] ['방송통신위원회', '콜센터', 'lg유플러스']\n",
      "\n",
      "['000', '1분', '기록', '으로'] [('1분', 0.34914862437758781), ('월드컵', 0.19639610121239318), ('1분 8초', 0.17457431218879391), ('8초', 0.17457431218879391), ('기록으로', 0.13093073414159545), ('여자', 0.13093073414159545), ('열린', 0.13093073414159545)] ['고다이라 나오']\n",
      "\n",
      "['추신수', '하원미', '미국', '내조'] [('있는', 0.1936007731655916), ('이방인', 0.17424069584903243), ('내조의', 0.15488061853247326), ('하원미는', 0.11616046389935496), ('떨어져', 0.11616046389935496), ('추신수는', 0.11616046389935496), ('아내', 0.11616046389935494)] ['하원미', '추신수', '이불', '결혼생활', '야구선수']\n",
      "\n",
      "['하게', '드림팀', 'kbs', '제가'] [('거죠', 0.22649125823527516), ('제가', 0.22649125823527516), ('드림팀', 0.22649125823527516), ('하고', 0.12582847679737508), ('하게', 0.10066278143790007), ('출발', 0.10066278143790007), ('출발 드림팀', 0.10066278143790007)] ['이창명', '음악캠프', '출연료', '짜장면', '바로']\n",
      "\n",
      "['대위', '교도소', '정해', '인이'] [('감빵생활', 0.26699649569399087), ('유대위', 0.26699649569399087), ('tvn', 0.13349824784699543), ('슬기로운', 0.13349824784699543), ('악마 유대위', 0.13349824784699543), ('악마', 0.13349824784699543), ('교도소', 0.13349824784699543)] ['정해인', '교도소', '신원호', '박해수', '지호']\n",
      "\n",
      "['신혜선', '이름을', '황금', '인생'] [('황금빛', 0.29524069878307374), ('신혜선', 0.22143052408730529), ('있다', 0.19682713252204917), ('kbs2', 0.19682713252204917), ('인생', 0.19682713252204917), ('황금빛 인생', 0.14762034939153687), ('신혜선이', 0.14762034939153687)] ['신혜선', '연기인생', '성훈', '비밀의숲', '눈길']\n",
      "\n",
      "['김장', '장신', '시어머니', '예비'] [('김장', 0.29242210729714757), ('기로', 0.14621105364857379), ('너는 운명', 0.14621105364857379), ('너는', 0.14621105364857379), ('운명', 0.14621105364857379), ('김치', 0.14621105364857379), ('예비', 0.14621105364857379)] ['장신영', '너는내운명', '동상이몽', '양파', '시어머니']\n",
      "\n",
      "['장항준', '감독', '자신', '영화'] [('장항준', 0.51783822240252375), ('장항준 감독은', 0.21576592600105154), ('감독은', 0.21576592600105154), ('명문대', 0.19418933340094641), ('드라마', 0.19418933340094641), ('자신의', 0.12945955560063094), ('출신', 0.12945955560063094)] ['강하늘', '김무열', '기억의밤', 'tv드라마', '살인사건']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['야구선수', '앵커', '으로', '이승엽'] [('이승엽', 0.57998658549941284), ('앵커', 0.53855897224945481), ('야구선수', 0.35903931483296986), ('이승엽 야구선수', 0.3038024971663591), ('제가', 0.13809204416652687), ('손석희', 0.1242828397498742), ('저는', 0.11047363533322149)] ['이승엽']\n",
      "\n",
      "['효연', '캠핑', '공개', '10년'] [('캠핑', 0.36997584958692442), ('데뷔', 0.27748188719019329), ('10년', 0.18498792479346221), ('살미도', 0.18498792479346221), ('효연은', 0.18498792479346221), ('데뷔 10년', 0.18498792479346221), ('만에', 0.18498792479346221)] ['캠핑', '살짝미쳐도좋아', '눈길', '소녀시대', '반려견']\n",
      "\n",
      "['지사', '대통령', '발언을', '지지'] [('대통령', 0.29528042171615398), ('지사의', 0.26247148596991465), ('대통령이', 0.16404467873119666), ('발언은', 0.13123574298495733), ('있는', 0.098426807238717995), ('발언을', 0.098426807238717995), ('지지자들은', 0.098426807238717995)] ['안희정', '문재인', '경선']\n",
      "\n",
      "['앞둔', '집권', '2기', 'kia'] [('2기', 0.36380343755449945), ('앞둔', 0.36380343755449945), ('김기태 감독', 0.36380343755449945), ('집권', 0.36380343755449945), ('길이', 0.36380343755449945), ('김기태', 0.36380343755449945), ('감독', 0.36380343755449945)] ['김기태']\n",
      "\n",
      "['북한', '바다', '시신', '백골'] [('북한', 0.41372837849387506), ('것이다', 0.19597660033920397), ('물고기를', 0.15242624470826976), ('지난해', 0.15242624470826976), ('발견된', 0.13065106689280265), ('해안에서', 0.10887588907733554), ('시신', 0.10887588907733554)] ['취재파일', '물고기', '김정은']\n",
      "\n",
      "['손동운', '하이라이트', '회사', '이름을'] [('51', 0.20000000000000001), ('라디오스타', 0.20000000000000001), ('하이라이트', 0.20000000000000001), ('이제는', 0.13333333333333333), ('기자', 0.13333333333333333), ('우빈', 0.13333333333333333), ('하이라이트 손동운이', 0.13333333333333333)] ['윤두준', '손동운', '하이라이트', '김구라', '윤종신']\n",
      "\n",
      "['교수', '권역외상센터', '귀순', '병사'] [('교수는', 0.20447945297729908), ('그는', 0.17039954414774924), ('국회', 0.15335958973297431), ('권역외상센터', 0.13631963531819941), ('개선', 0.10223972648864955), ('예산이', 0.10223972648864954), ('같은', 0.10223972648864954)] ['이슈 · 중증외상센터 지원 확대 여론', '이국종', '피눈물', '외상센터']\n",
      "\n",
      "['긴급체포', '검찰', '긴급', '체포'] [('긴급체포', 0.44776673559449509), ('대해', 0.26534325072266379), ('검찰은', 0.19900743804199783), ('석방', 0.19900743804199783), ('것이다', 0.1326716253613319), ('관행에', 0.1326716253613319), ('검찰', 0.1326716253613319)] ['이슈 · 전병헌 뇌물수수 의혹', '긴급체포', '구속적부심', '석방']\n",
      "\n",
      "['배고픔', '가짜', '으로', '호르몬'] [('가짜', 0.52668516238258756), ('과자', 0.17556172079419585), ('배고픔은', 0.15605486292817408), ('나타나는', 0.15605486292817408), ('초콜릿', 0.11704114719613057), ('배가 고프고', 0.11704114719613057), ('특정', 0.11704114719613057)] ['떡볶이', '과자', '호르몬']\n",
      "\n",
      "['낚싯배', '해경', '충돌', '당시'] [('것으로', 0.3296342573721317), ('수색', 0.24722569302909878), ('있다', 0.2060214108575823), ('영흥도', 0.18541926977182405), ('거센 물살', 0.12361284651454939), ('실종자', 0.12361284651454939), ('물살', 0.12361284651454939)] ['이슈 · 영흥도 낚싯배 전복사고', '낚싯배', '영흥도', '실종자수색']\n",
      "\n",
      "['가족들', '이날', '실종자', '낚싯배'] [('실종자', 0.24722569302909878), ('이날', 0.2060214108575823), ('입고', 0.18541926977182405), ('인천', 0.16481712868606585), ('가족들은', 0.16481712868606585), ('구명조끼를', 0.12361284651454937), ('소식을', 0.12361284651454937)] ['이슈 · 영흥도 낚싯배 전복사고', '실종자', '낚싯배', '선착장']\n",
      "\n",
      "['나지완', '광주', '양미', '신부'] [('양미희', 0.27272727272727271), ('나지완', 0.27272727272727271), ('광주', 0.18181818181818182), ('나지완은', 0.18181818181818182), ('신부', 0.18181818181818182), ('신부 양미희', 0.18181818181818182), ('키스', 0.13636363636363635)] ['나지완']\n",
      "\n",
      "['사도세자', '영조', '부모', '신잡'] [('영조가', 0.1803046114936547), ('알쓸신잡2', 0.16227415034428924), ('사도세자가', 0.14424368919492378), ('했다', 0.14424368919492378), ('사도세자는', 0.14424368919492378), ('사도세자의', 0.10818276689619283), ('거다', 0.10818276689619283)] ['알쓸신잡2', '유시민', '유현준', '유희열', '누가']\n",
      "\n",
      "['일본', '스즈키', '으로', '매독'] [('중국인', 0.22694084234167011), ('매독', 0.17020563175625261), ('일본에', 0.11347042117083507), ('지난달', 0.11347042117083507), ('관계가', 0.11347042117083507), ('있다', 0.11347042117083507), ('스즈키는', 0.11347042117083507)] ['소녀상', '도쿄', '중국인']\n",
      "\n",
      "['미국', '울스', '한국', '출전'] [('미국', 0.52116171962547342), ('농구', 0.17372057320849116), ('아울스의', 0.15441828729643658), ('했다', 0.15441828729643658), ('표정을 지었다', 0.11581371547232744), ('경기장을', 0.11581371547232744), ('한국', 0.11581371547232744)] NaN\n",
      "\n",
      "['불상', '사진', '데라우치', '19'] [('청와대', 0.26633280908566315), ('불상', 0.21306624726853049), ('사진', 0.21306624726853049), ('데라우치', 0.21306624726853049), ('불상을', 0.14204416484568699), ('개안식', 0.14204416484568699), ('총독이', 0.14204416484568699)] ['경주시', '관저', '청와대']\n",
      "\n",
      "['인터뷰', '환자', '상태', '분들'] [('인터뷰', 0.4133313738410947), ('기자', 0.4133313738410947), ('얘기를', 0.16533254953643786), ('그건', 0.13777712461369823), ('상태로', 0.11022169969095857), ('현장', 0.082666274768218931), ('저희도', 0.082666274768218931)] ['시화병원', '전복사고', '낚싯배']\n",
      "\n",
      "['낚시', '일단', '상황', '사고'] [('스티로폼', 0.20119582530445315), ('스티로폼 잡고', 0.16766318775371097), ('잡고', 0.16766318775371097), ('아뇨', 0.13413055020296877), ('저희가', 0.13413055020296877), ('당시', 0.11177545850247397), ('사고', 0.11177545850247397)] ['이슈 · 영흥도 낚싯배 전복사고', '스티로폼', '영흥도', '해경']\n",
      "\n",
      "['이병', '해병', '해병대', '하버드대'] [('해병대', 0.48867777742522089), ('이병은', 0.24433888871261045), ('하버드대', 0.24433888871261045), ('한국', 0.12216944435630522), ('미국', 0.12216944435630522), ('해병대에', 0.12216944435630522), ('수료식을', 0.081446296237536811)] ['해병대', '하버드대', '유학생']\n",
      "\n",
      "['대통령', '보고', '현장', '청와대'] [('대통령은', 0.22213828559138191), ('오전', 0.22213828559138191), ('보고를', 0.19040424479261306), ('필요한', 0.15867020399384421), ('등을', 0.12693616319507536), ('해경', 0.095202122396306529), ('것으로', 0.095202122396306529)] ['이슈 · 영흥도 낚싯배 전복사고', '컨트롤타워', '해경', '낚싯배']\n",
      "\n",
      "['토트넘', '손흥민', '선발', '영국'] [('손흥민', 0.27436984238236151), ('선발', 0.27436984238236151), ('후스코어드닷컴', 0.12194215216993846), ('손흥민의', 0.12194215216993846), ('영국', 0.12194215216993846), ('25', 0.12194215216993846), ('매체', 0.12194215216993846)] ['손흥민']\n",
      "\n",
      "['송지효', '체중계', '몸무게', '라며'] [('몸무게를', 0.19088542889273333), ('입어서', 0.19088542889273333), ('체중계에', 0.19088542889273333), ('송지효', 0.1908854288927333), ('런닝맨', 0.1908854288927333), ('송지효는', 0.12725695259515554), ('안전', 0.12725695259515554)] ['송지효', '몸무게', '양세찬', '김종국', '하하']\n",
      "\n",
      "['텍사스', '추신수', '선수', '칼훈'] [('텍사스', 0.38694022235311298), ('연봉', 0.19347011117655649), ('추신수를', 0.17197343215693911), ('트레이드', 0.12898007411770435), ('추신수', 0.12898007411770435), ('지명', 0.12898007411770432), ('좌익수로', 0.12898007411770432)] ['추신수']\n",
      "\n",
      "['원내대표', '으로', '친박', '대표의'] [('원내대표는', 0.21952851997938069), ('김성태', 0.16464638998453551), ('친홍', 0.16464638998453551), ('문재인', 0.16464638998453551), ('원내대표', 0.16464638998453551), ('친박', 0.10976425998969035), ('정권과', 0.10976425998969035)] ['이슈 · 한국당 원내대표 김성태 선출', '문재인', '김성태', '한국당']\n",
      "\n",
      "['으로', '줄리아', '마지막', '조선'] [('마지막', 0.32391708783691253), ('이구', 0.1439631501497389), ('봉행위원회 총재로', 0.10797236261230417), ('살림을 살았던', 0.10797236261230417), ('조선', 0.10797236261230417), ('단독', 0.10797236261230417), ('마지막 세자빈', 0.10797236261230417)] ['하와이', '요양원', '건축가']\n",
      "\n",
      "['낚시', '사고', '선창1호', '인명피해'] [('이후', 0.20647416048350559), ('돌고래호', 0.20647416048350559), ('인명피해를', 0.13764944032233706), ('발생한', 0.13764944032233706), ('낚싯배', 0.13764944032233706), ('사고', 0.13764944032233706), ('이날', 0.13764944032233706)] ['이슈 · 영흥도 낚싯배 전복사고', '돌고래호', '낚싯배', '낚시어선']\n",
      "\n",
      "['오타니', '에인절스', 'mlb', '에인절'] [('오타니의', 0.27597366233044085), ('오타니는', 0.24837629609739675), ('mlb', 0.16558419739826449), ('관심을', 0.13798683116522042), ('오타니가', 0.13798683116522042), ('모두', 0.13798683116522042), ('에인절스는', 0.11038946493217633)] ['오타니 쇼헤이', 'LA 에인절스']\n",
      "\n",
      "['윤균', '박나래', '사진', '인스타그램'] [('사진', 0.20365326999063921), ('윤균상', 0.20365326999063921), ('사람', 0.20365326999063921), ('윤균상은', 0.20365326999063921), ('세상에서 제일', 0.13576884666042613), ('박나래는', 0.13576884666042613), ('나래바는', 0.13576884666042613)] ['박나래', '좋은사람', '인스타그램']\n",
      "\n",
      "['서해', '아침', '구름', '곳이'] [('3도', 0.21417646843905966), ('2도', 0.17134117475124774), ('또는', 0.1285058810634358), ('곳이', 0.1285058810634358), ('남부', 0.1285058810634358), ('곳이 있겠다', 0.1285058810634358), ('서울', 0.1285058810634358)] ['체감온도', '비', '경기남부']\n",
      "\n",
      "['장신', '영은', '이몽', '동상'] [('라고', 0.35999999999999999), ('장신영은', 0.20000000000000001), ('시어머니는', 0.12), ('장신영에게', 0.12), ('동상이몽2', 0.12), ('함께', 0.12), ('사람이', 0.080000000000000002)] ['강경준', '시어머니', '신승훈', '우효광', '동상이몽']\n",
      "\n",
      "['결혼', '김예령', '김수현', '윤석민'] [('osen', 0.19334729780913271), ('윤석민과', 0.19334729780913271), ('야구선수', 0.14501047335684952), ('윤석민이 사위예요', 0.14501047335684952), ('사진', 0.14501047335684952), ('김예령', 0.14501047335684952), ('인사를 올리게', 0.14501047335684952)] ['김수현', '결혼', '윤석민', '결혼식']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오전', '구조', '대통령', '현장'] [('지시했다', 0.19936305570722498), ('오전', 0.19936305570722498), ('낚싯배', 0.1794267501365025), ('대통령은', 0.15949044456577999), ('행안부', 0.11961783342433499), ('필요한', 0.11961783342433499), ('않도록', 0.11961783342433499)] ['이슈 · 영흥도 낚싯배 전복사고', '낚싯배', '전복사고', '세월호']\n",
      "\n",
      "['성폭력', '이씨', '등을', '범행'] [('성폭력', 0.17349447958987207), ('며느리를', 0.17349447958987207), ('징역', 0.17349447958987207), ('아들이', 0.11566298639324804), ('밝혔다', 0.11566298639324804), ('등을', 0.11566298639324804), ('이씨는', 0.11566298639324804)] ['며느리', '성폭력범죄', '낙태']\n",
      "\n",
      "['린드블럼', '롯데', '선수', '투수'] [('롯데', 0.18028869304837589), ('린드블럼은', 0.16025661604300079), ('린드블럼의', 0.16025661604300079), ('평균자책점', 0.12019246203225059), ('린드블럼이', 0.12019246203225059), ('것으로', 0.12019246203225059), ('받았다', 0.080128308021500394)] ['조쉬 린드블럼']\n",
      "\n",
      "['kt', '윤석민', '황재균', '3루'] [('윤석민은', 0.21863473472331302), ('있다', 0.17490778777865043), ('kt', 0.17490778777865043), ('선발', 0.17490778777865043), ('황재균', 0.13118084083398782), ('감독은', 0.13118084083398782), ('부담을', 0.13118084083398782)] ['황재균', '윤석민']\n",
      "\n",
      "['다니엘', '스케줄', '출연', '아침'] [('강다니엘은', 0.30485538042484617), ('스케줄', 0.18291322825490769), ('무리한', 0.12194215216993846), ('새벽', 0.12194215216993846), ('강다니엘 언젠간', 0.091456614127453845), ('문제', 0.091456614127453845), ('4시에 기상해서', 0.091456614127453845)] ['감기몸살', '눈길', '웹툰', '라면']\n",
      "\n",
      "['일본', '조추첨', '손흥민', '보다'] [('h조에', 0.26099787548594028), ('제일', 0.15659872529156416), ('손흥민은', 0.15659872529156416), ('현실', 0.15659872529156416), ('한국은', 0.10439915019437611), ('f조에', 0.10439915019437611), ('대한', 0.10439915019437611)] ['손흥민']\n",
      "\n",
      "['커피', '알코올', '체내', '마시'] [('된다', 0.25177250044296223), ('커피를', 0.25177250044296223), ('하는', 0.14387000025312127), ('알코올', 0.14387000025312127), ('술을', 0.10790250018984096), ('알코올의', 0.10790250018984096), ('스위프트', 0.10790250018984096)] ['커피', '카페인', '알코올']\n",
      "\n",
      "['월드컵', '스페인', '포트', '로페테기'] [('월드컵', 0.25512498387349231), ('스페인', 0.24352839369742449), ('로페테기', 0.23193180352135664), ('감독은', 0.16235226246494966), ('로페테기 감독은', 0.16235226246494966), ('팀이', 0.139159082112814), ('스페인은', 0.139159082112814)] NaN\n",
      "\n",
      "['fa', '4년', '나이', '번째'] [('번째', 0.44149720961321681), ('4년', 0.32703497008386428), ('fa', 0.228924479058705), ('총액', 0.13081398803354571), ('않다', 0.13081398803354571), ('번째 fa', 0.098110491025159285), ('나이가', 0.098110491025159285)] NaN\n",
      "\n",
      "['어서', '처음', '이지', '한국은'] [('어서와', 0.29753082220827881), ('처음이지', 0.26447184196291451), ('어서와 한국은', 0.26447184196291451), ('한국은 처음이지', 0.26447184196291451), ('한국은', 0.26447184196291451), ('있다', 0.16529490122682156), ('시청률', 0.16529490122682156)] ['공약', '어서와한국은처음이지', '어서와', '제주도', 'mbc에브리원']\n",
      "\n",
      "['현빈', '영화', '강소라', '일정'] [('강소라', 0.21374114996372895), ('영화', 0.1424940999758193), ('같은', 0.1424940999758193), ('헤어졌다', 0.1424940999758193), ('바쁜', 0.1424940999758193), ('현빈과', 0.1424940999758193), ('현빈과 헤어졌다', 0.1424940999758193)] ['현빈', '결별', '변혁의사랑', '창궐', '꾼']\n",
      "\n",
      "['한화', '코치', '타격', '전상렬'] [('전상렬', 0.31410216574189925), ('코치는', 0.2792019251039104), ('추승우', 0.26175180478491605), ('한화', 0.2094014438279328), ('퓨처스', 0.17450120318994403), ('코치', 0.15705108287094963), ('2군', 0.1396009625519552)] NaN\n",
      "\n",
      "['장신', '김장', '강경준', '결혼'] [('장신영은', 0.20519567041703082), ('동상이몽2', 0.20519567041703082), ('장신영', 0.20519567041703082), ('결혼', 0.17099639201419237), ('예비', 0.15389675281277312), ('강경준은', 0.13679711361135388), ('말했다', 0.13679711361135388)] ['강경준', '장신영', '결혼', '너는내운명', '서장훈']\n",
      "\n",
      "['의원', '상실', '의원이', '착잡'] [('의원직', 0.30644169241703884), ('의원은', 0.20429446161135922), ('국민의당', 0.18386501545022332), ('의원이', 0.1634355692890874), ('라고', 0.1634355692890874), ('의원의', 0.12257667696681554), ('통합', 0.12257667696681554)] ['최명길', '의원직상실', '국민의당']\n",
      "\n",
      "['권창훈', '프랑스', '리그', '디종'] [('권창훈', 0.27074557691828405), ('리그', 0.24066273503847474), ('권창훈은', 0.18049705127885604), ('이후', 0.15041420939904671), ('프랑스', 0.15041420939904671), ('신의', 0.13537278845914202), ('유럽', 0.13537278845914202)] ['권창훈']\n",
      "\n",
      "['할머니', '속으', '식사', '춘천시'] [('불길', 0.27643280257527802), ('이모씨', 0.24571804673358047), ('청년', 0.18428853505018536), ('춘천', 0.18428853505018536), ('할머니', 0.18428853505018536), ('속으로', 0.12285902336679023), ('불편한', 0.12285902336679023)] ['춘천', '화재', '사우나']\n",
      "\n",
      "['삼성', '아델', '외국인투수', '허친슨'] [('삼성', 0.5), ('외국인투수', 0.5), ('삼성 아델만', 0.33333333333333331), ('아델만', 0.33333333333333331), ('아델만 허친슨보다', 0.33333333333333331), ('영입하나', 0.33333333333333331), ('외국인투수 영입하나', 0.33333333333333331)] NaN\n",
      "\n",
      "['디스', '파레', '두산', '안타'] [('달러', 0.36447404353847218), ('있다', 0.18223702176923609), ('총액', 0.18223702176923609), ('80만', 0.18223702176923609), ('80만 달러', 0.18223702176923609), ('두산', 0.18223702176923609), ('파레디스', 0.12149134784615739)] ['지미 파레디스']\n",
      "\n",
      "['김정혁', '으로', '전력', '선수'] [('전력', 0.35416880166057318), ('김정혁은', 0.17708440083028659), ('현역', 0.17708440083028659), ('내년부터', 0.11805626722019105), ('osen', 0.11805626722019105), ('전력 분석', 0.11805626722019105), ('데뷔', 0.11805626722019105)] ['김정혁']\n",
      "\n",
      "['팻딘', 'kia', '시즌', '달러'] [('팻딘은', 0.31647510885169422), ('있다', 0.19779694303230888), ('매우', 0.17801724872907798), ('2018년', 0.11867816581938534), ('kia는', 0.11867816581938533), ('달러', 0.11867816581938533), ('있었다', 0.11867816581938533)] ['팻 딘']\n",
      "\n",
      "['시즌', '지난', '토트넘', '알리'] [('지난', 0.21312504617883277), ('시즌', 0.18648441540647867), ('시소코는', 0.13320315386177048), ('없다', 0.13320315386177048), ('못했다', 0.10656252308941638), ('없는', 0.10656252308941638), ('토트넘은', 0.10656252308941638)] NaN\n",
      "\n",
      "['불펜', '투수', 'fa', '트레버'] [('불펜', 0.35603449745815596), ('설리반은', 0.15823755442584711), ('됐다', 0.11867816581938533), ('마무리', 0.11867816581938533), ('있다', 0.11867816581938533), ('올해', 0.079118777212923555), ('불펜 보강이', 0.079118777212923555)] NaN\n",
      "\n",
      "['권창훈', '팬들', '인사', '디종'] [('디종', 0.34660991202509034), ('이미', 0.2695854871306258), ('권창훈은', 0.2053984663852387), ('있는', 0.2053984663852387), ('디종의', 0.15404884978892905), ('프랑스', 0.15404884978892905), ('직접', 0.15404884978892902)] ['권창훈']\n",
      "\n",
      "['홍예슬', '개그우먼', '으로', '아기'] [('홍예슬은', 0.18935832092934096), ('홍예슬', 0.18935832092934093), ('개그우먼', 0.18935832092934093), ('개콘', 0.18935832092934093), ('개그우먼으로', 0.12623888061956062), ('8일', 0.12623888061956062), ('서울의', 0.12623888061956062)] ['개그콘서트', '개그우먼', '유민상', '결혼', '임신']\n",
      "\n",
      "['박진', '발레', '김희철', '소현'] [('처음', 0.2010451732239466), ('인생술집', 0.2010451732239466), ('사귄', 0.13403011548263111), ('연예계', 0.13403011548263111), ('하는데', 0.13403011548263108), ('박진영은', 0.13403011548263108), ('발레', 0.13403011548263108)] ['박진영', '박소현', '김희철', '데뷔전', '주간아이돌']\n",
      "\n",
      "['상하이', '유나이티드', '전북', '20'] [('2018', 0.23302069121418523), ('유력', 0.2330206912141852), ('acl', 0.2330206912141852), ('상하이', 0.15534712747612348), ('챔피언', 0.15534712747612348), ('현대', 0.15534712747612348), ('조추첨', 0.11651034560709261)] NaN\n",
      "\n",
      "['조달환', '연예가중계', '2tv', '김주혁'] [('촬영을', 0.13102435641608368), ('황수연', 0.13102435641608368), ('배우', 0.13102435641608368), ('2tv', 0.13102435641608368), ('2tv 연예가중계', 0.13102435641608368), ('배우 조달환이', 0.13102435641608368), ('좋은', 0.13102435641608368)] ['김주혁', '결혼', '김생민', '창궐', '연예가중계']\n",
      "\n",
      "['컴백', '타선', '넥센', '박병호'] [('넥센', 0.38729833462074165), ('넥센 타선', 0.38729833462074165), ('박병호', 0.38729833462074165), ('박병호 컴백', 0.38729833462074165), ('배지헌의 브러시백', 0.38729833462074165), ('브러시백', 0.38729833462074165), ('얼마만큼', 0.38729833462074165)] ['넥센 히어로즈', '박병호']\n",
      "\n",
      "['장기용', '아저씨', '고백', '부부'] [('아저씨', 0.40209034644789321), ('나의 아저씨', 0.33507528870657771), ('나의', 0.22338352580438514), ('장기용은', 0.17870682064350812), ('고백부부', 0.17870682064350812), ('tvn', 0.17870682064350812), ('아이유', 0.13403011548263111)] ['장기용', '나의아저씨', '대세', '고백부부', '이선균']\n",
      "\n",
      "['구하라', '건물', '매각', '논현동'] [('논현동', 0.32245846375500836), ('구하라는', 0.17914359097500465), ('건물을', 0.14331487278000371), ('건물은', 0.14331487278000371), ('구하라가', 0.10748615458500278), ('38억원에', 0.10748615458500278), ('4억원대', 0.10748615458500278)] ['구하라', '논현동', '카라', '청담동', '홍수현']\n",
      "\n",
      "['제혁', '준호', '친구', '정경호'] [('감빵생활', 0.31008683647302115), ('준호는', 0.28941438070815306), ('제혁의', 0.20672455764868078), ('제혁을', 0.20672455764868078), ('준호는 제혁을', 0.12403473458920847), ('있는', 0.082689823059472312), ('슬기로운', 0.082689823059472312)] ['정경호', '교도소', '박해수', '신원호', '지호']\n",
      "\n",
      "['득녀', '이야', '기를', '가족'] [('비는', 0.34041126351250522), ('그는', 0.15129389489444675), ('대한', 0.15129389489444675), ('같다', 0.11347042117083507), ('많은', 0.11347042117083507), ('osen', 0.11347042117083507), ('올해', 0.11347042117083507)] ['가족이야기', '뭉쳐야뜬다', '김태희', '퍼포먼스', '미니앨범']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['온유', '지난', '팬들', '활동'] [('온유는', 0.20982172726556325), ('샤이니', 0.14987266233254518), ('온유가', 0.14987266233254518), ('팬들은', 0.14987266233254518), ('지난', 0.14987266233254518), ('대한', 0.11989812986603614), ('온유의', 0.11989812986603614)] ['자숙', '샤이니', '사과문', 'sm엔터테인먼트', '이글']\n",
      "\n",
      "['다저스', '트레이드', '류현진', '윈터미팅'] [('트레이드', 0.40840266683181736), ('류현진', 0.17502971435649314), ('다저스', 0.15558196831688278), ('거닉', 0.15558196831688278), ('11일', 0.1166864762376621), ('류현진은', 0.1166864762376621), ('윈터미팅', 0.1166864762376621)] ['류현진']\n",
      "\n",
      "['기사', '결별', '한혜진', '박나래'] [('결별', 0.25682856549090732), ('라고', 0.22829205821413984), ('한혜진은', 0.17121904366060489), ('나혼자산다', 0.17121904366060486), ('말해', 0.11414602910706992), ('반응을', 0.11414602910706992), ('쿨한 반응을', 0.11414602910706992)] ['한혜진', '결별', '차우찬', '전현무', '박나래']\n",
      "\n",
      "['램지', '고든', '냉장고', '요리'] [('고든', 0.69411722007412124), ('고든 램지가', 0.20823516602223635), ('요리', 0.13882344401482427), ('램지가', 0.13882344401482424), ('램지는', 0.13882344401482424), ('고든 램지는', 0.13882344401482424), ('꺾었다', 0.092548962676549509)] ['고든램지', '이연복', '오승환', '대결', '냉장고를부탁해']\n",
      "\n",
      "['다음', '한파', '주말', '초반'] [('것으로', 0.23186944788008415), ('최강', 0.20868250309207573), ('보입니다', 0.18549555830406733), ('다음', 0.18549555830406733), ('내릴 것으로', 0.18549555830406733), ('것으로 보입니다', 0.18549555830406733), ('내릴', 0.18549555830406733)] ['한파', '비', '영하권']\n",
      "\n",
      "['남편', '박진희', '공개', '희의'] [('박진희의', 0.34022878627424563), ('남편은', 0.17011439313712282), ('박진희의 남편은', 0.17011439313712282), ('여행말고 美행', 0.11340959542474854), ('남편이', 0.11340959542474854), ('여행말고', 0.11340959542474854), ('눈길을', 0.11340959542474854)] ['키', '러브스토리', '눈길', 'sbs플러스', '냉장고를부탁해']\n",
      "\n",
      "['lg', '실장', '단장', '진혁'] [('lg', 0.28295591748714638), ('실장', 0.2122169381153598), ('진혁', 0.19806914224100247), ('lg는', 0.16977355049228784), ('있는', 0.084886775246143922), ('외에', 0.084886775246143922), ('지난', 0.084886775246143922)] NaN\n",
      "\n",
      "['우효광', '자신', '공개', '리마인드'] [('사진', 0.46602732450090006), ('웨딩', 0.27961639470054001), ('리마인드', 0.18641092980036), ('리마인드 웨딩', 0.18641092980036), ('자신의', 0.18641092980036), ('선물한', 0.12427395320024001), ('선물한 리마인드', 0.12427395320024001)] ['추자현', '웨이보', '임신']\n",
      "\n",
      "['차장', '검찰', '수사', '구속'] [('있다', 0.18276466495398153), ('검찰은', 0.18276466495398153), ('대한', 0.18276466495398153), ('우병우', 0.13707349871548616), ('검찰', 0.13707349871548616), ('구속영장을', 0.13707349871548616), ('받고', 0.13707349871548616)] ['이슈 · 국정원·군 정치개입 의혹', '우병우', '구속영장', '최윤수']\n",
      "\n",
      "['이시언', '혼자', '공개', '산다'] [('혼자', 0.42347080394435133), ('산다', 0.2823138692962342), ('혼자 산다', 0.2823138692962342), ('비가', 0.17644616831014637), ('절친', 0.15880155147913175), ('공개된', 0.1411569346481171), ('10년', 0.10586770098608783)] ['비', '이시언', '나혼자', '한혜진', '예고편']\n",
      "\n",
      "['이재진', '이재', '진의', '전지'] [('이재진은', 0.21864326664404848), ('전지적', 0.18740851426632729), ('이재진의', 0.18740851426632729), ('있다', 0.18740851426632729), ('전지적 참견시점', 0.18740851426632726), ('대해', 0.12493900951088485), ('혼자', 0.12493900951088485)] ['나 혼자 산다', '이재진', '이영자', '전현무', '젝스키스']\n",
      "\n",
      "['개최', '러시아', '러시아월드컵', '일본'] [('일본', 0.36380343755449945), ('개최국 러시아', 0.36380343755449945), ('러시아', 0.36380343755449945), ('최고의 행운', 0.36380343755449945), ('러시아월드컵', 0.36380343755449945), ('러시아월드컵 조추첨', 0.36380343755449945), ('이웃', 0.36380343755449945)] NaN\n",
      "\n",
      "['석현준', '트루아', '으로', '시즌'] [('있다', 0.2260417893022193), ('그는', 0.19375010511618798), ('트루아', 0.19375010511618798), ('석현준은', 0.16145842093015664), ('석현준', 0.14531257883714099), ('시즌', 0.12916673674412532), ('좋다', 0.12916673674412532)] NaN\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-675-01f550b94e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0midIs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ObjectId__id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtextrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtis2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metri_outcome\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m#workrank = RunWordKRwordRank(tis2, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#lexrank = LexRank()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-674-95aada515b29>\u001b[0m in \u001b[0;36mKeyword\u001b[0;34m(etri_outcome, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mne1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrl1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    collection = 'newsDaum'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/daum2/'\n",
    "elif site.lower() == 'naver':\n",
    "    collection = 'newsNaver'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/naver2/'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, collection)\n",
    "\n",
    "keywordList = list()\n",
    "pressList = list()\n",
    "for data in useCollection.find({'site':site}):\n",
    "    pressList.append(data['press'])\n",
    "    tis = data['title']+'. '+data['mainText']\n",
    "    tis2 = '.\\n'.join(tis.split('. '))\n",
    "    if len(tis) != 0:\n",
    "        if data['keywords'] !='NaN':\n",
    "            keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "        else:\n",
    "            pass\n",
    "    idIs = data['_id']._ObjectId__id\n",
    "    textrank = TextRank(tis2)\n",
    "    keywords = Keyword(etri_outcome,data)\n",
    "    #workrank = RunWordKRwordRank(tis2, 2)\n",
    "    #lexrank = LexRank()\n",
    "    #lexrank.summarize(tis2)\n",
    "    #lr = lexrank.probe(k=1)\n",
    "    print (textrank.keywords(),keywords, data['keywords'])\n",
    "    #print (lr)\n",
    "    print ()\n",
    "    '''\n",
    "    if not os.path.isfile(etri_outcome+idIs.hex()+'.picked.txt'):\n",
    "        try:\n",
    "            etri = USE_ETRI_ANALYSIS('srl', tis2)\n",
    "        except:\n",
    "            print (etri[1])\n",
    "            break\n",
    "        else:\n",
    "            pickle.dump(etri[0], open(etri_outcome+idIs.hex()+'.picked.txt','wb'))\n",
    "    break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunWordKRwordRank(text, min_count):\n",
    "    #min_count = 2   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "    max_length = 10 # 단어의 최대 길이\n",
    "    wordrank_extractor = KRWordRank(min_count, max_length)\n",
    "    beta = 0.85   # PageRank의 decaying factor beta\n",
    "    max_iter = 100\n",
    "    verbose = False\n",
    "    textIs = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "    textIs = list(map(lambda x: normalize(x, english = True, number = True), textIs))\n",
    "    keywords, rank, graph = wordrank_extractor.extract(textIs, beta, max_iter, verbose)\n",
    "    return keywords, rank, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pickle.load(open(etri_outcome+data['_id']._ObjectId__id.hex()+'.picked.txt','rb'))\n",
    "etri = testdata['return_object']['sentence']\n",
    "morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "\n",
    "for i in etri:\n",
    "    x = Extract_Text_Info(i)\n",
    "    morp += x[0]\n",
    "    wsd += x[1]\n",
    "    word +=x[2]\n",
    "    ne += x[3]\n",
    "    for ii in i['SRL']:\n",
    "        srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "    dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "morp1 = list(map(lambda x: x[0], morp))\n",
    "wsd1 = list(map(lambda x: x[0], wsd))\n",
    "word1 = list(map(lambda x: x[0], word))\n",
    "ne1 = list(map(lambda x: x[0], ne))\n",
    "srl1 = list(map(lambda x: x[0], srl))\n",
    "dependency1 = list(map(lambda x: x[0], dependency))\n",
    "outlist = []\n",
    "for idx in range(len(etri)):\n",
    "    y = pd.DataFrame(etri[idx]['dependency'])\n",
    "    y2 = y[y['mod'].apply(lambda x: len(x)) ==y['mod'].apply(lambda x: len(x)).max()]\n",
    "    y3 = y2[y2['weight'] == y2['weight'].max()]\n",
    "    y4 = y[y['id'].isin(y3['mod'][y3['mod'].index[0]])]\n",
    "    y5 = y[y['weight'] ==y4['weight'].max()]\n",
    "    y6 = y2.text.values.tolist() + y5.text.values.tolist()\n",
    "    y7 = y[y.text.isin(y6)]    \n",
    "    out = ' '.join(y7.text.tolist())\n",
    "    outlist.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'김국진·노홍철·한혜진 \\'한명회\\', 오늘(5일) 9회만에 종영.\\n영상 바로보기 [마이데일리 = 이승길 기자] JTBC \\'내 이름을 불러줘-한명(名)회\\'(이하 \\'한명회\\')가 막을 내린다.\\n최근 진행된 \\'한명회\\' 9회 녹화는 별난 이름 특집으로 꾸며졌다.\\n이날은 특집 명에 맞게 녹화 전까지 철저히 출연자들이 서로 이름을 공유하지 못하도록 조치를 취했다.\\n출연자들은 서로의 이름을 추측하며 자신의 이름보다 특이 한 지 궁금해 했다.\\n이날 8명의 출연자는 모두 \"출석 부르는 시간, 자기소개 시간을 가장 싫어한다\"며 다른 이름이지만 같은 심정으로 공감 대화를 나눴다.\\n그중 유독 눈에 띄는 한 남성 출연자의 기상천외한 이름이 밝혀져, 나머지 출연진은 물론 MC들도 당황하며 이름을 재차 물어 웃음을 자아냈다.\\n하지만 후에 밝혀진 이름 탄생 배경에는 마냥 웃을 수만은 없는 인생 스토리가 담겨있어 눈길을 끌기도 했다.\\n한편, 지난 10월 10일부터 방송된 \\'한명회\\'는 9회로 막을 내린다.\\n오는 12일부터 \\'한명회\\' 방송 시간에는 편성 시간이 변경된 \\'패키지로 세계일주-뭉쳐야 뜬다\\'가 전파를 탄다.\\n\\'한명회\\' 마지막 회는 5일 오후 9시 30분에 방송된다.\\n[사진 = JTBC 제공]- ⓒ마이데일리(www.mydaily.co.kr).\\n무단전재&재배포 금지 -'"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sTest = data['title']+'. '+data['mainText']\n",
    "sTest = '.\\n'.join(sTest.split('. '))\n",
    "sTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"김국진·노홍철·한혜진 '한명회', 오늘(5일) 9회만에 종영.\",\n",
       " \"영상 바로보기 [마이데일리 = 이승길 기자] JTBC '내 이름을 불러줘-한명(名)회'(이하 '한명회')가 막을 내린다.\",\n",
       " \"최근 진행된 '한명회' 9회 녹화는 별난 이름 특집으로 꾸며졌다.\",\n",
       " '이날은 특집 명에 맞게 녹화 전까지 철저히 출연자들이 서로 이름을 공유하지 못하도록 조치를 취했다.',\n",
       " '출연자들은 서로의 이름을 추측하며 자신의 이름보다 특이 한 지 궁금해 했다.',\n",
       " '이날 8명의 출연자는 모두 \"출석 부르는 시간, 자기소개 시간을 가장 싫어한다\"며 다른 이름이지만 같은 심정으로 공감 대화를 나눴다.',\n",
       " '그중 유독 눈에 띄는 한 남성 출연자의 기상천외한 이름이 밝혀져, 나머지 출연진은 물론 MC들도 당황하며 이름을 재차 물어 웃음을 자아냈다.',\n",
       " '하지만 후에 밝혀진 이름 탄생 배경에는 마냥 웃을 수만은 없는 인생 스토리가 담겨있어 눈길을 끌기도 했다.',\n",
       " \"한편, 지난 10월 10일부터 방송된 '한명회'는 9회로 막을 내린다.\",\n",
       " \"오는 12일부터 '한명회' 방송 시간에는 편성 시간이 변경된 '패키지로 세계일주-뭉쳐야 뜬다'가 전파를 탄다.\",\n",
       " \"'한명회' 마지막 회는 5일 오후 9시 30분에 방송된다.\",\n",
       " '[사진 = JTBC 제공]- ⓒ마이데일리(www.mydaily.co.kr).',\n",
       " '무단전재&재배포 금지 -']"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sTest2 = sTest.split('\\n')\n",
    "sTest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['별난', '기도', '12', '변경', '서로']"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextRank(re.sub('[\\W]','',sTest)).keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 형태소 분석 : morp\n",
    "* 어휘의미 분석 : wsd (동음이의어)\n",
    "* 어휘의미 분석 : wsd_poly (다의어)\n",
    "* 개체명 인식 : ner\n",
    "* 의존구문 분석 : dependency\n",
    "* 의미역 인식 : srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "iter = 1\r",
      "iter = 2\r",
      "iter = 3\r",
      "iter = 4\r",
      "iter = 5\r",
      "iter = 6\r",
      "iter = 7\r",
      "iter = 8\r",
      "iter = 9\r",
      "iter = 10\r",
      "iter = 11\r",
      "iter = 12\r",
      "iter = 13\r",
      "iter = 14\r",
      "iter = 15\r",
      "iter = 16\r",
      "iter = 17\r",
      "iter = 18\r",
      "iter = 19\r",
      "iter = 20\r",
      "iter = 21\r",
      "iter = 22\r",
      "iter = 23\r",
      "iter = 24\r",
      "iter = 25\r",
      "iter = 26\r",
      "iter = 27\r",
      "iter = 28\r",
      "iter = 29\r",
      "iter = 30\r",
      "iter = 31\r",
      "iter = 32\r",
      "iter = 33\r",
      "iter = 34\r",
      "iter = 35\r",
      "iter = 36\r",
      "iter = 37\r",
      "iter = 38\r",
      "iter = 39\r",
      "iter = 40\r",
      "iter = 41\r",
      "iter = 42\r",
      "iter = 43\r",
      "iter = 44\r",
      "iter = 45\r",
      "iter = 46\r",
      "iter = 47\r",
      "iter = 48\r",
      "iter = 49\r",
      "iter = 50\r",
      "iter = 51\r",
      "iter = 52\r",
      "iter = 53\r",
      "iter = 54\r",
      "iter = 55\r",
      "iter = 56\r",
      "iter = 57\r",
      "iter = 58\r",
      "iter = 59\r",
      "iter = 60\r",
      "iter = 61\r",
      "iter = 62\r",
      "iter = 63\r",
      "iter = 64\r",
      "iter = 65\r",
      "iter = 66\r",
      "iter = 67\r",
      "iter = 68\r",
      "iter = 69\r",
      "iter = 70\r",
      "iter = 71\r",
      "iter = 72\r",
      "iter = 73\r",
      "iter = 74\r",
      "iter = 75\r",
      "iter = 76\r",
      "iter = 77\r",
      "iter = 78\r",
      "iter = 79\r",
      "iter = 80\r",
      "iter = 81\r",
      "iter = 82\r",
      "iter = 83\r",
      "iter = 84\r",
      "iter = 85\r",
      "iter = 86\r",
      "iter = 87\r",
      "iter = 88\r",
      "iter = 89\r",
      "iter = 90\r",
      "iter = 91\r",
      "iter = 92\r",
      "iter = 93\r",
      "iter = 94\r",
      "iter = 95\r",
      "iter = 96\r",
      "iter = 97\r",
      "iter = 98\r",
      "iter = 99\r",
      "iter = 100\r",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'9회': 0.8126837885386783,\n",
       " '방송': 1.3006717524666578,\n",
       " '시간': 1.3519985939266275,\n",
       " '이름': 1.9076963967795106,\n",
       " '출연자': 0.7325719116229548,\n",
       " '한명회': 2.370508665957211}"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workrank1 = RunWordKRwordRank(data['title']+'. '+data['mainText'], 3)\n",
    "workrank1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('한명회', 0.4845437118523489),\n",
       " ('이름을', 0.21535276082326621),\n",
       " ('막을 내린다', 0.16151457061744967),\n",
       " ('5일', 0.16151457061744967),\n",
       " ('이름', 0.1076763804116331),\n",
       " ('했다', 0.1076763804116331),\n",
       " ('막을', 0.1076763804116331)]"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword(etri_outcome,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('한명회', 0.71237772470496141),\n",
       " ('5일', 0.43204307153704319),\n",
       " ('막을 내린다', 0.43204307153704319),\n",
       " ('이름을', 0.40594303011854527),\n",
       " ('jtbc', 0.28802871435802879)]"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keywords(text, stopwords):\n",
    "    etri = Run_ETRI_Analysis(stopwords, text)\n",
    "    soy1 = RunLRNounExtractor(text)\n",
    "    soy2 = RunWordExtractor(text)\n",
    "    et0 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[0]))\n",
    "    et0 = list(map(lambda x: x[0], et0))\n",
    "    et1 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[1]))\n",
    "    et1 = list(map(lambda x: x[0], et1))\n",
    "    et2 = list(map(lambda x: x[0], etri[2]))\n",
    "    et3 = list(map(lambda x: x[0], etri[3]))\n",
    "    et4 = list(filter(lambda x: x[1] in ['ARG0','ARG1','ARG2','ARG3','ARG4'], etri[4]))\n",
    "    et4 = list(map(lambda x: x[0], et4))\n",
    "    et5 = list(filter(lambda x: x[1] in ['NP','NP_SBJ','NP_OBJ','NP_AJT','VNP'], etri[5]))\n",
    "    et5 = list(map(lambda x: x[0], et5))\n",
    "    mecabout = list(filter(lambda x: x[1] in ['NNG','NNB','NNP'], mecab.pos(text)))\n",
    "    ctout = list(filter(lambda x: x[1] == 'Noun' , ct.pos(text)))\n",
    "    otout = list(filter(lambda x: x[1] == 'Noun' , ot.pos(text)))\n",
    "    mcout = list(map(lambda x: x[0], mecabout))\n",
    "    ctout = list(map(lambda x: x[0], ctout))\n",
    "    otout = list(map(lambda x: x[0], otout))\n",
    "    out = list(soy1.keys())+list(soy2.keys())+ctout+otout+mcout+et3+et4+et0+et1+et2+et5\n",
    "    vect = TfidfVectorizer().fit(out)\n",
    "    y = [list(soy1.keys()), list(soy2.keys()),et3, et4, ctout, otout, mcout,et0, et1, et2, et5]\n",
    "    y = list(filter(lambda x: len(x) !=0, y))\n",
    "    outdict = dict()\n",
    "    for i in y:\n",
    "        count = vect.transform(i).toarray().sum(axis = 0)\n",
    "        idx = np.argsort(-count)\n",
    "        count = count[idx]\n",
    "        feature_name = np.array(vect.get_feature_names())[idx]\n",
    "        out = dict(zip(feature_name, count))\n",
    "        for ii in out:\n",
    "            if not ii in outdict:\n",
    "                outdict[ii] = out[ii]\n",
    "            else:\n",
    "                outdict[ii] +=out[ii]\n",
    "    x = sorted(outdict.items(), key = itemgetter(1), reverse=True)[:5]\n",
    "    output = list(map(lambda x: x[0], x))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keyword_from_news_bigdata(path):\n",
    "    from glob import glob\n",
    "    fileList = glob(path+'*.csv')\n",
    "    listIs =list()\n",
    "    for i in fileList:\n",
    "        df = pd.read_csv(i,engine='python')\n",
    "        f = list(filter(lambda x: x[1] in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], enumerate(df.columns.values)))\n",
    "        if not len(f) == 0:\n",
    "            listIs += list(df[df.columns[f[0][0]]].values)\n",
    "    outlist = []\n",
    "    for ix in list(filter(lambda x: type(x)!=float, listIs)):\n",
    "        outlist += ix.split(',')\n",
    "    outlist1 = list(map(lambda x: ''.join(x.split('_')), outlist))\n",
    "    outlist2 = list(map(lambda x: ' '.join(x.split('_')), outlist))\n",
    "    return outlist1+outlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.twitter = ct\n",
    "        #self.twitter = ot\n",
    "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "        ,\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]    \n",
    "    def text2sentences(self, text):\n",
    "        sentences = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.phrases(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = sknorm(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=5):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data in useCollection.find({'site':'daum'}):\n",
    "    if len(data['mainText'].strip()) !=0:\n",
    "        if data['keywords'] !='NaN':\n",
    "            ct.add_dictionary(list(map(lambda x: x.split(' · ')[-1], data['keywords'])), 'Noun')\n",
    "        y = Extract_Keywords2(data['mainText'],stopwords)\n",
    "        z = TextRank(data['mainText'])\n",
    "        p = RunLRNounExtractor(data['mainText'])\n",
    "        q = RunWordExtractor(data['mainText'])\n",
    "        #print (y)\n",
    "        print (data['keywords'])\n",
    "        print (z.keywords())\n",
    "        #print (p.keys())\n",
    "        #print (q.keys())\n",
    "        print ()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding = 'utf-8', mode='r') as f:\n",
    "        data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
