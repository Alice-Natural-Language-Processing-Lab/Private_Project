{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagFile = pd.read_excel('./data/Korean_POS_tags_comparison.xlsx',sheet_name='chart3',dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagDict = dict()\n",
    "for idx in tagFile.index:\n",
    "    if tagFile.loc[idx]['Hannanum'][:3] =='XSN':\n",
    "        tag = 'XSN'\n",
    "    else:\n",
    "        tag = tagFile.loc[idx]['Hannanum']\n",
    "    tagDict[tag]=tagFile.loc[idx]['Twitter_Korean_Text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TagChange(tag):\n",
    "    return tagDict[tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add From NIA Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niaDictFile = pd.read_excel('./data/NIADic.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niaDictFile = niaDictFile[niaDictFile['tag']!='xp']\n",
    "niaDictFile.loc[:,'tag'] = niaDictFile['tag'].map(lambda x: TagChange(x.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in niaDictFile.tag.unique():\n",
    "    outfile = ('./data/'+'twitter_tagger_' + idx +'_TagChange_from_NIADic.txt')\n",
    "    with open(outfile,'w') as f:\n",
    "        outdata = niaDictFile[niaDictFile['tag']==idx]['term']\n",
    "        outdata.to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언론사 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> stopwords를 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressList = pd.read_excel('./data/presslist.xlsx')\n",
    "with(open('./data/newspress.txt','w')) as f:\n",
    "    f.write('\\n'.join(set(pressList['언론사'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwordList = Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[776, 766754, 22658]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ctwitter()\n",
    "#ot = Twitter()\n",
    "#ct = Twitter()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "list(map(lambda x: len(xxxx[x]), xxxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from __future__ import print_function, unicode_literals\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.hangle import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chat_bot as cb\n",
    "import Database_Handler as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 from ETRI AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenAPI():\n",
    "    import pickle\n",
    "    openapi = pickle.load(open('./etri_open_api_access_key', 'rb'))\n",
    "    return openapi\n",
    "def USE_ETRI_ANALYSIS(analysisCode, text):    \n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    requestJson = {\n",
    "        'access_key' : accessKey,\n",
    "        'argument' : {\n",
    "            'text' : text,\n",
    "            'analysis_code' : analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "    return json.loads(str(response.data,'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_INFO(fWord, sWord):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/WordRel\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    firstWord = fWord\n",
    "    #firstSenseId = fSenseId\n",
    "    secondWord = sWord\n",
    "    #secondSenseId = sSenseId\n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "        'first_word': firstWord,\n",
    "        #'first_sense_id': firstSenseId,\n",
    "        'second_word': secondWord,\n",
    "        #'second_sense_id': secondSenseId\n",
    "        }}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "    body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def USE_ETRI_RELATION(word):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseWWN/Word\"\n",
    "    accessKey = OpenAPI().strip()\n",
    "    word = word\n",
    "    requestJson = {\n",
    "    \"access_key\": accessKey,\n",
    "    \"argument\": {\"word\": word}}\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\"POST\",openApiURL,headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "                            body=json.dumps(requestJson))\n",
    "    return json.loads(str(response.data, 'utf-8')), str(response.status)\n",
    "\n",
    "def Extract_Text_Info(idx):\n",
    "    morp = list(map(lambda x: (x['lemma'], x['type']),idx['morp']))\n",
    "    wsd = list(map(lambda x: (x['text'], x['type']),idx['WSD']))\n",
    "    word = list(map(lambda x: (x['text'], x['type']),idx['word']))\n",
    "    ne = list(map(lambda x: (x['text'], x['type']),idx['NE']))\n",
    "    return morp, wsd, word, ne\n",
    "\n",
    "def Run_ETRI_Analysis(stopwordsList, text):\n",
    "    etri = USE_ETRI_ANALYSIS('srl', text)['return_object']    \n",
    "    etri = etri['sentence']\n",
    "    morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "    for i in etri:\n",
    "        x = Extract_Text_Info(i)\n",
    "        morp += x[0]\n",
    "        wsd += x[1]\n",
    "        word +=x[2]\n",
    "        ne += x[3]\n",
    "        for ii in i['SRL']:\n",
    "            srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "        dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "    return morp, wsd, word, ne, srl, dependency, etri\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import re\n",
    " \n",
    "class RawSentence:\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawSentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawTagger:\n",
    "    def __init__(self, textIter, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class RawTaggerReader:\n",
    "    def __init__(self, filepath, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    " \n",
    "    def load(self, sentenceIter, wordFilter = None):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "        def insertNearPair(a, b):\n",
    "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            for i, word in enumerate(sent):\n",
    "                if wordFilter and not wordFilter(word): continue\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                self.nTotal += 1\n",
    "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if wordFilter and not wordFilter(sent[j]): continue\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.dictNear.get((a, b), 0)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getI(self, a):\n",
    "        import math\n",
    "        if a not in self.dictCount: return None\n",
    "        return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def extract(self, ratio = 0.1):\n",
    "        ranks = self.rank()\n",
    "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "        pairness = {}\n",
    "        startOf = {}\n",
    "        tuples = {}\n",
    "        for k in cand:\n",
    "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "            for l in cand:\n",
    "                if k == l: continue\n",
    "                pmi = self.getPMI(k, l)\n",
    "                if pmi: pairness[k, l] = pmi\n",
    " \n",
    "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "            print(k[0], l[0], pairness[k, l])\n",
    "            if k not in startOf: startOf[k] = (k, l)\n",
    " \n",
    "        for (k, l), v in pairness.items():\n",
    "            pmis = v\n",
    "            rs = ranks[k] * ranks[l]\n",
    "            path = (k, l)\n",
    "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "            last = l\n",
    "            while last in startOf and len(path) < 7:\n",
    "                if last in path: break\n",
    "                pmis += pairness[startOf[last]]\n",
    "                last = startOf[last][1]\n",
    "                rs *= ranks[last]\n",
    "                path += (last,)\n",
    "                tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    " \n",
    "        used = set()\n",
    "        both = {}\n",
    "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "            if used.intersection(set(k)): continue\n",
    "            both[k] = tuples[k]\n",
    "            for w in k: used.add(w)\n",
    " \n",
    "        #for k in cand:\n",
    "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "        return both\n",
    " \n",
    "    def summarize(self, ratio = 0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords =['이제','너무','중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\",'뉴스1','CBS노컷뉴스', \"동아일보\",\n",
    "             \"중앙일보\", \"조선일보\", '기자','습니다','어요','이렇게', \"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\",\n",
    "             'OSEN','라고','지난','당시','오전','오후',\"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\",'데일리',\n",
    "             'JTBC','YTN','MBC','KBS','SBS','TVN','TV조선','채널A','MBN','이날','갑자기','같다','는데',\n",
    "             '다른','에서','하는','하게','이후','통해','이날','정말','다시','스스로','같은','지난주','이번주','다음주',\n",
    "             '그런','이런','첫째','둘째','셋째','넷째','다섯째','여섯째','일곱째','여덟째','아홉째','열째','기성',\n",
    "             '이었','라는','저번','이번','면서','이렇게','저렇게','그렇게','정근','판청','아르테',\n",
    "            '앵커','아나운서','좌윤','주고', 'B씨','A씨','경우','서로','대로','지사','인근','스완','위해','대한',\n",
    "            '통한','기성','영제','간다','대해','수준','젊은','스탠','이씨','헤아','하라','패스트','좌안','이토','2TV',\n",
    "            '성은','산다','나요','선우','누나','억원','태수','그냥','사이','어마','위엄','남자','로서','우리','누리','관련',\n",
    "            '윤종','다친','승헌','골망','있는','까지','반종빈','녹이','현재','세이','카스티','스티','물론','라덴','상대로',\n",
    "            '여러','아미','아사다','이닝','유기','신서','티브이데일리','공동','윤이','치원','타고','박씨','루수','유력','타고',\n",
    "            '하고','처음','김씨','연재','사람','다이어','이다','인근','니다','아직','이씨','탈린','윤계','이날',\n",
    "            '이것','저것','그것','신잡','장영','고씨','박씨','사실','경정','손새','장의','미도','말했',\n",
    "            '빈살','실제','분들','건너','최근','조씨','장씨','올해','내년','작년','지난해','여서','A군','B군',\n",
    "            '고든','램지','진혁','심지어','D씨','최근','니다','협회','그리즈','미오','그것','메이','이재','윈터',\n",
    "            '임오','레이먼','사는','잡고','이야','치치','있다','1호','2호','3호','4호','5호','6호','7호','8호','9호',\n",
    "            '다양한','부분','심씨','트루시','트루','심씨','부분','전체','주씨','펀치','원투','A사','1차','2차','3차','4차',\n",
    "            '5차','6차','7차','8차','9차','10차','계속','MK','매경닷컴','MK스포츠','처음','모현','들이','있는',\n",
    "            '더욱','차장','부장','시점','000','달라','모든','구름','맑음','방향','같아','믹스','나인','윤균','모두',\n",
    "            '라며','헤아','제트','라카','소속','1분','신잡','울스','일단','번째','어서','이지','원장','교수','선수',\n",
    "            '안젤리나','브래드','드려','조원','둥지','빅리그','어떻','대답했','루프','트리플','빅토르','지금','때문','해성',\n",
    "             '빠졌','그리팅','5일','점프','그룹','시즌','크게','없는'] + stopwordList\n",
    "\n",
    "#stopwords = stopwordList + ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "#        ,\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.twitter = ct\n",
    "        #self.twitter = ot\n",
    "        self.stopwords = stopwords\n",
    "    def text2sentences(self, text):\n",
    "        sentences = re.sub('\\. ','.\\n', text).split('\\n')\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.phrases(str(sentence))\n",
    "                                                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
    "                                                  if noun not in self.stopwords and len(noun) > 1]))\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.morphs(str(sentence))\n",
    "                                                   if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(ngram_range=(1,5),sublinear_tf=True, lowercase=False)\n",
    "        self.cnt_vec = CountVectorizer(ngram_range=(1,5), lowercase=False)\n",
    "        self.graph_sentence = []\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        #tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = sknorm(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "class TextRankX(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=5):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://v.media.daum.net/v/20171205151743503\n",
      "['노홍철', '한명회', '한혜진', '종영', '편성시간']\n",
      "['이름', '한명회', '출연자', '방송', '시간']\n",
      "['이날은 특집 명에 맞게 녹화 전까지 철저히 출연자들이 서로 이름을 공유하지 못하도록 조치를 취했다', '그중 유독 눈에 띄는 한 남성 출연자의 기상천외한 이름이 밝혀져, 나머지 출연진은 물론 MC들도 당황하며 이름을 재차 물어 웃음을 자아냈다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209104014068\n",
      "['류중일']\n",
      "['캠프', '신인', '류중일 감독', '류중일', '감독']\n",
      "['류중일 감독은 8일 \"캠프 명단은 지난달 29일 전체 미팅을 하면서 통보를 했다', '류중일 감독은 \"현재로서는 신인은 없다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205061623329\n",
      "['불륜', '비', '첫방', '최다니엘', '최대철']\n",
      "['보스', '비서', '상무', '좌윤이', '이는']\n",
      "['좌윤이는 연인 경준(성훈 분)과 달달한 첫날밤을 보내려다가 보스 봉상무와 봉상무 아내(정영주 분), 일명 머저리와 미저리 부부의 전화폭격으로 분위기를 망쳤다', '좌윤이는 바람난 봉상무의 알리바이를 만들기 위해 화장품이 묻은 셔츠를 수습하느라 분주했고, 갑자기 내린 비에 발길을 서두르다가 와이퍼가 고장난 남치원(최다니엘 분)의 차에 치이며 무릎을 다쳤다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205140114189\n",
      "['기억의밤', '시나리오', '리얼리티', '강하늘', '김무열']\n",
      "['기억', '감독', '스릴러', '장항준 감독', '장항준']\n",
      "['‘충분히 현실적인, 리얼리티 넘치는’ 추적 스릴러의 탄생을 위해 장항준 감독은 국과수는 물론, 정신분석학자 등 전문가들의 도움도 받았다', '장항준 감독은 ‘기억의 밤’의 포인트에 대해 “스릴과 서스펜스로 달려가는 막다른 종착역에 우리들의 슬픔이 있다고 말하고 싶다”며 “스릴러는 제가 진짜 하고 싶은 이야기를 하기 위한 도구다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205065011570\n",
      "['강호동', '정상훈', '조세호', '어청도', '달타냥']\n",
      "['정상훈', '강호동', '낚시', '어청도', '총사']\n",
      "[\"'낚시 스팟'에 도착한 강호동과 정상훈은 거대한 농어를 낚기 시작한 후 이후 참돔까지 낚아가며 함박웃음을 지었다\", \"한편, 이날 어청도의 한 강아지가 계속해 정상훈을 쫓아다니며 '신스틸러'로 거듭났다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171204154759778\n",
      "['윤계상', '마동석', '강경대응', 'sns']\n",
      "['범죄', '불법', '유포', '영화', '도시']\n",
      "['키위미디어그룹은 유포자들의 불법 행위에 대해 \"단순히 저작권의 침해 행위일 뿐만 아니라 청소년관람불가 등급의 영상물을 아무런 제약없이 배포하는 것으로 처벌 받아 마땅하며, 인터넷의 파급력을 고려할 때 피고소인의 범죄행위는 해당 영화의 매출에도 막대한 손해를 입혔다\"고 고소 이유를 밝혔다', '[CBS노컷뉴스 유원정 기자] ywj2014@cbs.co.kr']\n",
      "\n",
      "http://v.media.daum.net/v/20171211083610397\n",
      "['출산', '혼인신고', '해시태그', '인스타그램', '결혼식']\n",
      "['박지헌', '당신', '출산', '아내', '사랑']\n",
      "['박지헌은 12월 11일 자신의 인스타그램에 \"15살에 당신을 만나 이제 곧 41살 우리', '이어 \"당신이 이룬 그 사랑을 이제는 정신없이 흠뻑 누리며 살아가는 나와 우리 부모님들을 보면 정말 이건 너무 신기해']\n",
      "\n",
      "http://v.media.daum.net/v/20171205112749763\n",
      "['타임스스퀘어', '아이돌', '뉴욕', '워너원', '연말시상식']\n",
      "['전광판', '광고', '다니엘', '타임스퀘어', '강다니엘']\n",
      "['5일 가요계에 따르면 강다니엘의 팬들은 그의 22번째 생일(12월 10일)을 맞아 뉴욕 타임스퀘어 전광판 8개에 강다니엘의 영상과 소개 문구가 담긴 영상을 게재했다', '해당 전광판에는 삼성, LG의 신형 스마트폰 광고 등이 실리기도 하지만, 국내 아이돌 그룹의 단일 멤버의 광고 영상이 올라온 것은 이례적이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206100855949\n",
      "['당뇨병', '탄수화물', '식이섬유']\n",
      "['먼저', '식이섬유', '탄수화물', '혈당', '식사']\n",
      "['먹는 순서를 식이섬유→단백질→탄수화물 순으로 바꾸면 혈당이 급격하게 상승하는 것을 막고, 빠른 포만감을 준다', \"그 결과, 당뇨병 환자나 건강한 사람 모두 혈당치 상승폭이 '쌀밥을 먼저' 먹은 경우보다 '생선을 먼저' 먹은 쪽이 약 30%, '육류를 먼저' 먹은 쪽은 약 40% 낮았다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171205185612637\n",
      "['이지현', '눈길', '혼인신고', '인스타그램', '결혼식']\n",
      "['김가연', '임요환', '임요환 부부', '부부', '사진']\n",
      "['김가연과 임요환 부부 사이 두 딸과의 행복한 미소가 눈길을 끈다', '뿐만 아니라 김가연은 엄마와도 함께한 사진을 공개하며 \"엄마는 늘 우리곁에 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205212852229\n",
      "['이창명', '본격연예한밤', '무죄판결', '음주운전', '교통사고']\n",
      "['이창명', '상고', '검찰', '한밤', '항소심']\n",
      "['정신이 피폐해진다\"고 말문을 연 이창명은 \"항소심 무죄판결 받았을 때 가족들 전부 다 안고서 너무 기뻐하고 \\'정말 감사합니다\\' 그렇게 이야기를 했었는데 이렇게 상고가 될 줄은 전혀 몰랐다', '이어 이창명은 \"난 괜찮은데 가족들이 너무 아파한다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205081251688\n",
      "['이서진', '정유미', '윤여정', '알쓸신잡2', '윤식당']\n",
      "['식당', '윤식당2', '박서준', '촬영', '리포트']\n",
      "['[TV리포트=박귀임 기자] ‘윤식당2’ 멤버들과 제작진이 스페인 촬영을 마치고 귀국한다', 'TV리포트 취재 결과, tvN 새 예능프로그램 ‘윤식당 시즌2’(이하 윤식당2) 팀은 모든 촬영을 마치고 5일 인천국제공항을 통해 입국할 예정이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205102736063\n",
      "['임신', '애프터스쿨', '인스타그램']\n",
      "['가희', '소식', '응원', '격려', '임신 소식']\n",
      "['그룹 애프터스쿨 출신 가수 가희는 5일 자신의 인스타그램을 통해 짧은 글과 함께 초음파 사진을 공개하며 둘째 임신 소식을 전했다', \" 가희는 '축하해주신 모든 분들 감사드린다'면서 '둘째 아기는 건강히 잘 크고 있다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171205135100807\n",
      "['샤이니', 'sm', '사과문', 'sm엔터테인먼트', '아이돌그룹']\n",
      "['온유', '샤이니', '사과', 'SM', '논란']\n",
      "[\"이후 공개된 시즌 그리팅 티저 영상에 샤이니 멤버 5명 전원이 등장하자 팬들은 '굿즈(상품)에 온유가 포함되는지 여부를 알려달라' 요청했지만 소속사는 묵묵부답으로 일관했다\", '\"자숙하겠다\"던 SM의 말대로 온유는 4개월간 침묵하다가 시즌 그리팅 논란이 불거지고 나서야 \\'사과 없는 사과문\\'을 올렸다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205063301444\n",
      "['김지민', '돌직구', '데이트', '양세찬', '허지웅']\n",
      "['어머니', '김지민', '김지', '웃음', '엄마']\n",
      "['어머니는 또한번 “이제 꺾일 나이”라고 충고했고 김지민은 “엄마는 그렇게 말하면 안되지”라며 서운함을 표했다', '이에 어머니는 “엄마니까 할 수 있는 말”이라며 걱정했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207114905236\n",
      "['결혼', '코빅', '키', '결혼식', 'tvn']\n",
      "['정인영', '결혼', '코미디', '하차', '정인영의']\n",
      "['‘코미디 빅리그’ 측은 7일 오전 동아닷컴에 “정인영의 결혼에 관해서는 잘 알지 못한다', '한편 정인영은 ‘코미디 빅리그’에서 활약 중이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171211110006022\n",
      "['조쉬 린드블럼', '삼성 라이온즈']\n",
      "['삼성', '린드블럼', '영입', '투수', '롯데']\n",
      "['삼성 “린드블럼은 검증된 선수, 강민호와의 배터리 호흡도 기대” 언급된 세 팀 가운데 삼성은 린드블럼 영입에 관심이 없는 것으로 알려졌던 팀이다', '삼성, 린드블럼? 아니면 더 강력한 새로운 투수? 삼성은 스카우트 팀을 재편하고, 외부 자문 인력을 충원하는 등 공격적인 움직임을 통해 현역 메이저리거인 팀 아델만을 데려왔다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205114210320\n",
      "['판빙빙', '연습생', '프로듀스101', '우주소녀', '키']\n",
      "['판빙빙', '판청청', '엔터테인먼트', '연습생', '상황']\n",
      "['[서울경제] 중국의 인기 여배우 판빙빙의 남동생 판청청이 위에화엔터테인먼트에서 가수 연습생 생활 중이다', '위에화 엔터테인먼트 관계자는 5일 오전 서경스타에 “판청청이 소속 연습생 신분으로 한국에서 지내고 있다”며 “지난해 겨울에 한국에 와서 1년째 실력을 쌓고 있는 상황”이라고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205152610853\n",
      "['자이언티', '발라드곡', '음원차트', '피처링', '엠넷']\n",
      "['자이언티', '이문세', '겨울', '신곡', '애정']\n",
      "[\"앞서 이문세는 지난 11월 29일 딩고에서 공개된 자이언티와 함께 한 'DF Interview'를 통해 이번 신곡 및 자이언티에 대한 아낌 없는 애정을 표한 바 있다\", '이문세는 당시 인터뷰를 통해 \"이번 신곡 작업 시 자이언티에게 \\'가난한 연인의 행복한 겨울 노래\\'를 부탁했는데, 정말 멋있는 충격과 같은 곡이 나왔다\"며 자이언티의 음악적 감각과 실력에 대해 극찬을 아끼지 않았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205151620454\n",
      "['박세리', '닐슨코리아', '김병만', '리얼', '승부욕']\n",
      "['정글', '박세리', '제작', '모습', '진의']\n",
      "[\"박세리는 '정글' 제작진의 삼고초려 끝에 섭외된 특급 게스트다\", '완고하게 거절하던 박세리는 제작진의 진심 어린 모습에 이내 출연을 결심, 정글로 향했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205202640034\n",
      "['선우재덕', '임신', '출산', '배낭여행', '폴란드']\n",
      "['김가연', '탈출', '자신', '영상', 'tvN']\n",
      "[\"5일 방송된 tvN '둥지탈출2'에서는 첫 폴란드 배낭여행을 떠난 10대들의 모습이 그려졌다\", '이날 김가연은 본격적인 녹화 전부터 자신의 딸 사진과 영상을 보여주며 행복해 했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203191203904\n",
      "['해양플랜트', '침몰', '수주']\n",
      "['플랜트', '한국', '해양 플랜트', '해양', '싱가포르']\n",
      "['중국 역시 낮은 인건비를 무기로 고부가가치 선박 분야에서도 가격 공세를 벌이고 있다', '시황이 품질은 좋지만 가격이 비싼 한국 선박과 해양플랜트를 소화할 여건이 아니라는 설명이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207053039271\n",
      "['채태인', '최준석']\n",
      "['시장', '보상', 'FA', '카드', '계획']\n",
      "['보상선수 없는 FA선수의 등장은 크게 환영할 일이고 각 팀의 전력보강에 매우 좋은 카드가 될 수 있다고 보지만 올 시즌 우리 팀의 방향은 자연스러운 세대교체다”고 설명했다', '시장에 남은 보상선수가 필요 없는 FA는 분명 매력적인 카드지만 팀의 전략적인 계획과 교차하지 못했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205113623103\n",
      "['핀란드', '어서와', '딘딘', '김준현', '한국어']\n",
      "['한국', '핀란드', '친구', '페트', '빌레']\n",
      "['이탈리아나 독일만 해도 한식도 많고 관광객도 많고 러시아도 K팝으로 알려졌는데 핀란드 친구들은 표정만 봐도 아시아, 한국 문화를 전혀 몰랐다는 걸 알 수 있어요', '페트리: 한국 사람들이 핀란드에 대해 모르는데 정보를 줄 수 있어서 좋아요']\n",
      "\n",
      "http://v.media.daum.net/v/20171204142809127\n",
      "['이슈 · 2018 예산안 본회의 통과', '복지예산', '예산', 'soc']\n",
      "['예산', '복지', 'SOC', '확대', '사업']\n",
      "['국회예산정책처는 내년도 예산안은 확대 예산이 아니라 ‘미세한 축소 예산’이라고 한다', '앞서 말한 주택 관련 융자사업 확대액과 새로 생긴 일자리안정자금지원 예산액 3조원을 빼면 2018년도 복지예산 증가율은 7.8%에 그친다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205030604072\n",
      "['lng', '석탄', '스모그']\n",
      "['가스', '지역', '중국', '공급', '난방']\n",
      "['그 여파로 낮시간대 충전소에 차량들이 한꺼번에 몰리면서 한 번 가스를 충전하는 데 1~2시간씩 기다려야 하는 상황이다', '로이터통신은 \"중국 정부의 미숙한 탈(脫)석탄 정책이 가스 대란의 원인\"이라고 보도했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209053042886\n",
      "['정근우', '한화 이글스']\n",
      "['FA', '정근우', '롯데', 'SK', '한화']\n",
      "['정근우의 2017년 연봉은 7억원이다', '보상금과 보상선수는 한화를 제외한 나머지 팀들에게 고려사항이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205204805430\n",
      "['아주대병원', '혈압', '이국종교수']\n",
      "['귀순', '귀순병', 'CNN', '북한', '공개']\n",
      "['<앵커> 판문점 공동경비구역에서 총탄을 맞고도 달려 귀순했던 북한 병사의 응급수술 장면이 CNN을 통해 공개됐습니다', 'CNN이 공개한 이 영상은 아주대병원 의료진이 촬영한 것으로 긴박했던 순간을 생생하게 전하고 있습니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205220901793\n",
      "['이슈 · 북한 핵·미사일 도발', '문재인', '정관용', '트럼프']\n",
      "['미국', '북한', '대통령', '임상훈', '정관용']\n",
      "['결국 미국 정치권의 한반도 이 위기 조성, 이게 미국 군수산업을 활성화시키는 장점 외에 한반도 정세 안정에 무슨 기여를 했느냐, 이런 목소리가 나올 수 있다는 건데 이게 이제 중국에서 대표적인 그런 볼멘 소리가 나오는 것이 바로 이런 입장입니다', \"◇ 정관용> 뭐라고 평가를 했어요? ◆ 임상훈> 올해를 장식할 글로벌 리띵커로 문재인 대통령을 '한국의 품위 있는 민주주의 리더십을 되살리려 하는 인물이다', 이렇게 평가를 했습니다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171205165659882\n",
      "['이슈 · 국정원·군 정치개입 의혹', '원세훈', '국가정보원', '원전']\n",
      "['국정원', '원장 원장', '원장', '연구원', '스탠퍼드대']\n",
      "['검찰은 원 전 원장과 스탠퍼드대가 기부금 200만달러에서 나오는 연 10만달러 안팎의 이자를 원 전 원장이 미국에서 객원연구원으로 지내는 동안 필요한 생활비 명목으로 지급하도록 정한 것으로 의심하고 있다', '앞서 검찰은 원 전 원장이 2011년 말부터 2012년 초까지 국정원 해외공작금 200만달러를 스탠퍼드대 아시아태평양연구소 기금으로 보내도록 했다는 자료를 국정원에서 넘겨받아 조사해왔다']\n",
      "\n",
      "http://v.media.daum.net/v/20171210100127933\n",
      "NaN\n",
      "['석현준', '트루아', '리그', '득점', '기록']\n",
      "['석현준은 득점뿐만 아니라 수비 상황에서도 헌신적으로 했다', '이번 시즌 트루아에 합류해 리그 12라운드에 마수걸이 득점을 성공한 석현준은 14라운드까지 3경기 연속 골을 기록했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205104400777\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '갯벌', '영흥도', '선창1호']\n",
      "['수색', '시신', '수색 작업', '작업', '영흥도']\n",
      "['(인천=연합뉴스) 손현규 기자 = 인천 영흥도 인근 해상에서 발생한 낚싯배 추돌 사고의 실종자 2명을 찾는 수색 작업이 진행 중인 가운데 사고 지점 인근 해상에서 선창1호 선장의 시신이 발견됐다', '이날 시신이 발견된 용담해수욕장 남단은 사고 지점으로부터 남서방 2.7∼3㎞ 떨어진 곳이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203214940159\n",
      "['생존자', '낚싯배', '유조선']\n",
      "['바다', '낚싯배', '소리', '배가', '바로 바다']\n",
      "['‘갑자기 바다에 빠졌을 때 어떻게 대처했냐’는 질문에 A씨는 “배가 부딪히고 파도가 한 번 왔다가 두 번째 파도가 왔을 때 제가 휩쓸렸다”며 “물 속에 한참 들어갔다가 나왔는데 그땐 ‘살려달라’는 소리가 들리고 불빛도 보였다”고 대답했다', '또, ‘겨울철이고 수온이 낮아서 힘드셨을텐데 어떻게 버티셨느냐’고 묻자 A씨는 한숨을 푹 내쉬었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171211191715508\n",
      "[\"이슈 · '어금니 아빠' 살인사건\", '이영학', '여중생', '서울구치소']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이영학', '변호사', '사건', '변호', '재판']\n",
      "['(웃음) 아버지께선 ‘걔(이영학)는 똑똑해서 너도 속여먹을 수 있을 것 같더라’며 반대하셨고, 친구들도 “사건이 없어 배가 고픈 것도 아니면서 왜 그러냐’고 걱정하더라.” 김 변호사는 막상 사건을 맡은 뒤 걱정이 이만저만이 아니었다고 고백했다', '평생을 인권 변호사로 살다 간 고 조영래 변호사를 존경한다는 그는 “범죄사실만 놓고 보면 이영학이 ‘짐승만도 못한 사람’이라는 평가를 받을 수 있지만 그도 대한민국 국민”이라고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205140624398\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '문재인', '박근혜', '세월호']\n",
      "['대통령', '구조', '사고', '세월호', '상황']\n",
      "['이 답변서에서 박 전 대통령은 오전 10시께 국가안보실로부터 세월호 사고 첫 보고를 받고 오전 10시15분 김장수 당시 국가안보실장에게 전화해 “단 한명의 인명피해도 발생하지 않도록 (구조에 만전을 기)할 것, 여객선 내 객실 등을 철저히 확인하여 누락 인원이 없도록 할 것”을 지시했다고 주장했습니다', '결국 박 전 대통령의 세월호 관련 첫 발언은 오전 10시께 첫 보고 뒤 7시간이 지난 오후 5시15분께 중대본 방문 당시 발언으로 봐야 합니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201140855351\n",
      "NaN\n",
      "['리즈', '리그', 'LG', '관심', '영입']\n",
      "[' 메이저리그 구단의 한 스카우트는 “지금 리즈 구위와 전체적인 기량을 놓고 봤을 때, KBO리그 시절보다 훨씬 좋아졌다는 느낌을 받는다\"며 \"LG 트윈스를 비롯해 적지 않은 구단이 리즈 영입에 큰 관심을 두는 것으로 안다”고 전했다', '실제 수도권 프로구단 스카우트는 “도미니칸 윈터리그에서 가장 뜨거운 선수를 꼽자면 단연 리즈다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205090100814\n",
      "['비만', '체중', '다이어트']\n",
      "['시간', '음식', '하루', '섭취', '다이어트']\n",
      "[' 미국 캘리포니아에 있는 솔크연구소에서 스마트폰 앱을 이용해서 하루 일과 속 음식 섭취 패턴을 조사해보니, 사람은 잠자는 시간을 제외하고는 무언가를 계속 먹고 있다는 사실이 밝혀졌다', '하루 세끼만 먹는 사람은 거의 없었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203132521495\n",
      "['김동현', '인순이', '투자사기', '무죄판결', '이승현']\n",
      "['사기', '사기 혐의', '혐의', '부동산', '가수']\n",
      "['검찰은 김씨가 이미 빚이 많은 상태에서 ‘돌려막기’식으로 1억원을 속여 빼었다고 보고, 지난 여름 김씨를 사기 혐의로 불구속 기소했다', '  또 그룹 젝스키스의 강성훈(37)씨는 사기 혐의로 수년간 곤욕을 치르다 일부 혐의에 대해 유죄(징역1년6개월, 집행유예 2년) 판결을 받았고, 가수 송대관은 부인의 부동산 사업 때문에 사기 혐의로 피소돼 활동까지 중단했지만 무죄 판결을 받았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203140452283\n",
      "['김연아', '유영']\n",
      "['최다빈', '프리스케이팅', '김하늘', '유영', '대회']\n",
      "['두 번째 점프인 트리플 플립은 실수 없이 했지만 더블 악셀 + 트리플 토루프 콤비네이션에서 빙판에 넘어졌다', '최다빈은 플라잉 카멜 스핀에 이은 트리플 러츠 + 더블 토루프 + 더블 루프 점프에서는 첫 점프 착지 후 빙판에 손을 짚었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203055100367\n",
      "NaN\n",
      "['명단', '보류', '계약', '보류선수', '자유 계약']\n",
      "['보류선수 명단 제외가 곧 선수 경력이 차단되는 것은 아니다', 'B팀의 관계자도 “올해는 (자유계약선수 영입에 대한 팀 방침이)확실히 예년과 다른 분위기다 ”라고 전했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171202203939525\n",
      "['구속적부심', '대법원장', '인천지법']\n",
      "['판사', '부장판사', '구속적부심', '구속', '법관']\n",
      "['[기자] 인천지법 김동진 부장판사는 오늘(2일) 자신의 페이스북에 올린 글에서 김관진 전 국방부 장관과 임관빈 전 국방부 정책실장 등에 대한 구속적부심 결과를 납득할 수 없다고 적었습니다', \"김 부장판사는 지난 2014년 원세훈 전 국정원장의 1심 판결 직후 법원 내부망에 '법치주의는 죽었다' 제목의 글을 올리고 사슴을 가리켜 말이라고 주장한다는 뜻의 지록위마 판결이라고 공개 비판한 바 있습니다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171210071520136\n",
      "['서지수', '노명희', '유인영', '신혜선', '나영희']\n",
      "['인물', '황금', '궁금증', '추측', '서지수']\n",
      "['KBS 2TV 주말드라마 ‘황금빛 내 인생’에서 민부장(서경화 분)은 과거엔 해성그룹 직원이었지만 현재는 집안일을 돌보는 비서다', '네티즌들은 최서현(이다인 분)이 민부장의 친딸이 아니냐는 추측부터 민부장이 해성그룹에 복수하기 위한 인물이 아니냐, 그리고 서지수의 납치에 민부장이 관련돼 있는 것 아니냐는 것 등 다양한 추측이 나오고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205155319908\n",
      "['존스', '조선중앙통신', '배추']\n",
      "['북한', '평양', '사진', '모습', '존스']\n",
      "['존스 기자가 찍은 사진들을 보면 평양만큼은 아니지만 큰 도시로 분류되는 함흥에서조차 조선중앙통신이 보도했던 것과 같은 번쩍이는 고층 건물과 포장도로는 찾아보기 어렵다', '페인트칠이 벗겨진 건물 사이 도로 위로 두꺼운 겨울 외투를 입은 주민들이 자전거를 밀고 가는 모습, 배추가 실린 수레를 밀고 있는 여성들의 모습, 해안가를 따라 나무배들이 늘어서 있는 모습 등은 마치 수십년 전 우리의 농촌 풍경을 보는 듯 하다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203224815502\n",
      "NaN\n",
      "['도르트문트', '박주호', '계약', '독일', '해지']\n",
      "['<키커>가 전한 대로 박주호는 2013년 FC 바젤에서 마인츠로 이적했다', '박주호는 도르트문트에서 주전 경쟁에 나섰지만 밀렸다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206071200227\n",
      "['빅토르 안']\n",
      "['러시아', '올림픽', '출전', '한국', '안은']\n",
      "['러시아에 IOC 결정에 반발해 올림픽 출전 자체를 보이콧할 가능성을 배제할 수 없기 때문이다', '당시 빅토르 안은 \"(선수) 소개할 때 많은 분들이 응원해주셔서 많은 힘을 얻게 됐다\"며 한국 팬들에 대한 고마움을 드러내면서 \"만약 평창올림픽에서 시상대 맨 위에 올라 러시아 국가를 들으면 마음이 이상할 것 같다\"고 털어놓은 바 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171212073638106\n",
      "['조정석', '오현종', '조항준', '살인누명', '닐슨코리아']\n",
      "['동탁', '동탁 수창', '수창', '지안', '수사']\n",
      "['게다가 여고생 납치 사건에 별 관심이 없던 동탁(수창)가 형사 공권력을 통해 알아내고 싶은 진실은 따로 있었다', '여고생 납치사건의 중심에 있는 박실장(민성욱 분)에게 받아낸 돈을 수녀에게 기부한 게 아니라 소매치기 지인에게 맡긴 것이었고 지안과 여고생 납치법을 잡기로 했던 약속마저 저버린 채 떠날 조짐을 보였다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203125355093\n",
      "['포체티노']\n",
      "['포체', '포체 티노', '티노', '토트넘', '경기']\n",
      "['경기 후 인터뷰에서 포체티노 감독은 “지금의 고난이 미래에 좋은 결과를 낳을 것”이라 이야기했다고 <런던 이브닝 스탠다드>는 전했다', '포체티노 감독은 어려운 시기를 지나고 있지만 “리그는 장기간의 프로젝트”라면서 힘든 순간을 통해 더 배울 수 있고, 미래를 좋게 만들 수 있다며 긍정적인 전망을 이야기했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203112104835\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '낚싯배', '영흥도', '급유선']\n",
      "['인터뷰', '사고', '구조', '시간', '해역']\n",
      "['[앵커] 대부분이 구명조끼를 입었는데도 사망자가 발생한 게 충돌 당시에 충격이 가장 큰 원인일까요? [인터뷰] 충돌 당시 충격도 있지만 지금 해수 온도가 겨울철이기 때문에 10도 미만일 때는 2시간 정도에 구조가 돼야 되고 4시간이 지나면 생존 가능성이 희박하다 이렇게 교범에 나와 있습니다', '조류가 10시 25분까지 빠져나가는 물이기 때문에 외해 쪽으로, 그러니까 사고 지점에서 외해 쪽으로 수색 범위를 넓히고 두 번째는 전복된 낚싯배 안에 실종자가 있는지 이것을 찾는 두 가지가 병행이 돼야 되는데 지금 해역에서 이루어지고 있는 것은 육지 쪽에서 바다 쪽으로 멀리 물에 떠밀려갔을 가능성에 대비해서, 즉 모든 낚시 가신 분들이 구명조끼를 입었다고 하니 다 바다에 떠 있을 것으로 보이고 그다음에 떠 있는 것을 우선 찾고 그다음 낚싯배 안에 혹시 갇혀져 있을지 모르기 때문에 낚싯배 안에 갇혀져 있나 이것을 수색을 해야 될 것으로 보여집니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205165305779\n",
      "['리튬', '전기차', '배터리']\n",
      "['리튬', '중국', '전기차', '확보', '업체']\n",
      "['중국 최대 리튬 생산업체 톈치리튬은 세계 최대 규모의 리튬광산인 호주 그린부시의 지분 51%를 확보했다', '심지어 중국 업체들은 아프리카에까지 손을 뻗쳐 리튬광산 개발에 참여하거나 리튬 생산업체의 지분을 공격적으로 사들이고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206140301737\n",
      "['대중교통', '빌라', '월세']\n",
      "['부부', '남편', '주말', '주말 부부', '시간']\n",
      "['우리는 그 거리를 감내하며 피폐해지는 대신 주말 부부라는 선택을 했다', '주말 부부라도 결국 돈과 시간을 길에 버리는 일이 반복되다 보니 남편은 2년 만에 완전히 지쳐버렸고, 삶의 질도 현저히 낮아졌다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203122805860\n",
      "NaN\n",
      "['두산', '항상', '최대성', '각오', '모습']\n",
      "['▲ 그동안 안해 본 운동이 없을 정도로 준비는 열심히 했다', '나는 항상 즐겁게 던지고 있는데, 다른 사람들이 볼 때는 마운드에서 불안하고 쫓겨보인다는 소리를 많이 하더라']\n",
      "\n",
      "http://v.media.daum.net/v/20171208102101715\n",
      "['갑질', '공유', '폭언']\n",
      "['직장', '가면', '피해자', '119', '회사']\n",
      "['가면무도회는 직장갑질 피해자들이 가면을 쓴 채 서로의 피해경험을 공유하는 자리다', '갑질 피해자들은 다른 피해자와 직장갑질119 스태프들과 삼삼오오 모여 갑질 피해는 물론 자신만의 갑질 대처 방법을 공유했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203064439717\n",
      "['데이비드 허프']\n",
      "['LG', '허프', '일본', '야쿠르트', '계약']\n",
      "['허프는 올해 LG와 연봉 총액 140만 달러와 인센티브 30만 달러의 별도 계약을 맺었다', '일본 구단들은 외국인 선수들에게 첫 시즌은 거액 계약을 하지 않는 편이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203150150600\n",
      "NaN\n",
      "['선발', '한화', '선발투수', '우완', '투수']\n",
      "['적어도 토종 선발 가운데 1~2명은 10승을 찍어야 순위싸움 한복판에 설 수 있다는 계산을 하고 있기도 하다', '한화는 우선 외국인투수 다음을 이어줄 확실한 3선발이 필요하다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203155612682\n",
      "NaN\n",
      "['부산', 'FA', '승격', 'FA 우승', '울산']\n",
      "['[인터풋볼=울산] 유지선 기자= 부산 아이파크의 이승엽 감독대행이 승격과 FA컵 우승 두 마리의 토끼를 놓쳤지만 후회는 없다고 밝혔다', '부산은 올 시즌 클래식 승격과 FA컵 우승 두 마리의 토끼를 쫓았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203064721733\n",
      "['박지성', '린가드', '데 헤아']\n",
      "['기록', '가드', 'ESPN', '아스널', '10']\n",
      "['멀티골을 기록한 제세 린가드, 아스널 공격을 이끈 알렉상드르 라카제트, 메수트 외질의 활약도 눈부셨지만 그 누구도 다비드 데 헤아의 스포트라이트를 가져갈 수 없었다', '이어 ESPN은 \"데 헤아만큼 잘하는 골키퍼는 전세계에 2, 3명 정도 찾을 수 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203070006824\n",
      "['신혜선', '유인영', '박시후', '장소라', '결혼']\n",
      "['도경', '장소라', '최도경', '인생', '유인영']\n",
      "['서지안은 \"잘 지내고 있어요\"라며 말했고, 최도경은 \"지금보다 더\"라며 작별 인사했다', '또 최도경은 장소라를 집에 데려다줬고, 장소라는 \"약혼을 언제 하면 좋을까요\"라며 물었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206210603313\n",
      "['방송통신위원회', '콜센터', 'lg유플러스']\n",
      "['해지', '해지 방어', '방어', '전화', '콜센터']\n",
      "['[홍순성/故 홍수연 양 아버지 (지난 3월 15일 국회) : 저는 아무런 의심도 없이 이런 일반 콜센터인 줄 알고 보냈고, 그런데 5개월 뒤에 제 앞에 놓인 건 (딸의) 싸늘한 주검이었어요.] 방통위가 통신사들의 해지 방어 실태를 조사했더니 해지 접수를 받고 고객에게 무려 73통의 전화를 걸어 철회를 요구한 사례도 있었습니다', '[고삼석/방송통신위원회 상임위원 : 통신사 콜센터의 과도한 해지 방어 활동이 이용자의 해지권을 침해한다는 그러한 법 위반에 대한 제재입니다.] 방통위는 위반 정도가 가장 심한 LG유플러스에 8억 원, SK브로드밴드에 1억 400만 원의 과징금을 부과했습니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203051106166\n",
      "['고다이라 나오']\n",
      "['이상화', '000', '월드컵', '10', '기록']\n",
      "['앞서 열린 여자 1,000ｍ 디비전B 경기에서는 박승희(스포츠토토)가 1분15초05로 3위를, 김현영(성남시청)이 1분 15초 66으로 8위를 차지했다', '남자 디비전B에선 정재웅(동북고)이 1분 8초 41로 7위, 장원훈(의정부시청)이 1분 8초 64로 12위에 올랐다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203065425780\n",
      "['하원미', '추신수', '이불', '결혼생활', '야구선수']\n",
      "['하원', '추신수', '생활', '미국', '내조']\n",
      "['추신수는 한국을 떠난 지 17년, 하원미는 추신수를 따라 미국으로 온지 15년째였다', '추신수가 이렇게 성공할 수 있었던 데엔 하원미의 공이 크다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207174039126\n",
      "['이창명', '음악캠프', '출연료', '짜장면', '바로']\n",
      "['드림팀', 'PD', '만원', '스타', '출발']\n",
      "['저는 할 게 없고 먹을 게 없어서 편집실에 간 건데 PD들은 그걸 열심히 사는 이창명이라고 생각하더라고요', \"'출발 드림팀'이 잠깐 없어졌을 때가 있었는데, 결국은 컨트롤 하기 힘든 이창명이 된 거죠\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171207141925573\n",
      "['정해인', '교도소', '신원호', '박해수', '지호']\n",
      "['대위', '생활', '유대위', '방송', '감빵']\n",
      "['7일 방송되는 tvN 수목드라마 ‘슬기로운 감빵생활’ (연출 신원호, 극본기획 이우정, 극본 정보훈) 6화에 유대위 역을 맡은 배우 정해인이 새롭게 합류해 극에 새 바람을 몰고 온다', '제작진은 “유대위는 온갖 흉악범들이 모인 교도소에서도 기죽는 법이 없고, 주변의 눈치 따위는 전혀 보지 않는 성격의 캐릭터”라고 소개하며 “항상 인상을 쓰고 교도소 사람들에게 까칠하게 대하는 유대위의 등장에, 2상6방의 분위기가 살얼음판이 된다”고 귀띔했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203143053923\n",
      "['신혜선', '연기인생', '성훈', '비밀의숲', '눈길']\n",
      "['신혜선', '황금', '인생', '으로', '황금 인생']\n",
      "['3일 한국기업평판연구소가 발표한 ‘2017년 12월 드라마배우 브랜드평판’ 조사 결과, KBS2 주말드라마 ‘황금빛 내 인생’ 신혜선이 1위에 이름을 올렸다', '뒤이어 찾아온 KBS2 주말드라마 ‘황금빛 내 인생’으로 연기 인생에 황금빛을 비추고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203141246475\n",
      "['장신영', '너는내운명', '동상이몽', '양파', '시어머니']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['김장', '장신영', '강경준', '시어머니', '예비 시어머니']\n",
      "['지난주, 평소 함께 취미로 요리를 배우러 다니던 예비 시어머니에게 먼저 김장을 제안한 장신영은 가족들과 나눠먹기 위해 김치 ‘40포기’ 만들기에 도전하는 모습으로 관심을 모았다', '김장 경험이 있는 예비 시어머니의 주도 하에 찹쌀 풀 쑤기, 무채 썰기, 양파 갈기 등 각자 역할을 나눠 맡게 된 세 사람은 파이팅을 다지며 재료 준비를 시작했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205102425927\n",
      "['강하늘', '김무열', '기억의밤', 'tv드라마', '살인사건']\n",
      "['감독', '장항준 감독', '장항준', '드라마', '자신']\n",
      "['\"드라마가 왜 명문대 출신 PD들을 뽑는지 알겠더라\"고 말문을 연 장항준 감독은 \"잠을 안 자고 공부해 본 사람들이다', '방송에 쫓기게 되면 그 피드백을 보면서 작가도 대본을 쓰고 감독도 연출을 하게 되더라\"고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207215944021\n",
      "['이승엽']\n",
      "['이승엽', '야구선수', '이승엽 야구선수', '야구', '천직']\n",
      "['[이승엽/전 야구선수 : 다 연관이 있는 것 같은데 굳이 꼽으시라면 저는 3번을 꼽고 싶습니다.] [앵커] 천직의식', '[이승엽/전 야구선수 : 천직의식과.] [앵커] 하나 더 해야 합니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203003803154\n",
      "['캠핑', '살짝미쳐도좋아', '눈길', '소녀시대', '반려견']\n",
      "['효연', '캠핑', '공개', '데뷔', '소녀시대']\n",
      "[\"3일 방송된 SBS '살짝 미쳐도 좋아'에서는 소녀시대 효연이 캠핑에 미쳐있는 ‘캠핑 미스타’로 출연했다\", '효연은 “혼자 캠핑 장비를 들고 다닌 건 5~6년 됐다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204165700272\n",
      "['안희정', '문재인', '경선']\n",
      "['대통령', '발언', '지사', '지지자', '지사의']\n",
      "['안 지사가 8개월여 만에 문 대통령 지지자들을 비판하는 발언을 하자, 일부 문 대통령 지지자들은 안 지사의 페이스북에 글을 남기며 문 대통령 지지자들의 입장을 대변하기도 했다', '문 대통령 지지자들은 절실하다\"고 답답함을 토로했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206052405721\n",
      "['김기태']\n",
      "['감독', '우승', 'KIA', '시스템', '으로']\n",
      "['김 감독은 “12월이 되면 처음 KIA에 부임했을 때가 생각난다', '김 감독은 “강을 건너야 하는데 다리가 없으면 보통 사람들은 돌아갈 생각을 한다']\n",
      "\n",
      "http://v.media.daum.net/v/20171208091801282\n",
      "['취재파일', '물고기', '김정은']\n",
      "['시신', '물고기', '북한', '백골', '바다']\n",
      "['● 북한 당국, \"바다를 한시도 비우지 말라\"  이들은 왜 먼바다까지 물고기를 잡으러 나가는가? 북한 당국이 정책적으로 물고기잡이를 다그치기 때문이다', '북한 어민들이 아무 바다에나 나가 물고기를 잡아올 수 없다는 것이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207000559804\n",
      "['윤두준', '손동운', '하이라이트', '김구라', '윤종신']\n",
      "['손동운', '하이라이트', '회사', '이름', '멤버']\n",
      "['영상 바로보기 [스포츠투데이 우빈 기자] 그룹 하이라이트 손동운이 회사를 차린 후 달라진 점을 밝혔다', '이날 손동운은 하이라이트 멤버들끼리 회사를 차렸다고 말하며 \"회사에는 하이라이트뿐인데 후일을 위해 인디밴드를 발굴할 생각이다\"고 설명했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207102502166\n",
      "['이슈 · 중증외상센터 지원 확대 여론', '이국종', '피눈물', '외상센터']\n",
      "['예산', '교수', '외상', '센터', '권역']\n",
      "['하지만 이 교수는 이날 의원들 앞에서 국내 권역외상센터를 근본적으로 개선하려면 일회성 예산 증액에 그칠 것이 아니라, 권역외상센터 체계가 왜 필요한지를 이해해야 한다고 호소했다', '그는 \"의원들이 좋은 뜻에서, (예산을 편성하지만) 밑으로 투영이 안 된다\"며 \"외상센터는 만들었는데 환자가 없으니 (병원장들이 우리에게) 일반환자를 진료하게 한다\"며 권역외상센터의 힘든 \\'현실\\'과 \\'실상\\'을 털어놨다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201112628489\n",
      "['이슈 · 전병헌 뇌물수수 의혹', '긴급체포', '구속적부심', '석방']\n",
      "['체포', '긴급 체포', '긴급', '검찰', '긴급체포']\n",
      "['조씨의 경우 10월13일 스스로 조사를 받으러 검찰에 출석했다가 14일 새벽 긴급체포 됐다', '결국 스스로 두 차례에 걸쳐 조사를 받으러 출석했고, 대부분 범행을 인정한 사람에 대해 긴급체포는 부당하다는 것이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204131302560\n",
      "['떡볶이', '과자', '호르몬']\n",
      "['가짜', '배고픔', '스트레스', '가짜 배고픔', '호르몬']\n",
      "['가짜 배고픔은 혈당이 저하되고 신체 에너지원으로 사용될 영양분이 부족해 나타나는 진짜 배고픔과 달리 스트레스가 주된 원인으로, 식욕과 관련된 호르몬의 불균형으로 생긴다', '가짜 배고픔의 증상은 ▲떡볶이나 과자 등 자극적인 맛을 내는 특정 음식이 당김 ▲식사를 한지 3시간 이내에 나타나는 허기짐 ▲배고픔이 점진적으로 나타나지 않고 갑자기 나타남 ▲스트레스 받는 상황에서 심해짐 등이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203132005424\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '낚싯배', '영흥도', '실종자수색']\n",
      "['낚싯배', '해경', '사고', '수색', '수색 작업']\n",
      "['낚싯배에는 선원 2명, 낚시객 20명 등 총 22명이 탑승했다', ' 해경 관계자는 \"낚싯배 출항 당시 날씨나 시간 등은 적법했던 것으로 알고 있다\"면서 \"구리밑 협수로를 오가기 위해 지나가다 충돌한 것으로 보인다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203134411852\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '실종자', '낚싯배', '선착장']\n",
      "['구명조끼', '낚시', '실종자', '소식', '구조']\n",
      "['그는 이날 오전에도 늘 하던 대로 인천 부평구 집에서부터 구명조끼를 입고 낚시에 나섰다', '망연자실하게 앉아있던 이씨의 아내는 \"최근에는 바빠서 낚시를 통 못하다가 오랜만에 나섰는데 이런 사고가 날 줄은 몰랐다\"며 \"구명조끼를 입었으니 그래도 조금은 안전하지 않겠느냐\"고 구조에 대한 희망을 내비쳤다']\n",
      "\n",
      "http://v.media.daum.net/v/20171202124204823\n",
      "['나지완']\n",
      "['나지완', '양미', '광주', '신부', '포토']\n",
      "['나지완은 12월 2일 낮 12시 광주 홀리데이인호텔 컨벤션홀에서 신부 양미희(24) 씨와 화촉을 밝혔다', '나지완은 2년 전 지인의 소개로 광주지역 방송국의 기상캐스터였던 양 씨를 만났다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209060045007\n",
      "['알쓸신잡2', '유시민', '유현준', '유희열', '누가']\n",
      "['사도세자', '영조', '교육', '부모', '영조 사도세자']\n",
      "['잡학박사들은 사도세자가 미치게 된 이유, 영조가 그를 뒤주에 가둬 죽인 이유 등을 짚으며 부모의 교육의 중요성을 돌아봤다', '뒤늦게 이를 알게 된 영조는 사도세자에게 자결하라고 했지만, 사도세자는 죽지 않았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203134413853\n",
      "['소녀상', '도쿄', '중국인']\n",
      "['스즈키', '일본', '발언', '매독', '극우']\n",
      "['스즈키는 도쿄신문의 취재에 “매독에 대해선 내 나름대로 생각해 일본에 오는 중국인 증가율과 인과 관계가 있다고 생각해왔다', '<거리로 나온 넷우익>을 쓴 저널리스트 야스다 고이치(安田浩一)는 “외국인도 많이 살고 있는 지역에 분열과 차별을 가져오는 스즈키의 발언은 허용될 수 없다”면서 “구 의원이나 의회 사무국도 책임과 자각을 갖고 차별은 허용하지 않는다는 성명을 내는 등 적극적으로 대응해야 한다”고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204060008499\n",
      "NaN\n",
      "['미국', '농구', '한국', '필리핀', '워싱턴']\n",
      "['아울스의 빠른 플레이에 미국 선수들은 당황했다', '최종 결과는 15-10 아울스의 승리였다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203175611950\n",
      "['경주시', '관저', '청와대']\n",
      "['사진', '불상', '데라우치', '총독', '안식']\n",
      "['정인성 영남대 문화인류학과 교수는 최근 일본 도쿄대 박물관 소장유물을 조사하는 과정에서 청와대불상의 옛 개안식 사진 2점을 발견했다며 3일 사진들을 <한겨레>에 공개했다', ' 정 교수는 “불상은 1912년 11월 총독의 경주 순시 당시 환심을 사려는 현지의 일본인 유지에 의해 몰래 반출됐고, 개안식 뒤에도 이 사실이 오래 묻혀있다가 21년이 지난 1934년 3월 <매일신보>에 총독부박물관이 불상 소재를 찾았다는 기사가 보도되면서 세간에 알려지게 됐다”면서 “사진들은 데라우치가 개안식을 통해 불법반출된 불상을 사유화하고, 조선 유적지에서 암약하던 일본인들의 문화재 밀반출 행위에 사실상 면죄부를 주었음을 의미한다”고 분석했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203132400466\n",
      "['시화병원', '전복사고', '낚싯배']\n",
      "['인터뷰', '상태', '환자', '사망', '얘기']\n",
      "['저도 현장 쪽 상황은 정확히는 모르는데 현장 쪽에 한 분이 나가셔서 사망선고를 하셨다고 얘기를 들었습니다', '지금 저체온증 겪고 계시는 환자 두 분은 곧 퇴원하실 수도 있는 건가요? [인터뷰]']\n",
      "\n",
      "http://v.media.daum.net/v/20171203160120759\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '스티로폼', '영흥도', '해경']\n",
      "['사고', '상황', '정도', '구조', '스티로폼']\n",
      "['경향신문은 3일 오후 2시 40분쯤 인천 길병원에 9층에서 생존자 서모씨(37)와 만나 사고 당시의 상황에 대해 들었다', '-아무래도 옆 배에서 구조돼서 빨리 구조됐는데, 다른 분들은 사실 해경에서 출동하는 데 시간이 걸렸다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203091829855\n",
      "['해병대', '하버드대', '유학생']\n",
      "['이병', '해병대', '해병', '하버드대', '입대']\n",
      "['3일 해병대사령부에 따르면 미국 하버드대 컴퓨터공학과에 재학 중인 홍찬의(21) 이병은 지난달 30일 경북 포항 해병대 교육훈련단에서 신병 수료식을 마치고 해병이 됐다', '해병대 빨간 명찰을 가슴에 단 홍 이병은 \"꿈을 향한 첫 번째 도전 목표였던 하버드대 입학에 이어 두 번째 도전을 해병대에서 시작한다\"며 \"내게 해병대의 가치는 하버드보다 크다\"고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203150915767\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '컨트롤타워', '해경', '낚싯배']\n",
      "['대통령', '구조', '해경', '현장', '보고']\n",
      "['문 대통령은 보고를 받을 때마다 필요한 조치를 지시했고 오전 9시25분 위기관리센터에 도착했다', '문 대통령은 위기관리센터에서 해경과 행정안전부, 세종 상황실 등을 화상으로 연결해 상세한 보고를 받고 오전 9시31분 6가지 지시를 내렸다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209060655078\n",
      "['손흥민']\n",
      "['손흥민', '경기', '선발', '벨레', '스코어']\n",
      "['[스포탈코리아] 김진엽 기자= 손흥민(25, 토트넘 홋스퍼)이 스토크 시티전서 선발 출전할 전망이다', '포메이션은 4-2-3-1이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203180739150\n",
      "['송지효', '몸무게', '양세찬', '김종국', '하하']\n",
      "['송지효', '체중계', '몸무게', '방송', '런닝맨']\n",
      "['체중계에 올라가기 전 송지효는 VJ에게 \"이거 찍을 거야?\"라며 불안한 모습을 보였다', '이어 송지효는 조심스레 체중계에 올라섰고, 몸무게를 확인한 스태프가 \"Oh My God\"이라고 장난을 치자 \"옷 입어서 그래']\n",
      "\n",
      "http://v.media.daum.net/v/20171201113240738\n",
      "['추신수']\n",
      "['텍사스', '추신수', '칼훈', '트레이드', '연봉']\n",
      "['만약 텍사스가 수비 전문 중견수를 영입하면, 드실즈가 좌익수로 이동하고 칼훈은 외야 백업과 지명 타자로 출전하면 된다\"고 했다', '신문은 \"추신수를 트레이드하고 드실즈를 좌익수로, 칼훈을 지명 타자로 쓰는 게 이상적\"이라면서 \"구단 역시 추신수를 내보내고 싶을 거라고 믿는다']\n",
      "\n",
      "http://v.media.daum.net/v/20171212191127395\n",
      "['이슈 · 한국당 원내대표 김성태 선출', '문재인', '김성태', '한국당']\n",
      "['원내대표', '야당', '의원', '투쟁', '홍준표']\n",
      "[' 복당파인 김 원내대표는 이날 정견발표를 통해 \"한국당의 당면과제는 첫째도 둘째도 문재인 정권과 맞서 싸우는 것\"이라며 \"잘 싸울 줄 아는 사람, 투쟁 전문가가 저 김성태\"라면서 \\'선명 투쟁야당\\'을 천명했다', ' 이날 원대대표 경선은 비홍(비홍준표) 표심의 결집으로 결선투표까지 갈 것이라는 예상도 있었으나 김 원내대표는 1차 투표에서 정확히 과반을 획득해 승부를 마무리 지었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206023108402\n",
      "['하와이', '요양원', '건축가']\n",
      "['마지막', '줄리아', '조선', '이구', '영친왕']\n",
      "['이구 선생에게 직접 전해주고 싶었을 조선왕가의 유물과 한국 근대사 관련 사진 450여 점을 덕수궁박물관에 기증했는데 이때 모습을 담은 다큐멘터리가 ‘줄리아의 마지막 편지’라는 제목으로 방송되기도 했다', '대한제국 황실 제3대 수장인 영친왕 이은과 일본인 부인 이방자의 아들이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203134943978\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '돌고래호', '낚싯배', '낚시어선']\n",
      "['선창', '사고', '피해', '인명', '인명 피해']\n",
      "[' 3일 인천해경에 따르면 이날 오전 6시 9분께 인천시 옹진군 진두항 남서방 1마일 해상에서 낚시 어선 선창1호(9.77t)가 급유선 명진15호(336t급)과 충돌한 뒤 전복됐다', '이 사고로 선창1호에 타고 있던 22명 중 13명이 숨지고 2명이 실종됐다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209043737597\n",
      "['오타니 쇼헤이', 'LA 에인절스']\n",
      "['오타', '에인절스', 'MLB', '관심', '오타니']\n",
      "['각 구단별로 할당된 올해 계약금 한도를 봤을 때 오타니는 당장 MLB에 간다면 최대 약 350만 달러 정도만 받을 수 있었다', '에이전시 측은 오타니가 에인절스를 선택한 배경에 대해 오타니가 모든 팀들의 관심과 호의에 감사함을 표했다면서도 “에인절스와 가장 강한 유대감을 느꼈고, 에인절스가 오타니의 MLB 목표를 도와줄 수 있는 최고의 팀이라고 믿었다”고 설명했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171211175737507\n",
      "['박나래', '좋은사람', '인스타그램']\n",
      "['윤균상', '박나래', '사진', '나래', '사진 윤균상']\n",
      "['사진 속 윤균상과 박나래는 엄청난 키 차이를 보이며 장난스러운 포즈를 하고 있다', '이어 윤균상은 \"하지만 나래바는 안갈거야\"라고 덧붙였고 박나래는 \"균상아']\n",
      "\n",
      "http://v.media.daum.net/v/20171203171100213\n",
      "['체감온도', '비', '경기남부']\n",
      "['아침', '서해', '내일', '영향', '남하']\n",
      "['기상청은 3일 \"내일 서해상에서 남하하는 기압골의 영향을 받다가 북서쪽에서 확장하는 찬 대륙고기압의 영향을 차차 받겠다\"며 \"전국에 가끔 구름이 많겠으나 경기남부와 강원영서남부, 충청도, 전라도는 새벽부터 아침 사이에 비 또는 눈, 제주도는 오전에 가끔 비가 오는 곳이 있겠다\"고 밝혔다', '서해 앞바다에선 0.5~2.5m, 남해 앞바다에선 0.5~2.0m로 일 전망이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205065021584\n",
      "['강경준', '시어머니', '신승훈', '우효광', '동상이몽']\n",
      "['장신영', '시어머니', '이몽', '동상 이몽', '동상']\n",
      "[\"4일 방송한 SBS 예능 프로그램 '동상이몽2-너는 내 운명'에서 장신영은 연인 강경준, 예비 시어머니와 함께 김장을 담갔다\", '장신영은 \"오빠가 저희 부모님과 함께 식사 하면서 결혼 허락을 받았다\"라고 말하면서 \"\\'동상이몽\\'에 출연해 우리 두 사람이 잘 지내고 응원도 받으니까 부모님이 더 좋아하셨다\"라고 방송이 큰 도움이 됐던 일도 털어놨다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209160144985\n",
      "['김수현', '결혼', '윤석민', '결혼식']\n",
      "['윤석민', '김예령', '김수현', '결혼식', '결혼']\n",
      "['[OSEN=박준형 기자] 9일 서울 광진구 그랜드 워커힐 비스타홀에서 윤석민과 김수현의 결혼식이 진행됐다', '윤석민과 김수현은 약 1년을 미뤄온 결혼식을 치루며 주변에 부부로서 인사를 올리게 됐다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203103735000\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '낚싯배', '전복사고', '세월호']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['구조', '대통령', '현장', '오전', '사항']\n",
      "['문 대통령은 곧바로 해경·행안부·세종상황실 등을 화상으로 연결해 상세보고를 받은 뒤 오전 9시 31분 국가위기관리센터에 구조작업 전반에 대한 사항을 지시했다', '문 대통령은 “지금 현재 총력을 다하고 있는데 그래도 정부가 추가로 지원할 것이 있으면 현장에 가서 상황을 파악하고 건의하라”고 김부겸 행안부 장관에게 별도 지시했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203190258781\n",
      "['며느리', '성폭력범죄', '낙태']\n",
      "['며느리', '성폭력', '성폭행', '중형', '아들']\n",
      "['의정부지법 형사합의12부(부장판사 노태선)는 성폭력 범죄의 처벌 등에 관한 특례법상 친족 관계에 의한 강간 등의 혐의로 기소된 이모(70)씨에게 징역 7년을 선고했다고 3일 밝혔다', '이씨는 2015년 함께 살던 아들이 숨진 지 불과 며칠이 지나지 않은 시점부터 며느리를 성폭행하려고 시도했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203100441479\n",
      "['조쉬 린드블럼']\n",
      "['린드블럼', '롯데', '구단', '연봉', '협상']\n",
      "['기본적으로 린드블럼은 계약 관계를 떠나 우리가 아끼는 선수다', '롯데는 린드블럼이 잘해도 연말 재계약 협상에서 큰 어려움이 없을 것으로 봤다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201060029695\n",
      "['황재균', '윤석민']\n",
      "['황재균', '윤석민', 'kt', '지명타자', '감독']\n",
      "['프로 입단 당시 3루 수비를 주로 했던 윤석민은 트레이드 전 몸담았던 넥센에서는 주로 1루수나 지명타자로 출장했다', '김 감독은 \"올 시즌 도중 윤석민을 트레이드로 데려오면서 취약한 포지션이 조금 메워졌다\"면서도 \"황재균과 달리 윤석민은 전문 3루수가 아니다\"고 했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206161349937\n",
      "['감기몸살', '눈길', '웹툰', '라면']\n",
      "['다니엘', '스케줄', '강다니엘', '시간', '새벽']\n",
      "['강다니엘은 최근 출연한 MBC ‘이불밖은 위험해’에서 계속되는 스케줄로 인한 고충을 토로했다', '당시 강다니엘은 “새벽 4시에 기상해서 스케줄 준비하고 연습하고, 숙소 들어오면 1시간 정도 잔다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203031909820\n",
      "['손흥민']\n",
      "['한국', '일본', '약체', '손흥민', '조추첨']\n",
      "['한국은 세계최강 독일이 있는 F조에, 일본은 그것보다 수월한 H조에 들어갔다', '손흥민은 \"사실 H조에 들어갔다고 하더라도 우리가 최약체였을 것\"이라고 했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171202100004988\n",
      "['커피', '카페인', '알코올']\n",
      "['커피', '알코올', '카페인', '체내', '클릭']\n",
      "['미국 브라운 대학교 알코올 및 중독 연구 센터 로버트 스위프트 박사에 따르면, 커피 속 카페인은 실제로 마신 술보다 적게 마신 것처럼 뇌를 속일 수 있다', '스위프트 박사는 \"카페인이 알코올의 진정 효과를 감춰버리지만 혈액 속 알코올 양은 전혀 줄어들지 않는 게 문제\"라며 \"술과 커피를 섞어 마시면 잠 잘 때에도 알코올 성분으로 인해 자다가 중간에 깨는 일이 생기고 오랫동안 체내에 남아 있는 카페인 성분으로 인해 다시 잠들기도 힘들게 된다\"고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201112057230\n",
      "NaN\n",
      "['월드컵', '포트', '스페인', '감독', '로페테기']\n",
      "['개최국 러시아가 1펀 포트에 들어갈 8개국 중 한 자리를 차지하면서, 스페인은 한 순위 차이로 2번 포트로 밀려났다', '로페테기 감독은 자신의 월드컵 경험을 묻자 “우리는 그때 한국과 비겼다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204061855808\n",
      "NaN\n",
      "['FA', '계약', '나이', '4년', '총액']\n",
      "['첫 번째 FA로는 두둑한 돈과 대우를 받았지만 두 번째 FA로는 찬바람만 분다', '4년 전 두산에서 롯데로 돌아오며 35억원을 받은 최준석, LG를 떠나 KIA로 가며 24억원을 받은 이대형도 첫 번째 FA처럼 대우를 받긴 힘들다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201145707416\n",
      "['공약', '어서와한국은처음이지', '어서와', '제주도', 'mbc에브리원']\n",
      "['한국', '시청률', '공약', '한국 한국', '어서와']\n",
      "[\"1일 시청률조사회사 닐슨코리아의 집계에 따르면 지난 11월30일 저녁 8시30분 방송된 '어서와 한국은 처음이지'는 4.805%의 전국유료방송가구 기준 시청률을 기록했다\", '앞서 \\'어서와 한국은 처음이지?\\' 출연진은 지난 9월 뉴스1과 진행한 인터뷰에서 시청률 5% 공약을 세우며 \"출연했던 친구들 다 불러서 단체 여행으로 제주도를 가면 어떨까 싶다\"는 바람을 전했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171208123637085\n",
      "['현빈', '결별', '변혁의사랑', '창궐', '꾼']\n",
      "['강소라', '현빈', '영화', '일정', '강소라 현빈']\n",
      "['강소라와 현빈은 지난해 10월 교제를 시작해 12월 공개적으로 이 같은 사실을 인정했다', \"현빈은 올초 선보인 영화 '공조'에 이어 '협상' '창궐' 촬영에 11월 개봉한 '꾼' 홍보 일정까지 바쁜 나날을 보내고 있다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171201140337198\n",
      "NaN\n",
      "['한화', '백승룡', '추승우', '손상득', '전상렬']\n",
      "['한화는 1일 전상렬 두산 2군 타격코치를 비롯해 손상득 전 LG 코치, 김남형, 백승룡, 추승우 등 5명의 코치들과 계약했다고 밝혔다', '- 1군(9명) : 한용덕 감독, 장종훈 수석코치 겸 타격코치, 송진우 투수코치, 김해님 불펜코치, 강인권 배터리코치, 전형도 작전주루코치, 고동진 1루주루코치, 채종국 수비코치, 이양기 타격보조코치 - 퓨처스(7명) : 최계훈 감독, 정민태 투수코치, 마일영 불펜코치, 김성래 타격코치, 전상렬 작전주루코치, 손상득 배터리코치, 김남형 수비코치 - 육성군(6명) : 윤학길 총괄코치, 이재우 투수코치, 추승우 작전주루코치, 이희근 배터리코치, 백승룡 수비코치, 정현석 타격코치 - 재활군(1명) : 차일목 재활담당코치 [사진] 전상렬 코치']\n",
      "\n",
      "http://v.media.daum.net/v/20171205060019094\n",
      "['강경준', '장신영', '결혼', '너는내운명', '서장훈']\n",
      "['장신영', '김장', '강경준', '결혼', '부모님']\n",
      "['이에 강경준은 \"식사하면서 허락을 받았다\"고 말했다', '장신영은 \"만남에 대한 허락은 받았는데, 결혼 승낙은 정식으로 받지는 않았었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205113928215\n",
      "['최명길', '의원직상실', '국민의당']\n",
      "['의원', '상실', '의원직', '국민의당', '의원 의원']\n",
      "[' 국민의당 의원들은 결국 최종심에서도 결과를 뒤집지 못하고 의원직을 상실한 최 전 의원의 소식에 안타까움과 착잡함을 감추지 못하는 모습이다', '한 중진 의원은 연합뉴스와의 통화에서 \"너무나 안타까운 소식에 당 분위기가 착잡함으로 침체됐다\"라면서 \"다만, 제대로 된 심리가 있었는지에 대해서는 고개를 갸우뚱할 수 있는 부분이 있지만 사법부 판결은 존중할 수밖에 없다\"라고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201155952702\n",
      "['권창훈']\n",
      "['권창훈', '디종', '프랑스', '리그', '유럽']\n",
      "['2016/2017시즌에 최고로 많이 뛴 라운드는 31라운드 마르세유전 79분이다', '권창훈은 아미앵 원정에서 환상적인 골을 터트렸다']\n",
      "\n",
      "http://v.media.daum.net/v/20171210220204480\n",
      "['춘천', '화재', '사우나']\n",
      "['할머니', '불길', '청년', '25', '춘천']\n",
      "['(춘천=뉴스1) 김경석 기자 = 강원 춘천시 한 음식점에서 식사 중이던 청년 3명이 화재가 난 주택 불길 속으로 들어가 거동이 불편한 할머니의 목숨을 구한 사실이 알려졌다', '결국 청년들은 전동휠체어를 타고 있는 할머니 이모씨(75·여)를 안전하게 구조했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201164533132\n",
      "NaN\n",
      "['삼성', '영입', '투수', '아델', '외국인']\n",
      "[' 덧붙여 이 관계자는 “아델만이 메이저리그에서도 손꼽히는 ‘타자 친화 구장’에서 뛴 것도 꼼꼼하게 체크했다', '삼성 관계자는 “꽤 비중 있는 외국인 선수가 영입 후보군에 들어 있다”며 “상황에 따라선 장기전이 될 수 있겠지만, 입단 협상이 긍정적으로 진행 중이라 조만간 팬들께 좋은 소식을 들려드릴 수 있을 것 같다”고 밝혔다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201104109612\n",
      "['지미 파레디스']\n",
      "['파레 디스', '디스', '파레', '두산', '파레디스']\n",
      "['[OSEN=이종서 기자] 두산 베어스는 1일 도미니카공화국 출신 스위치 타자 지미 파레디스(29)와 총액 80만 달러(계약금 10만 달러, 연봉 70만 달러)에 계약했다', '올 시즌 지바 롯데(타율 2할1푼9리(269타수 59안타) 10홈런 26타점)에서 뛰는 등 일본 프로야구 경험도 갖고 있어 KBO 리그에 잘 적응할 수 있을 것으로 보고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201075853915\n",
      "['김정혁']\n",
      "['김정혁', '전력', '데뷔', '전력 분석 업무', '전력 분석']\n",
      "['[OSEN=대구, 손찬익 기자] 삼성 라이온즈 내야수 김정혁이 전력 분석원으로 야구 인생 2막을 시작한다', '김정혁은 올 시즌 데뷔 첫 한 경기 4안타를 달성하는 등 타율 2할7푼4리(84타수 23안타) 13타점 15득점으로 드디어 성공의 꽃을 피우는가 했더니 오른쪽 팔꿈치 통증으로 수술대에 올랐다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201063244054\n",
      "['팻 딘']\n",
      "['팻딘', 'KIA', '달러', '계약', '다소간']\n",
      "['팻딘은 2017년 시즌을 앞두고 총액 90만 달러에 계약했다', '이어 팻딘은 “2018년 시즌이 또 무엇을 가져다줄지 매우 기대된다”며 2년차 생활에 대한 설렘도 숨기지 않았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201170017484\n",
      "NaN\n",
      "['에릭', '토트넘', '알리', '출전', '부상']\n",
      "[' ■ 긴장감 없는 주전 경쟁 올 시즌 케일, 알리, 에릭센의 활약은 지난 시즌까지와 비교하면 아쉬운 것이 사실이다', '케인은 올 시즌 리그에서 10골을 터트리며 제 몫을 다해주고 있지만, 알리와 에릭센은 좀처럼 만족스러운 경기력을 보여주지 못하고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201081854307\n",
      "NaN\n",
      "['불펜', '투수', '설리반', '마무리', '컵스']\n",
      "['오승환은 FA로 빠졌고, 잭 듀크와 후안 니카시오도 마찬가지다', '설리반은 \"컵스는 이번 오프 시즌에서 뒷문을 맡을 선수를 영입하려고 할 것이다\"라고 썼다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204103449370\n",
      "['권창훈']\n",
      "['권창훈', '디종', '프랑스', '인사', '에게']\n",
      "['그는 디종 팬임에도 불구하고 이미 \"권창훈에게 기회가 온다면 더 좋은 팀으로 가길 바란다\"고 말했다', '특히 그 중에서도 그는 이날 경기 시작 전부터 태극기를 들고와서 한쪽 구석에서 흔들던 팬 앞으로 직접 찾아가서 인사를 건넸는데, 그 팬은 한국인이 아닌 프랑스 현지인이었다.(권창훈과의 인터뷰 내용)    경기가 모두 끝난 후, 프랑스 방송사 카날+에서 권창훈에게 인터뷰를 요청했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209090043072\n",
      "['개그콘서트', '개그우먼', '유민상', '결혼', '임신']\n",
      "['홍예슬', '개그우먼', '엄마', '활약', '개그맨']\n",
      "[\"[TV리포트=이우인 기자] KBS2 '개그콘서트'에서 미녀 개그우먼으로 활약한 홍예슬이 엄마가 됐다\", '홍예슬의 임신 사실은 최근 자신의 인스타그램을 통해 개그우먼 동기들과 베이비샤워 하는 모습을 담으며 알려졌다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201083030620\n",
      "['박진영', '박소현', '김희철', '데뷔전', '주간아이돌']\n",
      "['박진영', '발레', '소현', '인생', '술집']\n",
      "['이에 박진영은 \"이제는 재밌는 것만 하고 싶다', '그러자 박진영은 \"소현이는 디테일한 건 기억 못 하지만 그 사람에 대한 인상은 그대로 남아있어서 \\'저 사람 나한테 잘못했어\\'는 확실하다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206175750619\n",
      "NaN\n",
      "['리그', '현대', '전북', 'ACL', '2018']\n",
      "['3.5장의 티켓을 보유한 K리그에서는 전북 현대, 제주 유나이티드(2위), 울산 현대(FA컵 우승)이 ACL 참가가 확정됐고, 수원 삼성(3위)은 플레이오프를 거쳐 차후 본선행이 결정된다', '# 2018 ACL 조추첨(동아시아) E조: 전북 현대, 킷치, PO4(텐진 콴잔 유력), PO2(세레소 오사카 또는 가시와 레이솔 유력) F조: 가와사키 프론탈레, 울산 현대, 멜버른 빅토리, PO3(상하이 상강 유력) G조: 광저우 에버그란데, J리그 일왕배 우승팀, 제주 유나이티드, 부리람 유나이티드 H조: 시드니FC, 상하이 선화, 가시마 앤틀러스, PO1(수원 삼성 유력) 사진= 윤경식 기자']\n",
      "\n",
      "http://v.media.daum.net/v/20171201214331076\n",
      "['김주혁', '결혼', '김생민', '창궐', '연예가중계']\n",
      "['조달환', '결혼', '김주혁', '연예가중계', 'TV']\n",
      "[\"조달환은 故 김주혁과 최근까지 영화 '창궐' 촬영을 함께했다\", '조달환은 \"저희가 결혼하기 일주일 전에 술을 사주면서 부럽다고 하셨다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201100155038\n",
      "['넥센 히어로즈', '박병호']\n",
      "['넥센', '타자', '2017', '박병호', '상대']\n",
      "['일단 4번타자 자리는 박병호의 차지가 될 가능성이 크다', ' 박병호 없이 치른 2017시즌, 넥센은 4번 자리가 자주 바뀌었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201172651193\n",
      "['장기용', '나의아저씨', '대세', '고백부부', '이선균']\n",
      "['장기', '아저씨', '부부', '고백', '고백 부부']\n",
      "[\"(서울=뉴스1) 윤효정 기자 = KBS '고백부부'에서 '남길선배'로 열연한 배우 장기용(25)이 차기작을 결정했다\", \"이선균 아이유 등이 출연하는 tvN '나의 아저씨'다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171205095327726\n",
      "['구하라', '논현동', '카라', '청담동', '홍수현']\n",
      "['건물', '구하라', '논현동', '매각', '임대']\n",
      "['부동산업계에 따르면 구하라는 2015년 6월5일 32억1500만원에 매입한 논현동 건물을 지난 9월21일 한 법인에게 38억원에 매각해 임대수익과 시세차익을 동시에 잡았다', '빌딩중개회사 ㈜빌사남의 김근오 팀장은 \"구하라가 논현동 건물을 매각하면서 세금, 이자비용, 임대수익 등을 제외한 단순 시세차익만 5억5000만~6억원에 이른다\"며 \"해당 건물은 위례-신사선 지하철역 개통 호재와 강남구에서 30억원대 건물 중 임대수익률이 5% 이상 나오는 매물이 없다는 희소성 때문에 매각된 것으로 보인다\"고 밝혔다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201095538792\n",
      "['정경호', '교도소', '박해수', '신원호', '지호']\n",
      "['제혁', '준호', '준호 제혁', '친구', '정경호']\n",
      "['누구도 믿을 수 없고 믿어서도 안되는 감옥 안에서 준호는 제혁의 유일한 희망이다', '구치소에서의 만남은 우연이었지만, 교도소의 내부 사정을 잘 알고 있는 준호는 과거 친구였으며 선의의 라이벌이었던 제혁을 홀로 둘 수 없었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201161351194\n",
      "['가족이야기', '뭉쳐야뜬다', '김태희', '퍼포먼스', '미니앨범']\n",
      "['이야기', '비는', '가족', '득녀', '또한']\n",
      "['그러다 그는 득녀 소감에 대한 질문을 받았고 \"가족 이야기를 하는 것에 대한 고민이 많다\"고 토로했다', '또한 비는 초심에 대한 이야기를 털어놓으면서도 \"난 태생이 여러분들에게 퍼포먼스를 보여드리는 가수다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205111504211\n",
      "['자숙', '샤이니', '사과문', 'sm엔터테인먼트', '이글']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['온유', '사과', '팬들', '개월', '샤이니']\n",
      "['이어 “지난 4개월 동안 활동을 쉬면서 부족한 저를 아껴주신 팬 여러분께 얼마나 큰 실망을 드렸는지, 깊이 반성하고 돌아보게 되었고, 제 스스로를 끝없이 원망하고 자책하기도 했다”는 온유는 “많이 사랑 받고 주목을 받을수록 더 철저하게 사적인 시간에도 책임감 있게 행동했어야 했는데, 실망스러운 모습을 보여드려 죄송한 마음뿐”이라고 밝혔습니다', '아직 검찰 결과가 나오지 않았다고는 해도 본인의 잘못을 인정하고 팬들에게 사과를 건네는 것은 사건 직후 충분히 할 수 있는 일이었죠']\n",
      "\n",
      "http://v.media.daum.net/v/20171209132938485\n",
      "['류현진']\n",
      "['다저스', '트레이드', '류현진', '미팅', '거닉']\n",
      "['역시 내년에 1천760만 달러(192억7천200만원)를 받는 왼손 투수 스콧 카즈미어도 다저스에 부담을 준다', '거닉 기자는 내년 다저스 전력에 포함될지 미지수인 곤살레스와 카즈미어가 트레이드 매물이 될 가능성은 작다면서 대신 류현진, 브랜던 매카시, 브록 스튜어트, 로스 스트리플링이 트레이드 카드가 될 수 있다고 예상했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201225254745\n",
      "['한혜진', '결별', '차우찬', '전현무', '박나래']\n",
      "['한혜진', '기사', '결별', '전현무', '박나래']\n",
      "['이날 박나래는 얼마 전 화제가 됐던 한혜진과 차우찬의 결벌 기사를 언급하자 한혜진은 발끈하며 \"난 여자도 때려\"라고 농담해 폭소를 유발했다', '박나래는 \"영부인 말고 회장(전현무) 부인은 어떠냐\"고 말했고 전현무는 \"결별 후에 왜 내 기사가 그렇게 난 거냐']\n",
      "\n",
      "http://v.media.daum.net/v/20171211223633282\n",
      "['고든램지', '이연복', '오승환', '대결', '냉장고를부탁해']\n",
      "['고든', '이연', '램지', '고든 램지', '대결']\n",
      "[\"고든 램지는 '퀵 차돌박이 볶음 말이'를 만들었다\", '특히 고든 램지는 긴장한 듯 바로 옆에 있는 도구도 찾지 못 했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171208183900414\n",
      "['한파', '비', '영하권']\n",
      "['다음', '한파', '주말', '기온', '다음 초반']\n",
      "['다음 주에는 기온이 급강하하면서 영하 10도를 밑도는 최강 한파가 찾아오겠습니다', '[최정희 / 기상청 예보분석관 : 다음 주 초반에는 우리나라 약 5km 상공에 영하 30도 이하의 찬 공기가 다시 남하하고, 찬 바람도 강하게 불어 체감온도도 낮겠습니다.] 서울 아침 기온이 월요일 -8도, 화요일에는 -11도까지 곤두박질하는 등 올겨울 들어 가장 춥겠고, 낮에도 영하권의 매서운 추위가 이어지겠습니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207180600061\n",
      "['키', '러브스토리', '눈길', 'sbs플러스', '냉장고를부탁해']\n",
      "['남편', '박진희', '방송', '여행', '남편 남편']\n",
      "['7일 방송되는 SBS플러스 \\'여행말고 美행\\' 측은 \"이날 방송에서 박진희와 남편의 러브스토리가 공개된다\"고 밝혔다', '박진희는 지난달 28일 방송된 JTBC \\'냉장고를 부탁해\\'에 출연해 \"남편이 순천에서 판사로 근무해 주말부부 생활을 한다\"고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171211070231027\n",
      "NaN\n",
      "['LG', '단장', '실장', '팀장', '구단']\n",
      "['LG는 선수 운영 부문과 경영 일반 부문을 이원화하여 구단 운영의 전문성을 높여 나갈 계획이라고 밝혔지만 실상은 인사권과 경영권을 독점하고 있는 진혁 실장 체제에서 송 전 단장의 권한과 역할은 운영팀장에서 벗어나지 못했다', '이 과정에서 그나마 마케팅 직원에게 맡겨야 야 할 업무를 홍보팀장과 팀원을 차출해 시즌 내내 발을 묶어뒀고, 송 전 단장의 후임 운영팀장은 뽑지 않았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171212131937690\n",
      "['추자현', '웨이보', '임신']\n",
      "['효광', '웨딩', '마인드', '마인드 웨딩', '사진']\n",
      "['[엑스포츠뉴스 김미지 기자] 배우 추자현의 남편인 중국 배우 우효광이 아내가 자신의 부모님에 선물한 리마인드 웨딩 비하인드 사진을 공개했다', ' 공개된 사진 속에는 지난 10월 추자현이 시부모님에게 선물한 리마인드 웨딩 비하인드 컷이 게재돼 있어 시선을 사로잡는다']\n",
      "\n",
      "http://v.media.daum.net/v/20171202023910089\n",
      "['이슈 · 국정원·군 정치개입 의혹', '우병우', '구속영장', '최윤수']\n",
      "['수석', '검찰', '수사', '구속영장', '기각']\n",
      "['검찰은 영장심사에서 공범으로 지목된 우 전 수석과 최 전 차장, 추명호 전 국정원 국익정보국장(54·구속)이 수사에 대비해 ‘제3자’를 중간에 끼고 여러 차례 전화 통화를 했다는 점 등을 들어 “증거인멸의 우려가 있다”고 주장했지만 법원은 받아들이지 않았다', '추 전 국장은 검찰 조사에서 “우 전 수석 지시를 받고 불법사찰을 수행했다”고 진술한 바 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201152048292\n",
      "['비', '이시언', '나혼자', '한혜진', '예고편']\n",
      "['이시언', '혼자', '공개', '모습', '절친']\n",
      "['공개된 예고편에서 이시언과 비는 거친 말투로 서로를 대하는가 하면, 노래방에서 지칠 때까지 노래를 부르며 흥을 발산해 진정한 ‘절친 케미’를 보였다', '공개된 스틸과 예고편만으로 두 사람의 사이가 짐작되는 바, 그동안 남궁민, 한혜진 등 다양한 스타들과 케미를 발산한 이시언이 비와는 어떤 모습을 보일지 기대감을 높인다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201133506384\n",
      "['나 혼자 산다', '이재진', '이영자', '전현무', '젝스키스']\n",
      "['이재진', '모습', '참견', '전지적', '이영자']\n",
      "['이후 VCR을 통해 이영자의 추천메뉴 대로 점심을 먹는 매니저의 모습을 보며 이영자가 \"좋아하지 않냐\"고 만족하자 이재진은 \"저렇게 먹으면 살찐다\"고 지적했다', '\\'전지적 참견시점\\' MC로 이를 지켜보던 \\'나 혼자 산다\\' 무지개 회장 전현무는 \"무슨 소리 하시는거냐']\n",
      "\n",
      "http://v.media.daum.net/v/20171202011507212\n",
      "NaN\n",
      "['러시아', '월드컵', '일본', '최고', 'FIFA']\n",
      "[' 지구촌 축구팬을 설레게 했던 2018 러시아 월드컵 본선 조추첨이 끝났다', '최고의 수혜자는 사상 처음으로 월드컵 본선을 개최한 러시아가 됐다']\n",
      "\n",
      "http://v.media.daum.net/v/20171212025211767\n",
      "NaN\n",
      "['석현준', '트루아', '가르시아', '활약', '영입']\n",
      "['그는 투쟁적이며 제공권이 좋다', '석현준의 정신력은 우리 팀에 본보기를 제시하고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203204211721\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '해경', '급유선', '낚싯배']\n",
      "['급유선 선장', '충돌', '급유선', '선장', '낚싯배']\n",
      "\n",
      "\n",
      "http://v.media.daum.net/v/20171201101735646\n",
      "['윤석민', '김수현', '결혼', '환희', '결혼식']\n",
      "['손자', '김예령', 'MVP', '윤석민', '손녀']\n",
      "['윤석민과 김수현은 9일 서울 광진구의 한 웨딩홀에서 결혼식을 올린다', '2011년에는 한국프로야구 MVP에 올랐고, 국가대표로서도 큰 활약을 펼치며 한국 최고의 오른손 투수로 성장했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201065024172\n",
      "['핀란드', '속초', '어서와', '막걸리', '어서와한국은처음이지']\n",
      "['핀란드', '속초', '막걸리', '한국', '설악산']\n",
      "['핀란드에는 동해란 이름을 가진 바다가 서쪽에 있는데 한국은 진짜 동쪽에 있다면서 신기함을 표했다', '속초의 이바이마을을 찾은 네 사람은 순대와 막걸리를 먹었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171206073112505\n",
      "['숙박업소', '바가지', '평창']\n",
      "['올림픽', '예약', '숙박', '관람객', '가격']\n",
      "['6일 강원도가 파악한 도내 숙박요금 동향을 보면 이달 1일 기준 개최 시·군과 배후도시 등 10개 시·군의 계약률은 업소 수(4천797개소) 기준 6％(265개소)다', '강원도는 계약률이 낮은 이유로 최근 일부 업소가 고액의 요금을 요구하면서 장기·단체 고객만 선호, 개별 관람객 예약을 받지 않는다는 여론이 확산해 관람객들이 올림픽 개최지 숙박을 포기한 결과로 보고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201193331290\n",
      "['신혼여행', '결혼식']\n",
      "['정주희', '기상캐스터', '정주희 기상캐스터', '축하', '날씨']\n",
      "['[SBS funE l 강경윤 기자] 날씨 뉴스를 책임지는 SBS 대표 미녀 정주희(30)기상캐스터가 품절녀가 됐다', '1일 SBS 8시뉴스 날씨뉴스로 돌아온 정주희 기상캐스터는  SBS funE 취재진에“많은 분들이 진심 어린 축하를 해주신 덕에 부부로서 뜻 깊은 첫 출발을 했다.”고 소감을 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203122651845\n",
      "['윌린 로사리오']\n",
      "['로사리오', '한국', '리그', '도미니카', '한화']\n",
      "[' 올해 한화와 총액 150만 달러에 계약한 로사리오는 내년 시즌 한신으로부터 훨씬 좋은 조건을 제시받았다', '한화는 로사리오에 재계약 의사를 전달했지만 로사리오는 한화 측의 제안을 거절한 것으로 전해지고 있다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201171509864\n",
      "['미지급', '눈길', '김우빈', 'sm', '고수']\n",
      "['불량', '광고', '협업', '에이전시', '협업 금지']\n",
      "['대중문화예술 발전에 근간을 흔드는 불법행위로, 위 문제 해결에 적극적으로 대처 하고, 또 다른 제3의 피해를 막기 위해 상벌위 운영규칙안에 따라 본 협회 회원(사)들에게 S사와의 협업 금지를 공지한 사실을 확인한다\"고 밝혔다', '이어 \"본 사건의 심각성과 악의적인 행태를 반복하고 있는 실체를 알리고자 본 협회 회원(사)들에게 이와 관련 미지급 광고 모델료가 피해 당사자들에게 입금될 때까지 불량에이전시 S사와의 절대 협업 금지 의결을 다시 한번 공지한다\"고 덧붙였다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209193005504\n",
      "['이리', '출산', '임신']\n",
      "['의사', '폐경', '여성', '뉴스', '개인']\n",
      "['\"이렇게 갑자기 폐경이 되나요?\" 의사는 예의로나마 안타까워 하는 표정을 지으며 \"좀 빨리 온 편이네요\"라고 했다', '\"달맞이꽃 같은 건강보조식품 먹으면 호르몬 수치가 좀 올라가지 않을까요?\" 난 당황한 나머지 의사 앞에서 아무 말 대잔치를 했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171211133422561\n",
      "NaN\n",
      "['원투펀치', '월드컵', '친구 검색', '방법', '관건']\n",
      "['원투펀치 팬 여러분 안녕하세요', \"이번 주제는'대한민국, 월드컵에서 어떻게 살아남아야할까'입니다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171201170904702\n",
      "['소녀시대', 'sm', '서주현', '모범생', '뮤지컬']\n",
      "['서현', '소녀시대', '막내', 'SM', '이유']\n",
      "['고민하는 동안 불안함도 있었지만, 지금은 \\'그래 이런 것도 겪어야 성장하지!\\'라고 편하게 생각하고 있어요.\" \\'소녀시대 막내\\', 아니 서주현 서현은 \"소녀시대 이름에 먹칠하지 않게 잘 살겠다\"고 했다', '팬분들은 저희 자신보다 저희 생각을 더 많이 해주세요']\n",
      "\n",
      "http://v.media.daum.net/v/20171203200953478\n",
      "['신혜선', '박시후', '결혼', '장소라', '황금빛내인생']\n",
      "['도경', '장소라', '결혼', '황금', '박시후']\n",
      "[\"3일 방송된 KBS 2TV 주말드라마 '황금빛 내 인생' 28회에서는 최도경(박시후 분)이 장소라(유인영)에게 청혼한 모습이 그려졌다\", '이날 최도경은 \"합시다, 결혼\"이라며 고백했고, 장소라는 \"결혼이야 원래 하기로 했던 거고 약혼은 왜 생략해요? 하루라도 빨리 같이 살고 싶은 사람들이 약혼 생략하는데 우린 아니잖아요\"라며 의아해했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201115315594\n",
      "['박정희정권', '법무부', '국유지']\n",
      "['사건', '정부', '농지', '배상액', '법무부']\n",
      "[\"(서울=뉴스1) 최동순 기자 = 박정희 정권 당시 정부가 시민들의 땅을 강제로 빼앗은 이른바 '구로동 분배농지 사건'에 대한 국가 배상액이 약 1조원에 달할 것으로 전망된다\", '법무부 관계자는 \"아직 재판이 진행 중인 사건에 대해서도 대법원이 같은 취지의 판결을 내릴 경우 배상액이 9000억원 이상이 될 것으로 파악하고 있다\"며 \"재판이나 지급이 늦어질 경우 법정이자가 더해져 배상액이 더 증가할 가능성도 있다\"고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203154638521\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '낚싯배', '해경', '시화병원']\n",
      "['병원', '으로', '구조', '해경', '시화병원']\n",
      "['구조 당시 자력으로 병원에 온 생존자 2명은 이날 오후 1시40분께 주거지 근처의 병원으로 옮기겠다며 시화병원을 빠져 나간 상태다', ' 앞서 이날 오전 6시9분께 인천 영흥도 인근 해상에서 22명이 탑승한 낚싯배가 전복돼 13명이 숨졌다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201083729801\n",
      "['이슈 · 발리, 화산 분화', '발리', '전세기', '수라바야']\n",
      "['귀국', '발리', '외교부', '전세기', '화산']\n",
      "['(영종도=연합뉴스) 외교부 공동취재단·조준형 기자 = 인도네시아 발리에 갔다가 화산 분화로 발이 묶였던 한국인 여행객 266명이 정부가 투입한 전세기 편으로 1일 아침 인천공항을 통해 귀국했다', \"전세기편으로 귀국한 266명의 우리 국민은 화산 분화로 발리 공항이 폐쇄되는 통에 대부분 예정한 날 귀국을 하지 못한 채 공항과 그 주변에서 불안한 시간을 보냈고, 결국 300km 떨어진 수라바야 공항까지 버스로 15시간여 이동하는 '겹고생'을 한 끝에 무사히 귀국했다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171210205400802\n",
      "['평창', '강릉시', '모텔']\n",
      "['요금', '숙박', '가격', '숙박비', '업소']\n",
      "['[경포 주변 모텔 주인 : 과했다는 생각은 기본적으로 누구나 가지고 있을 겁니다', '낮춰서라도 편안하게 손님 받고 싶은 마음이 많은 거죠.] 인근의 이 펜션도 내년 2월 동안 침대 하나에 취사도 되는 방은 20만 원에, 복층 구조에 침대가 2개인 방은 25만 원을 받기로 했습니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201145742431\n",
      "['김정은', '헬기', '참수작전']\n",
      "['특임여단', '김정은', '특임', '작전', '참수']\n",
      "['군 당국은 1일 이른바 ‘김정은 참수부대’로 불리는 특임여단 부대 개편식을 충북 증평에서 남영신 특수전사령관 주관 하에 개최했다', '특임여단이 평양의 북한 지도부에 침투해 김정은 등을 제거하는 것과 동시에 우리 군의 현무 등 탄도미사일과 공군 및 해군의 타격 전력으로 북한 주요 군사시설을 파괴하는 개념이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171207161133071\n",
      "['김주혁', '1박2일', '차태현', '결방', '총파업']\n",
      "['추모', '김주혁', '49', '고인', '고인 추모']\n",
      "[\"[OSEN=이소담 기자] 고(故) 김주혁의 49재에 KBS 2TV '해피선데이-1박 2일'(이하 '1박2일') 멤버들이 참석해 고인을 추모한다\", \"이날 한 매체는 '1박2일' 멤버들이 고 김주혁이 있는 충남 서산시에 함께 내려가 고인의 마지막을 추모할 예정이라고 보도한 바 있다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171203182537364\n",
      "['김관진', '구속적부심', '석방']\n",
      "['판사', '석방', '결정', '법원', '수석']\n",
      "['최근 서울중앙지법 형사51부(재판장 신광렬 수석부장판사)가 군 사이버사령부 댓글 공작 등에 관여돼 구속된 김 전 장관과 임관빈 전 국방부 정책실장(64)을 구속적부심을 거쳐 석방한 것을 공개 비판한 것이다', '판사 출신의 더불어민주당 추미애 대표는 지난달 27일 당 최고위원회의에서 “사법부에 대한 국민의 불신이 높아짐을 직시해야 한다”고 했고, 같은 당 송영길 의원은 “석방을 결정한 신광렬 판사는 우병우 전 청와대 민정수석과 TK 동향에 대학·연수원 동기”라고 지적했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205070052680\n",
      "NaN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['일본', '감독', '월드컵', '16', '콜롬비아']\n",
      "['이어 트루시에 감독은 \"원래 월드컵은 첫 번째 경기가 가장 중요하다\"며, \"일본은 반드시 콜롬비아를 이겨야 한다', '아울러 트루시에 감독은 \"세네갈에는 사디오 마네(리버풀)만 있는 게 아니다\"라며, \"세네갈 선수들은 유럽 정상급 무대에서 활약 중이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171210014852769\n",
      "['손흥민']\n",
      "['손흥민', '토트넘', '전반', '경기', '스토크']\n",
      "['손흥민이 공격을 주도한 토트넘이 전반을 지배했다', '◆ 손흥민 3경기 연속 골, 토트넘 스토크 대파 후반에도 손흥민의 활약은 여전했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205071527845\n",
      "['샤이니', '이글', '청춘시대', 'sm엔터테인먼트', '사과문']\n",
      "['온유', '실망', '여러분', '자책', '시간']\n",
      "['샤이니 온유는 12월 4일 오후 샤이니 공식 홈페이지에 \"온유입니다\"라는 제목으로 자필 사과문을 게재했다', '온유는 이 글에서 \"지난 4개월 동안 쉬면서 깊이 반성하고 돌아보게 됐고 스스로를 원망하고 자책했다\"며 \"더 철저하게 사적인 시간에도 책임감 있게 행동했어야 했는데 실망스러운 모습 보여드려 죄송한 마음 뿐\"이라고 사과했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201060037707\n",
      "['이슈 · 국정원·군 정치개입 의혹', '국가정보원', '김병찬', '댓글공작']\n",
      "['서장', '검찰', '국정원', '안씨', '증언']\n",
      "['2012년 12월 당시 서울경찰청을 담당하던 안씨는 “김 서장과 통화하며 국정원 댓글 수사상황을 들었다”고 검찰에 진술한 것으로 알려졌다', '그러나 안씨는 4년이 지난 최근 검찰에서 진술을 번복했고, 검찰은 이를 바탕으로 김 서장을 공무상 비밀누설 혐의의 피의자 신분으로 수사선상에 올린 것이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171211075017579\n",
      "['마녀사냥', '헌법재판소', '위헌']\n",
      "['의료', '치과', '네트워크', '병원', '대표']\n",
      "[\"유디치과와 의료민영화는 무관하며 2012년 8월에 시행된 '1인1개소법(의료법 33조8항)'도 위헌이라는 주장을 굽히지 않았다\", \"건강보험이 적용되지 않는 비급여 치과시술에 '가격파괴'를 선언하면서 치과계에 미운털이 박혔고 결과적으로 '1인1개소법'이 제정됐다는 게 고 대표의 생각이다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171201120458874\n",
      "['가상화폐', '도박', '댓글']\n",
      "['투자', '가상', '화폐', '가상 화폐', '화폐 투자']\n",
      "['A씨는 눈앞이 컴컴해졌다', '최근까지 어머니는 A씨에게 \"투자가 잘 돼 가냐\"고 물었는데 A씨는 \"아무 걱정하지 마시라\"는 식으로 거짓말을 할 수밖에 없었다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204175448480\n",
      "['채수빈', '로봇이아니야', '미모', 'mbc드라마', '김영록']\n",
      "['유승호', '로봇', '채수빈', '흑백', '유승호 채수빈']\n",
      "['MBC 드라마 측은 4일 공식 SNS에 \"작품이 따로 없는 우리 남주여주 \\'로봇이 아니야\\'\"라는 글과 함께 비하인드 사진들을 올렸다', '특히 유승호는 흑백 사진 속에서도 또렷이 떠오르는 꽃미모로 보는 이들을 설레게 했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201092243468\n",
      "['국방비', '인도', '대우조선']\n",
      "['국방비', '국방', '세계', '지출', '국방비 지출']\n",
      "[' 1위는 미국으로 6110억달러의 국방비를 지출한 것으로 나타났다', '기품원은 “2016년 세계 15대 국방비 지출국 중 5개국(중국, 인도, 일본, 한국, 호주)이 아시아와 오세아니아 지역에 위치한다”며 “아시아 지역에서의 계속되는 긴장, 즉 한반도, 북한과 남한, 중국과 일본, 동중국해를 둘러싼 중국과 동남아시아 국가들, 남중국해와 관련한 인도와 파키스탄, 인도와 중국 사이의 긴장은 각국의 군사력 현대화와 국방비 지출 증가를 견인한다”고 분석했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201201117785\n",
      "['이슈 · 포항 규모 5.4 지진', '흥해읍', '액상화', '지진']\n",
      "['동공', '도로', '흥해읍', '공간', '발견']\n",
      "['[기자] 지진 피해가 컸던 흥해읍의 한 아파트 옆 도로입니다', '이곳의 땅 밑을 조사해 봤더니 모두 4곳에서 텅 비어있는 공간인 동공이 발견됐습니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204084710945\n",
      "['방시혁', '대통령표창', '애니메이션', '썰전', '낭만닥터김사부']\n",
      "['콘텐츠', '부문', '대한민국', '산업', '수상']\n",
      "['방송영상산업발전유공포상의 비드라마 부문에서는 썰전(JTBC)’을 통해 시사와 예능의 결합이라는 새로운 방송 장르를 제시하며 정치, 사회 이슈에 대한 대중의 관심을 높인 이동희CP가 대통령표창을 받는다', '문체부 도종환 장관은 “우리 콘텐츠는 어려운 여건 속에서도 열정과 재능을 쏟은 콘텐츠업계 종사자들이 있었기 때문에 꾸준히 성장하고 세계적인 경쟁력을 가질 수 있었다.”라며, “오늘 수상하신 분들을 비롯한 모든 콘텐츠업계 종사자분들이 바로 대한민국 콘텐츠 산업을 밝히는 별들이다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203175101871\n",
      "NaN\n",
      "['민병헌', '롯데', '결혼식장', '롯데 민병헌', '인사']\n",
      "[매경닷컴 MK스포츠(서울)=천정환 기자] SK 와이번스 정의윤이 3일 오후 서울 강남구 역삼동 더 라움 마제스틱 볼룸에서 신부 이하야나양과 결혼식을 올렸다. 이날 결혼식장을 찾은 김현수가 롯데 민병헌과 인사를 나누고 있다.\n",
      "\n",
      "http://v.media.daum.net/v/20171206060439294\n",
      "NaN\n",
      "['FA', '영입', '4년', '계약', '롯데']\n",
      "['삼성은 2002년 프랜차이즈 스타 양준혁을 4년 27억2000만원을 주고 재영입, 창단 첫 우승의 퍼즐을 맞췄다', '한화는 2014시즌 정근우, 이용규를 각각 70억원, 67억원에 영입한 것을 시작으로 이듬해 권혁, 배영수, 송은범을 데려왔고, 2016시즌에는 간판타자 김태균과 4년 84억원에 계약한 뒤 정우람, 심수창까지 영입하는 과감한 행보를 했으나 올해까지 10년 연속 ‘가을야구’에 실패하고 말았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171212054510895\n",
      "NaN\n",
      "['이정은', 'KIA', '관계자', '사인', '이정']\n",
      "['이 모습을 본 골프계 관계자는 이정은의 순천이 고향이고 현재 조례동에 살고 있어 이정은이 KIA 타이거스의 팬일 것이라고 추측했다', '이정은은 KIA 관계자들을 잘 몰라 평소 안면이 있었던 한 프로야구 관계자의 도움으로 KIA 타이거즈 허영택 사장, 김기태 감독이 있는 테이블을 직접 찾아가 인사를 했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203063009629\n",
      "['동물원', '사우스차이나모닝포스트', '소셜네트워크서비스']\n",
      "['동물원', '동물', '펭귄', '관광객', '구이']\n",
      "['구이산 동물원은 희귀 동물을 볼 수 있고, 아이들은 교육프로그램을 통해 자연과 야생동물에 대해 더 많이 배울 수 있다며 홍보했다', '관광객들은 위쳇 등 소셜네트워크서비스(SNS)에 \"이걸 동물원이라고 부르는 것이냐\", \"나는 아직도 동물원에서 받은 충격이 잊혀지지 않는다\"며 분노를 터뜨렸다']\n",
      "\n",
      "http://v.media.daum.net/v/20171203201438511\n",
      "['김연아', '유영']\n",
      "['유영', '피겨', '김연아', '유영은', '보고']\n",
      "['저는 무대에서 즐기면 더 기분이 좋고… \"  유영은 김연아를 보고 피겨를 시작한 \\'김연아 키즈\\'입니다', ' [유영 / 피겨스케이팅 선수] \"2010년 벤쿠버 올림픽때 연아 언니보고 꿈을 키웠고…\"  4위에 오른 최다빈은 2차 선발전까지 종합점수 선두에 올라 평창행 가능성을 높였습니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171204110536658\n",
      "['최준석', '이우민']\n",
      "['보상', '롯데', '최준석', '협상', '이적']\n",
      "['넥센 히어로즈의 경우 채태인에 대해 이적시 보상선수를 받지 않겠다고 선언한 바 있다', '이우민의 경우 협상이 계속 지지부진할 경우 마찬가지로 보상선수 영입 철회 논의가 오갈 것으로 보인다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205061501311\n",
      "['이슈 · JSA 초소 북한군 귀순', '귀순병사', '아주대병원', '이국종']\n",
      "['병사', 'CNN', '북한', '의료', '수술']\n",
      "['[이국종 / 아주대병원 중증외상센터장 : 절반보다 훨씬 많은 피를 흘려, 그는 저혈압과 쇼크로 죽어가고 있었어요.] 이국종 교수를 비롯한 의료진이 30분여 분간 사투를 벌인 끝에 다행히 병사는 죽음의 문턱에서 다시 돌아왔습니다', '[이국종 / 아주대병원 중증외상센터장 : 병사가 여기가 진짜 남한 맞아요? 라고 묻길래, 제가 저 태극기를 한번 보라고 대답해줬죠.] CNN은 자유를 찾아 귀순한 병사를 매우 자랑스럽게 여긴다는 이 교수의 인터뷰도 소개했습니다']\n",
      "\n",
      "http://v.media.daum.net/v/20171211154156173\n",
      "NaN\n",
      "['하리', '경기', '글로리', '경기 계약', '무대']\n",
      "['[엠파이트=조형규 기자] K-1의 전성기 시절 영광을 누렸던 세기의 격투 악동 바다 하리(31, 네덜란드)가 돌아온다', '이번에 글로리와 다경기 계약을 체결하며 다시 킥복싱 무대로 돌아오는 하리의 복귀전은 오는 2018년 3월 4일 네덜란드 로테르담에서 개최되는 글로리 51에서 열린다']\n",
      "\n",
      "http://v.media.daum.net/v/20171212060128098\n",
      "['추자현', '우효광', '임신', '너는내운명', '결혼']\n",
      "['효광', '추자현', '태명', '추자', '추자 현은']\n",
      "['동동 콩콩 이런 식으로 하는 거다\"고 설명했지만, 우효광은 \\'우블리2\\'라는 태명을 제안했다', '내년이 개의 해니까 강아지는 어떻냐\"며 엉뚱한 태명을 지었고, 결국 추자현은 우효광을 노려봤다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205130918392\n",
      "['이슈 · 영흥도 낚싯배 전복사고', '선창1호', '해경', '실종자']\n",
      "['인천', '선창', '해경', '유해', '실종자']\n",
      "['인천구조대 보트로 인양된 이 씨의 유해는 진두항으로 이송돼 12시 29분께 이 씨의 배우자가 육안으로 신원을 확인했다', ' 인천해양경찰청은 앞서 용담 해수욕장 남단에서 선창 1호 실종자인 선장 오모(70) 씨의 유해도 발견했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171209171200844\n",
      "['최승호', '뉴스데스크', '박성호']\n",
      "['사장', '뉴스', '뉴스데스크', 'MBC', '체제']\n",
      "['이상현-배현진 앵커가 물러난 후, 평일 앵커가 된 김수지 아나운서는 오프닝에서 \"저희 MBC는 신임 최승호 사장의 취임에 맞춰, 오늘부터 \\'뉴스데스크\\' 앵커를 교체하고 당분간 뉴스를 임시체제로 진행한다\"고 말했다', \" 하지만 8일 뉴스에서는 최 사장이 취임 첫 행보로 택한 '해고자(강지웅·박성제·박성호·이용마·정영하·최승호) 즉각 복직 MBC 공동선언'이 보도됐다\"]\n",
      "\n",
      "http://v.media.daum.net/v/20171203180029003\n",
      "['직장인', '광화문', '뉴욕']\n",
      "['광고', '서울', '뉴욕', '여성', '서울시']\n",
      "['  익명을 원한 여성정책 전문가는 “여성의 이미지를 전통적인 여성상에 한정지었고, 부각된 실루엣과 문구가 어우러져 오해를 불러일으킬 수 있다”면서 “해외에서 선보일 서울의 이미지를 이런 식으로 표현한 것은 문제가 있다”고 지적했다', '남성 직장인 김모(37)씨는 “이미지 자체보다는 광고 속 문구가 문제인 것 같다”고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201103954573\n",
      "['평창롱패딩', '한정', '패션업계']\n",
      "['가격', '롱패딩', '제조', '생산', '저렴']\n",
      "['제조사인 신성통상은 베트남에 생산 설비를 갖춰 주문자상표부착생산(OEM)이 가능해 원가를 낮출 수 있었다', '패션업계 관계자는 \"일반적으로 의류 상품의 100% 완판은 구조적으로 어렵다\"며 \"보통 기업들이 정상 가격으로 제품을 70% 정도 판매하고 나머지 30%에 대해 가격 할인을 진행한다는 점을 고려하면 이번 판매는 상당히 이례적인 성공이다\"고 말했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171205070404735\n",
      "['임신', '프로듀스101', '판타스틱', '눈길', '결혼']\n",
      "['가희', '공개', '모습', '연예계', '임신']\n",
      "[' 지난해 3월 3살 연상의 사업가 양준무 씨와 결혼한 가희는 같은 해 10월 노아 군을 얻었다', '무엇보다 SNS에 예쁜 아들과 멋진 남편의 모습을 공개해 눈길을 모았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201094609457\n",
      "['식약처', '제조업체', '1인가구']\n",
      "['도시락', '식품', '편의점', '편의점 도시락', '원료']\n",
      "['이번 점검은 최근 1인 가구 증가로 수요가 늘고 있는 도시락 제품 등의 안전관리 강화를 위해 실시됐다고 식약처는 설명했다', '식약처 관계자는 “1인 가구 증가 등으로 생활 패턴과 식습관이 변하는 추세에 맞춰 편의점 도시락처럼 국민이 평소 많이 소비하는 식품의 안전관리를 보다 강화하겠다”고 강조했다']\n",
      "\n",
      "http://v.media.daum.net/v/20171201044337187\n",
      "['의료사고', '손해배상', '수명']\n",
      "['병원', '부모', '법원', '10', '가슴']\n",
      "['겁에 질린 A군이 수 차례 몸부림을 쳐 시술에 실패하자 병원 측은 버둥대지 못하게 온 몸을 천으로 감고 가슴과 머리 아래 베개를 댄 채 엎드리게 만들었다', '당시 병원 상대 소송에서 A군 부모는 2억 5,000만원을 배상 받았다']\n",
      "\n",
      "http://v.media.daum.net/v/20171210015042793\n",
      "['손흥민', '토트넘 핫스퍼 FC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-98-31a5113a39dd>\", line 48, in <module>\n",
      "    workrank = TextRankX(tis)\n",
      "  File \"<ipython-input-97-1edc7036bb10>\", line 66, in __init__\n",
      "    self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
      "  File \"<ipython-input-97-1edc7036bb10>\", line 52, in get_ranks\n",
      "    ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\", line 375, in solve\n",
      "    r = gufunc(a, b, signature=signature, extobj=extobj)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-98-31a5113a39dd>\", line 51, in <module>\n",
      "    workrank = TextRankX(tis)\n",
      "  File \"<ipython-input-97-1edc7036bb10>\", line 59, in __init__\n",
      "    self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
      "  File \"<ipython-input-97-1edc7036bb10>\", line 19, in get_nouns\n",
      "    nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/ckonlpy/tag/_twitter.py\", line 45, in nouns\n",
      "    tagged = self.pos(phrase)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/ckonlpy/tag/_twitter.py\", line 41, in pos\n",
      "    tagged += self._base.pos(eojeol)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/konlpy/tag/_twitter.py\", line 51, in pos\n",
      "    jpype.java.lang.Boolean(stem)).toArray()\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/jpype/_jclass.py\", line 60, in _getClassFor\n",
      "    def _getClassFor(javaClass):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-98-31a5113a39dd>\", line 53, in <module>\n",
      "    workrank = TextRankX(tis)\n",
      "  File \"<ipython-input-97-1edc7036bb10>\", line 59, in __init__\n",
      "    self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
      "  File \"<ipython-input-97-1edc7036bb10>\", line 21, in get_nouns\n",
      "    nouns.append(' '.join([noun for noun in self.twitter.morphs(str(sentence))\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/ckonlpy/tag/_twitter.py\", line 49, in morphs\n",
      "    return [s for s, t in self.pos(phrase)]\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/ckonlpy/tag/_twitter.py\", line 41, in pos\n",
      "    tagged += self._base.pos(eojeol)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/konlpy/tag/_twitter.py\", line 51, in pos\n",
      "    jpype.java.lang.Boolean(stem)).toArray()\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/jpype/_jclass.py\", line 60, in _getClassFor\n",
      "    def _getClassFor(javaClass):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/inspect.py\", line 1442, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 165, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/hyunyoun/anaconda3/lib/python3.6/inspect.py\", line 732, in getmodule\n",
      "    if f == _filesbymodname.get(modname, None):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-31a5113a39dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mworkrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRankX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-1edc7036bb10>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_sent_rank_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_rank_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_rank_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_rank_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_word_rank_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_rank_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_rank_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-1edc7036bb10>\u001b[0m in \u001b[0;36mget_ranks\u001b[0;34m(self, graph, d)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 연립방정식 Ax = b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-31a5113a39dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mworkrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRankX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mworkrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRankX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mworkrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-1edc7036bb10>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext2sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnouns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_sent_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-1edc7036bb10>\u001b[0m in \u001b[0;36mget_nouns\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n\u001b[1;32m     20\u001b[0m                                                   if noun not in self.stopwords and len(noun) > 1]))\n\u001b[0;32m---> 21\u001b[0;31m                 nouns.append(' '.join([noun for noun in self.twitter.morphs(str(sentence))\n\u001b[0m\u001b[1;32m     22\u001b[0m                                                    if noun not in self.stopwords and len(noun) > 1]))\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ckonlpy/tag/_twitter.py\u001b[0m in \u001b[0;36mmorphs\u001b[0;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mphrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ckonlpy/tag/_twitter.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtagged\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtagged0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtagged\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/konlpy/tag/_twitter.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jpype/_jclass.py\u001b[0m in \u001b[0;36m_getClassFor\u001b[0;34m(javaClass)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_getClassFor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjavaClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjavaClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_CLASSES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    binList = []\n",
    "    collection = 'newsDaum'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/daum2/'\n",
    "elif site.lower() == 'naver':\n",
    "    binList = []\n",
    "    collection = 'newsNaver'\n",
    "    etri_outcome = '/Users/hyunyoun/Documents/GitHub/Private_Project/personal_project/data/ETRI_OUTCOME/naver2/'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "slack = cb.Slacker(cb.slacktoken())\n",
    "useCollection = dh.Use_Collection(useDb, collection)\n",
    "keywordList = list()\n",
    "pressList = list()\n",
    "for data in useCollection.find({'site':site}):\n",
    "    pressList.append(data['press'])\n",
    "    #if len(data['mainText'].strip()) == 0:\n",
    "        #print (data['link'])\n",
    "        #break\n",
    "    idIs = data['_id']._ObjectId__id\n",
    "    tis = data['title']+'. '+data['mainText']\n",
    "    #tis2 = '.\\n'.join(tis.split('. '))\n",
    "    tis2 = data['mainText']\n",
    "    if len(tis) != 0:\n",
    "        if data['keywords'] !='NaN':\n",
    "            keywordList += list(map(lambda x: ''.join(x.split(' · ')[-1].split(' ')), data['keywords']))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    print (data['link'])\n",
    "    print (data['keywords'])\n",
    "    #print (tis2)\n",
    "    #print (tis2)\n",
    "    try:\n",
    "        from lexrankr import LexRank\n",
    "        lexrank = LexRank()\n",
    "        lexrank = LexRank(tagger='twitter')\n",
    "        lexrank.summarize(tis2)\n",
    "        lr = lexrank.probe(None)\n",
    "    except:\n",
    "        lr = tis2\n",
    "    try:\n",
    "        workrank = TextRankX(tis)\n",
    "    except:\n",
    "        try:\n",
    "            workrank = TextRankX(tis)\n",
    "        except:\n",
    "            workrank = TextRankX(tis)\n",
    "    print (workrank.keywords())\n",
    "    print (lr)\n",
    "    print ()\n",
    "    '''\n",
    "    if not os.path.isfile(etri_outcome+idIs.hex()+'.picked.txt'):\n",
    "        try:\n",
    "            etri = USE_ETRI_ANALYSIS('srl', tis2)\n",
    "        except:\n",
    "            print (etri[1])\n",
    "            break\n",
    "        else:\n",
    "            pickle.dump(etri[0], open(etri_outcome+idIs.hex()+'.picked.txt','wb'))\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-581aa7cd2d59>, line 168)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-85-581aa7cd2d59>\"\u001b[0;36m, line \u001b[0;32m168\u001b[0m\n\u001b[0;31m    tuples[path] = pmis / (len(path) - 1) * rs ** ( / len(path)) * len(path)\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import networkx\n",
    "import re\n",
    " \n",
    "class RawSentence:\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawSentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawTagger:\n",
    "    def __init__(self, textIter, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class RawTaggerReader:\n",
    "    def __init__(self, filepath, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    " \n",
    "    def load(self, sentenceIter, wordFilter = None):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "        def insertNearPair(a, b):\n",
    "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            for i, word in enumerate(sent):\n",
    "                if wordFilter and not wordFilter(word): continue\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                self.nTotal += 1\n",
    "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if wordFilter and not wordFilter(sent[j]): continue\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.dictNear.get((a, b), 0)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getI(self, a):\n",
    "        import math\n",
    "        if a not in self.dictCount: return None\n",
    "        return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def extract(self, ratio = 0.1):\n",
    "        ranks = self.rank()\n",
    "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "        pairness = {}\n",
    "        startOf = {}\n",
    "        tuples = {}\n",
    "        for k in cand:\n",
    "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "            for l in cand:\n",
    "                if k == l: continue\n",
    "                pmi = self.getPMI(k, l)\n",
    "                if pmi: pairness[k, l] = pmi\n",
    " \n",
    "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "            print(k[0], l[0], pairness[k, l])\n",
    "            if k not in startOf: startOf[k] = (k, l)\n",
    " \n",
    "        for (k, l), v in pairness.items():\n",
    "            pmis = v\n",
    "            rs = ranks[k] * ranks[l]\n",
    "            path = (k, l)\n",
    "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "            last = l\n",
    "            while last in startOf and len(path) < 7:\n",
    "                if last in path: break\n",
    "                pmis += pairness[startOf[last]]\n",
    "                last = startOf[last][1]\n",
    "                rs *= ranks[last]\n",
    "                path += (last,)\n",
    "                tuples[path] = pmis / (len(path) - 1) * rs ** ( / len(path)) * len(path)\n",
    " \n",
    "        used = set()\n",
    "        both = {}\n",
    "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "            if used.intersection(set(k)): continue\n",
    "            both[k] = tuples[k]\n",
    "            for w in k: used.add(w)\n",
    " \n",
    "        #for k in cand:\n",
    "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "        return both\n",
    " \n",
    "    def summarize(self, ratio = 0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pickle.load(open(etri_outcome+data['_id']._ObjectId__id.hex()+'.picked.txt','rb'))\n",
    "etri = testdata['return_object']['sentence']\n",
    "morp = [] ; wsd = [] ; word = [] ; ne = [] ; srl = [] ; dependency = []\n",
    "\n",
    "for i in etri:\n",
    "    x = Extract_Text_Info(i)\n",
    "    morp += x[0]\n",
    "    wsd += x[1]\n",
    "    word +=x[2]\n",
    "    ne += x[3]\n",
    "    for ii in i['SRL']:\n",
    "        srl += list(map(lambda x: (x['text'], x['type']),ii['argument']))\n",
    "    dependency += list(map(lambda x: (x['text'], x['label']), i['dependency']))\n",
    "morp1 = list(map(lambda x: x[0], morp))\n",
    "wsd1 = list(map(lambda x: x[0], wsd))\n",
    "word1 = list(map(lambda x: x[0], word))\n",
    "ne1 = list(map(lambda x: x[0], ne))\n",
    "srl1 = list(map(lambda x: x[0], srl))\n",
    "dependency1 = list(map(lambda x: x[0], dependency))\n",
    "outlist = []\n",
    "for idx in range(len(etri)):\n",
    "    y = pd.DataFrame(etri[idx]['dependency'])\n",
    "    y2 = y[y['mod'].apply(lambda x: len(x)) ==y['mod'].apply(lambda x: len(x)).max()]\n",
    "    y3 = y2[y2['weight'] == y2['weight'].max()]\n",
    "    y4 = y[y['id'].isin(y3['mod'][y3['mod'].index[0]])]\n",
    "    y5 = y[y['weight'] ==y4['weight'].max()]\n",
    "    y6 = y2.text.values.tolist() + y5.text.values.tolist()\n",
    "    y7 = y[y.text.isin(y6)]    \n",
    "    out = ' '.join(y7.text.tolist())\n",
    "    outlist.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sTest = data['title']+'. '+data['mainText']\n",
    "sTest = '.\\n'.join(sTest.split('. '))\n",
    "sTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sTest2 = sTest.split('\\n')\n",
    "sTest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextRank(re.sub('[\\W]','',sTest)).keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 형태소 분석 : morp\n",
    "* 어휘의미 분석 : wsd (동음이의어)\n",
    "* 어휘의미 분석 : wsd_poly (다의어)\n",
    "* 개체명 인식 : ner\n",
    "* 의존구문 분석 : dependency\n",
    "* 의미역 인식 : srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workrank1 = RunWordKRwordRank(data['title']+'. '+data['mainText'], 3)\n",
    "workrank1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword(etri_outcome,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMICalc:\n",
    "    def __init__(self, **kargs):\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.minNum = kargs.get('minNum', 5)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.searchPair = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    "    def train(self, sentenceIter):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    "            if a in self.searchPair: self.searchPair[a].add(b)\n",
    "            else: self.searchPair[a] = set([b])\n",
    "            if b in self.searchPair: self.searchPair[b].add(a)\n",
    "            else: self.searchPair[b] = set([a])\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            self.nTotal += len(sent)\n",
    "            for i, word in enumerate(sent):\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def getCoOccurrence(self, a, b):\n",
    "        if a > b: a, b = b, a\n",
    "        elif a == b: return\n",
    "        return self.dictBiCount.get((a, b), 0)\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.getCoOccurrence(a, b)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getHighestPair(self, a, n = 10):\n",
    "        return sorted(map(lambda b:(b, self.getPMI(a,b)), filter(lambda x:self.getCoOccurrence(a,x) >= self.minNum, self.searchPair[a])), key=lambda x:x[1], reverse=True)[:n]\n",
    "# 행 별로 문장이 구분되어있고, 탭으로 단어가 구분된 텍스트 파일을 읽어줍니다.\n",
    "class SentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxFilter = re.compile('.+/[NAVM].+')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            yield list(s.split('\\t'))\n",
    "\n",
    "pc = PMICalc(window=8)\n",
    "pc.train(ct.nouns(tis)) # test.txt 파일을 읽어와 분석합니다.\n",
    "print(pc.getHighestPair('노홍철', 10)) # 사람/NNG과 관련성이 높은 단어 100개를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Keywords(text, stopwords):\n",
    "    etri = Run_ETRI_Analysis(stopwords, text)\n",
    "    soy1 = RunLRNounExtractor(text)\n",
    "    soy2 = RunWordExtractor(text)\n",
    "    et0 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[0]))\n",
    "    et0 = list(map(lambda x: x[0], et0))\n",
    "    et1 = list(filter(lambda x: x[1] in ['NNG', 'NNP','NNG','SL','SH','NH'], etri[1]))\n",
    "    et1 = list(map(lambda x: x[0], et1))\n",
    "    et2 = list(map(lambda x: x[0], etri[2]))\n",
    "    et3 = list(map(lambda x: x[0], etri[3]))\n",
    "    et4 = list(filter(lambda x: x[1] in ['ARG0','ARG1','ARG2','ARG3','ARG4'], etri[4]))\n",
    "    et4 = list(map(lambda x: x[0], et4))\n",
    "    et5 = list(filter(lambda x: x[1] in ['NP','NP_SBJ','NP_OBJ','NP_AJT','VNP'], etri[5]))\n",
    "    et5 = list(map(lambda x: x[0], et5))\n",
    "    mecabout = list(filter(lambda x: x[1] in ['NNG','NNB','NNP'], mecab.pos(text)))\n",
    "    ctout = list(filter(lambda x: x[1] == 'Noun' , ct.pos(text)))\n",
    "    otout = list(filter(lambda x: x[1] == 'Noun' , ot.pos(text)))\n",
    "    mcout = list(map(lambda x: x[0], mecabout))\n",
    "    ctout = list(map(lambda x: x[0], ctout))\n",
    "    otout = list(map(lambda x: x[0], otout))\n",
    "    out = list(soy1.keys())+list(soy2.keys())+ctout+otout+mcout+et3+et4+et0+et1+et2+et5\n",
    "    vect = TfidfVectorizer().fit(out)\n",
    "    y = [list(soy1.keys()), list(soy2.keys()),et3, et4, ctout, otout, mcout,et0, et1, et2, et5]\n",
    "    y = list(filter(lambda x: len(x) !=0, y))\n",
    "    outdict = dict()\n",
    "    for i in y:\n",
    "        count = vect.transform(i).toarray().sum(axis = 0)\n",
    "        idx = np.argsort(-count)\n",
    "        count = count[idx]\n",
    "        feature_name = np.array(vect.get_feature_names())[idx]\n",
    "        out = dict(zip(feature_name, count))\n",
    "        for ii in out:\n",
    "            if not ii in outdict:\n",
    "                outdict[ii] = out[ii]\n",
    "            else:\n",
    "                outdict[ii] +=out[ii]\n",
    "    x = sorted(outdict.items(), key = itemgetter(1), reverse=True)[:5]\n",
    "    output = list(map(lambda x: x[0], x))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPMI(self, a, b):\n",
    "    import path\n",
    "    co = self.dictNear.get((a,b), 0)\n",
    "    if not co: return None\n",
    "    return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    "def iterate_textrank(self):\n",
    "    converged = True\n",
    "    for node in self.graph.nodes(data=True):\n",
    "        old_score = node[1]['score']\n",
    "        self.graph.add_node(node[0],\n",
    "                            score=self.calculate_score(node[0]))\n",
    "        if converged and abs(old_score - node[1]['score']) > self.convergence_threshold:\n",
    "            converged = False\n",
    "def calculate_score(self, node):\n",
    "    score_from_neighbors = sum(map(lambda n: self.graph.node[n]['score'] / len(self.graph.neighbors(n)),\n",
    "                                   self.graph.neighbors(node)))\n",
    "    return (1-self.damping_factor) + self.damping_factor * score_from_neighbors\n",
    "\n",
    "def textrank(self, max_iter = None, data = False):\n",
    "    if max_iter is None:\n",
    "        max_iter = self.graph.number_of_nodes()\n",
    "    for _in in range(max_iter):\n",
    "        if self.iterate_texterank():\n",
    "            break\n",
    "    sorted_nodes = sorted(self.graph.node(data = True), key= lambda node: node[1]['score'], reverse=True)\n",
    "    if not data:\n",
    "        sorted_nodes = list(mpa(lambda node:node[0], sorted_nodes))\n",
    "    return sorted_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k, l), v in pairness.items():\n",
    "    pmis = v\n",
    "    rs= ranks[k] * ranks[l]\n",
    "    path = (k,l)\n",
    "    tuples[path] = pmis / (len(path)-1) * rs ** (1 / len(path))* len(path)\n",
    "    last = l\n",
    "    while last in startOf and len(path) < 7:\n",
    "        if last in path:break\n",
    "        pmis += pairness[startOf[last]]\n",
    "        last = startOf[last][1]\n",
    "        rs *=ranks[last]\n",
    "        path += (last, )\n",
    "        tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "'''\n",
    "documents = [[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]]\n",
    "'''\n",
    "random.seed(0)\n",
    "K=4\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding = 'utf-8', mode='r') as f:\n",
    "        data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
