{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 ( Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter, Mecab\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chat_bot as cb\n",
    "import Database_Handler as dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 533869, 5532]\n",
      "[('이', 'Noun'), ('것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n",
      "best: [('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "mecab = Mecab()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "print (list(map(lambda x: len(xxxx[x]), xxxx)))\n",
    "\n",
    "score_weights = {\n",
    "    'num_nouns': -0.1,\n",
    "    'num_words': -0.2,\n",
    "    'no_noun': -1\n",
    "}\n",
    "\n",
    "def my_score(candidate):\n",
    "    num_nouns = len([w for w,t in candidate if t == 'Noun'])\n",
    "    num_words = len(candidate)\n",
    "    no_noun = 1 if num_nouns == 0 else 0\n",
    "    \n",
    "    score = (num_nouns * score_weights['num_nouns'] \n",
    "             + num_words * score_weights['num_words']\n",
    "             + no_noun * score_weights['no_noun'])\n",
    "    return score\n",
    "\n",
    "ct.set_selector(score_weights, my_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 사전 추가 및 Stopwords 를 위한 전처리 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언론사 목록 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> stopwords를 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressList = pd.read_excel('./data/presslist.xlsx')\n",
    "\n",
    "with(open('./data/newspress.txt','w',encoding='utf-8')) as f:\n",
    "    f.write('\\n'.join(set(pressList['언론사'].tolist())))\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwordList = Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIA Dictionary 파일의 태그 변환을 위한 Twitter 태그와 Hannanum(KAIST) 태그의 매칭파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagFile = pd.read_excel('./data/Korean_POS_tags_comparison.xlsx',sheet_name='chart3',dtype = str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagDict = dict()\n",
    "for idx in tagFile.index:\n",
    "    if tagFile.loc[idx]['Hannanum'][:3] =='XSN':\n",
    "        tag = 'XSN'\n",
    "    else:\n",
    "        tag = tagFile.loc[idx]['Hannanum']\n",
    "    tagDict[tag]=tagFile.loc[idx]['Twitter_Korean_Text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TagChange(tag):\n",
    "    return tagDict[tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIA Dictionary파일로부터 Twitter 태그를 가진 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "niaDictFile = pd.read_excel('./data/NIADic.xlsx')\n",
    "\n",
    "niaDictFile = niaDictFile[niaDictFile['tag']!='xp']\n",
    "niaDictFile.loc[:,'tag'] = niaDictFile['tag'].map(lambda x: TagChange(x.upper()))\n",
    "\n",
    "for idx in niaDictFile.tag.unique():\n",
    "    outfile = ('./data/'+'twitter_tagger_' + idx +'_TagChange_from_NIADic.txt')\n",
    "    with open(outfile,'w',encoding='utf-8') as f:\n",
    "        outdata = niaDictFile[niaDictFile['tag']==idx]['term']\n",
    "        outdata.to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위키피디아 dump 파일로부터 title 정보만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikifile = './data/first_title_from_wikidumps.txt'\n",
    "if not os.path.isfile(wikifile):\n",
    "    outfile = open(wikifile,'w',encoding='utf-8')\n",
    "    wikiTextList = glob('./data/wikidumps/text/A*')\n",
    "    for folder in wikiTextList:\n",
    "        fileList = glob(folder+'/*')\n",
    "        for file in fileList:\n",
    "            with open(file, 'r',encoding ='utf-8') as f:\n",
    "                while 1:\n",
    "                    line = f.readline()\n",
    "                    if not line:break\n",
    "                    try:\n",
    "                        x1 = re.search('title',line).group()\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        outfile.write(line)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403187\n",
      "344623\n"
     ]
    }
   ],
   "source": [
    "outSet = set()\n",
    "with open('./data/first_title_from_wikidumps.txt','r',encoding='utf-8') as f:\n",
    "    while 1:\n",
    "        line = f.readline()\n",
    "        if not line:break\n",
    "        try:\n",
    "            tmp1 = re.split('title=',line)[1].strip()\n",
    "        except:pass\n",
    "        else:\n",
    "            tmp2 = re.sub('[\"]','',tmp1)[:-1]\n",
    "            tmp2 = tmp2.strip()\n",
    "            outSet.add(tmp2)\n",
    "    f.close()\n",
    "print (len(outSet))\n",
    "outSet = list(filter(lambda x: x!='', outSet))\n",
    "outSet = pd.DataFrame(outSet)\n",
    "outSet = outSet[outSet[0].str.match('[a-zA-Z0-9가-힣\\s]+$')]\n",
    "outSet.reset_index(drop=True, inplace = True)\n",
    "print (len(outSet))\n",
    "\n",
    "title_out_path = './data/title_from_wiki.txt'\n",
    "if not os.path.isfile(title_out_path):\n",
    "    outSet.to_csv(title_out_path,index=False, header = False, encoding='utf-8')\n",
    "del outSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### namu wiki 로부터 title정보만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451251\n",
      "451251\n"
     ]
    }
   ],
   "source": [
    "outSet = set()\n",
    "with open('./data/out_namu_wiki.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = html.unescape(line).strip()\n",
    "        if re.match('[a-zA-Z0-9가-힣\\s]+$',line):\n",
    "            outSet.add(line)\n",
    "    f.close()\n",
    "print (len(outSet))\n",
    "outSet = list(filter(lambda x: x!='', outSet))\n",
    "print (len(outSet))\n",
    "outSet = pd.DataFrame(list(outSet))\n",
    "if not os.path.isfile('./data/title_from_namuwiki.txt'):\n",
    "    outSet.to_csv('./data/title_from_namuwiki.txt',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Title from namuwiki & wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567759\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('./data/title_from_wiki_and_namuwiki.txt'):\n",
    "    title_from_wiki = pd.read_csv('./data/title_from_wiki.txt',header=None,encoding='utf-8')\n",
    "    title_from_namuwiki = pd.read_csv('./data/title_from_namuwiki.txt',header=None,encoding='utf-8')\n",
    "    titleDf = pd.concat([title_from_namuwiki, title_from_wiki])\n",
    "    titleDf.reset_index(drop=True, inplace = True)\n",
    "    print (len(titleDf))\n",
    "    titleDf.dropna(inplace=True)\n",
    "    titleDf.reset_index(drop=True, inplace = True)\n",
    "    print (len(titleDf))\n",
    "    titleDf.drop_duplicates(subset = [0], keep = False, inplace = True)\n",
    "    titleDf.reset_index(drop=True, inplace = True)\n",
    "    print (len(titleDf))\n",
    "    titleDf.drop(titleDf[titleDf[0].str.match('[0-9]+$')].index,inplace = True)\n",
    "    titleDf.reset_index(drop=True, inplace = True)\n",
    "    print (len(titleDf))\n",
    "    titleDf.drop(titleDf[titleDf[0].str.match('[a-zA-Z]+$')].index,inplace = True)\n",
    "    titleDf.reset_index(drop=True, inplace = True)\n",
    "    print (len(titleDf))\n",
    "    titleDf = titleDf[titleDf[0].str.split(' ').str.len() < 5]\n",
    "    titleDf.reset_index(drop=True, inplace = True)\n",
    "    print (len(titleDf))\n",
    "    titleDf.to_csv('./data/title_from_wiki_and_namuwiki.txt',index=False, header = False, encoding='utf-8')\n",
    "titleDf = pd.read_csv('./data/title_from_wiki_and_namuwiki.txt',encoding='utf-8', header=None)\n",
    "print (len(titleDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mecab과 customized-konlpy의 사용자사전 추가를 위한 namu wiki와 위키피디아 title 정보 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325438\n",
      "1157924\n",
      "885658\n",
      "882163\n",
      "882163\n",
      "882135\n",
      "882063\n",
      "882017\n",
      "881619\n",
      "702748\n",
      "616503\n"
     ]
    }
   ],
   "source": [
    "outSet = set()\n",
    "with open('./data/first_title_from_wikidumps.txt','r',encoding='utf-8') as f:\n",
    "    while 1:\n",
    "        line = f.readline()\n",
    "        if not line:break\n",
    "        try:\n",
    "            tmp1 = re.split('title=',line)[1].strip()\n",
    "        except:pass\n",
    "        else:\n",
    "            tmp2 = re.sub('[\"]','',tmp1)[:-1]\n",
    "            tmp2 = html.unescape(tmp2).strip()\n",
    "            outSet.add(tmp2)\n",
    "    f.close()\n",
    "arrayOut = pd.DataFrame(list(outSet))\n",
    "outSet = set()\n",
    "with open('./data/out_namu_wiki.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = html.unescape(line).strip()\n",
    "        outSet.add(line)\n",
    "    f.close()\n",
    "arrayOut2 = pd.DataFrame(list(outSet))\n",
    "\n",
    "df1 = pd.concat([arrayOut, arrayOut2])\n",
    "print (len(df1))\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop_duplicates(subset = [0], keep = False, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1 = pd.DataFrame(df1[0].str.split('/').str.get(0))\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1 = pd.DataFrame(df1[0].str.split(':').str.get(0))\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1 = pd.DataFrame(df1[0].str.split('\\([a-zA-Z0-9가-힣\\s\\W\\w\\S]+\\)').str.get(0))\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop_duplicates(inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[^a-zA-Z0-9가-힣\\s]+$')].index, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('\\([a-zA-Z]+\\)')].index, inplace=True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('\\([0-9]\\)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('\\([가-힣\\s]\\)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+\\.[0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+\\.[0-9]+[a-zA-Z]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+\\.[0-9].[a-zA-Z]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[\\.][0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[\\.][0-9]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('[\\.][a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('^(\\.[\\.a-zA-Z0-9])+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+png[.]+)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+jpg[.]+)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('(.+\\.+jpg+$)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+\\.+png+$)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+\\.+[a-zA-Z]+$)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9a-zA-Z\\./]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[\\W]+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600477\n",
      "586771\n",
      "577580\n",
      "577397\n",
      "577383\n",
      "575476\n"
     ]
    }
   ],
   "source": [
    "df1.drop(df1[df1[0].str.match('^[a-zA-Z ]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.\\([a-zA-Z0-9가-힣\\s]\\)+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.\\([^a-zA-Z0-9가-힣\\s]\\)+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('.[^a-zA-Z0-9가-힣\\s]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[^a-zA-Z0-9가-힣\\s]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[^가-힣]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('^[0-9]+년')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[\\(]+[a-zA-Z]+[\\)]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('.[0-9]+.+[a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[~].')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+[-]+[0-9]+')].index,inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+[-]+[0-9]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('^[0-9 ]+\\.+[0-9 ]+[a-zA-Z]+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.\\([a-zA-Z0-9가-힣\\s]+\\)+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('.+-[0-9]{4,}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[a-zA-Z0-9가-힣\\s]+[0-9]{0,4}-[0-9]{1,2}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.*[0-9]{0,4}-[0-9]{1,2}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561700\n",
      "561427\n",
      "561322\n",
      "559890\n",
      "559197\n",
      "531241\n",
      "530612\n",
      "524465\n",
      "524464\n",
      "523685\n",
      "523684\n",
      "520667\n"
     ]
    }
   ],
   "source": [
    "df1.drop(df1[df1[0].str.match('.*-$')].index, inplace =True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.+[0-9]{1,4}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('^[0-9]{1,}[a-zA-Z]{1,}')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+II$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+I$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[^a-zA-Z]+[\\-a-zA-Z]{1,1}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+및.+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+의 .+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+,[ ]{1,}')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1 = df1[df1[0].str.split(' ').str.len() < 5]\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0] ==''].index, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[a-zA-Z0-9가-힣\\s]{1,1}[^a-zA-Z0-9가-힣][a-zA-Z0-9가-힣\\s]{1,1}$')].index, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+imf.*')].index, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+[~`\\!@#\\$%\\^\\&\\*\\(\\)\\_\\+=\\{\\}\\[\\]\\\\\\|;:\\'\\\"<,>\\?/].*')].index, inplace = True)\n",
    "\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.to_csv('./data/second_title_from_wikidumps.txt',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/merged_data.txt',encoding = 'utf-8', engine = 'python',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>클라텐버그</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>노후준비지수</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>전략종목</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>재벌구속특위</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정치개혁특위</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0   클라텐버그\n",
       "1  노후준비지수\n",
       "2    전략종목\n",
       "3  재벌구속특위\n",
       "4  정치개혁특위"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공공데이터 포털에서 에서 뉴스 빅데이터 분석 정보로부터 키워드와 명사등의 정보를 추출하기\n",
    "* 자료 출처 \n",
    "> * [공공데이터 포털](http://www.data.go.kr/)  \n",
    "> * [빅카인즈 공공데이터](https://www.kinds.or.kr/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './data/news/high_frequency_noun/'\n",
    "path3 = './data/news/newstopic/'\n",
    "path4 = './data/news/people/'\n",
    "path5 = './data/news/4th_industry/'\n",
    "path6 = './data/news/have_negative_positive/constitution/'\n",
    "path7 = './data/news/have_negative_positive/household_debt/'\n",
    "path8 = './data/news/have_negative_positive/olymphic/'\n",
    "path9 = './data/news/have_negative_positive/'\n",
    "path10 = './data/news/etc1/'\n",
    "path11 = './data/news/etc2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인물, 기관, 지역, 장소 등의 명사 추출 from news big data in kinds.or.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathList2 = [path11,path10,path4, path5, path6, path7, path8, path9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/news/etc2/김영란법 관련 뉴스메타데이터(20110616-20160731).csv\n",
      "./data/news/people/연도별 분야별 TOP5 뉴스메이커.csv\n",
      "./data/news/people/2017년 11월 경제 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 11월 정치 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 9월 정치 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 9월 경제 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 10월 정치 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 10월 경제 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/4th_industry/4차 산업혁명 (20130101-20170630)_1.csv\n",
      "./data/news/4th_industry/4차 산업혁명 (20130101-20170630)_2.csv\n",
      "./data/news/4th_industry/4차 산업혁명 (20130101-20170630)_3.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_5.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_4.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_3.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_2.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_1.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_6.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_5.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_4.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_1.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_3.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_2.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_5.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_4.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_6.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_7.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_3.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_2.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_1.csv\n",
      "./data/news/have_negative_positive/누진제 관련 뉴스메타데이터(19900101-20160731).csv\n",
      "./data/news/have_negative_positive/증강현실 관련 뉴스메타데이터(19900101-2016.9.30).csv\n"
     ]
    }
   ],
   "source": [
    "def Read_File(file):\n",
    "    df = pd.read_csv(file,encoding = 'utf-8', engine = 'python')\n",
    "    targetColumns = list(filter(lambda x: ('인물' in x) or ('기관' in x) or ('지역' in x) or ('장소' in x),df.columns.values))\n",
    "    if len(targetColumns) != 0:\n",
    "        return df[targetColumns]\n",
    "    else:\n",
    "        return df[targetColumns]\n",
    "outpath = './data/news/prenoun/'\n",
    "for idx1, path in enumerate(pathList2):\n",
    "    fileList = glob(path+'*.csv')\n",
    "    fileList.extend(glob(path+'CSV'))\n",
    "    for idx2, file in enumerate(fileList):\n",
    "        outfile = outpath+str(idx1)+'_'+str(idx2)+'.txt'\n",
    "        if not os.path.isfile(outfile):\n",
    "            print (file)\n",
    "            rawFile = Read_File(file)\n",
    "            rawFile.to_csv(outfile,index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter1(isList):\n",
    "    tmp1 = set(isList)\n",
    "    #tmp3 = set(map(lambda x: x.strip(), set(itertools.chain.from_iterable(tmp2))))\n",
    "    tmp3 = list(filter(lambda x: len(x)>1, tmp1))\n",
    "    #tmp3 = list(filter(lambda x: x!='', tmp2))\n",
    "    #tmp3 = set(map(lambda x: x.strip(), set(itertools.chain.from_iterable(tmp3))))\n",
    "    tmp3 = list(map(lambda x: x.strip(), tmp3))\n",
    "    return tmp3\n",
    "def MakeCombination(li):\n",
    "    outlist = []\n",
    "    for ix in range(1, len(li)+1):\n",
    "        outlist += list(itertools.permutations(li,ix))\n",
    "    outlist = list(map(lambda x: ' '.join(x), outlist)) + list(map(lambda x: ''.join(x), outlist))\n",
    "    outlist = list(filter(lambda x: len(x)!=1, outlist))\n",
    "    outlist = outlist + list(map(lambda x: re.sub('[\\W]','', x), outlist))\n",
    "    return outlist\n",
    "def filter2(idx):\n",
    "    idx2 = set(filter(lambda x: x!='', set(MakeCombination(idx.split(' ')))))\n",
    "    idx2 = set(filter(lambda x: not '  'in x , idx2))\n",
    "    idx2 = set(map(lambda x: x.strip(), idx2))\n",
    "    return idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36409,), (188220,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/news/prenoun/'\n",
    "second_file_list = glob(path+'*.txt')\n",
    "outList = list()\n",
    "for file in second_file_list:\n",
    "    second_file = pd.read_csv(file, encoding = 'utf-8', engine = 'python', header=None)\n",
    "    for colorder in second_file.columns.values:\n",
    "        targetData = second_file[colorder]\n",
    "        targetData.dropna(inplace = True)\n",
    "        targetData.drop_duplicates(inplace = True)\n",
    "        targetData.reset_index(drop=True, inplace = True)\n",
    "        targetData = pd.DataFrame(list(set(list(itertools.chain.from_iterable(targetData.str.split(','))))))\n",
    "        targetData.reset_index(drop=True, inplace = True)\n",
    "        targetData.drop_duplicates(inplace = True)\n",
    "        targetData.reset_index(drop=True, inplace = True)\n",
    "        targetData = pd.DataFrame(list(set(list(itertools.chain.from_iterable( targetData[0].str.split('[가-힣 ]{3,3}기자'))))))\n",
    "        targetData = targetData[targetData[0].str.len() >1 ]\n",
    "        outList += targetData[0].tolist()\n",
    "outDf = pd.Series(outList)\n",
    "outDf.dropna(inplace = True)\n",
    "outDf.drop_duplicates(inplace=True)\n",
    "outDf.reset_index(drop = True, inplace = True)\n",
    "tmp1 = outDf[outDf.str.split(' ').str.len() == 1]\n",
    "#tmp1 = tmp1[tmp1.str.len() <= 15]\n",
    "tmp1.drop_duplicates(inplace=True)\n",
    "tmp1.dropna(inplace = True)\n",
    "tmp1.reset_index(drop = True, inplace = True)\n",
    "tmp2 = outDf[outDf.str.split(' ').str.len() > 1]\n",
    "tmp2 = pd.Series(list(set(list(itertools.chain.from_iterable( outDf.str.split(' '))))))\n",
    "#tmp2 = tmp2[tmp2.str.len() <15]\n",
    "tmp2.reset_index(drop = True, inplace = True)\n",
    "tmp2 = tmp2[tmp2.str.len() >1 ]\n",
    "tmp2.reset_index(drop = True, inplace = True)\n",
    "tmp2.drop_duplicates(inplace=True)\n",
    "tmp2.dropna(inplace = True)\n",
    "tmp1.shape, tmp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outis = pd.concat([tmp1,tmp2])\n",
    "outis.drop_duplicates(inplace = True)\n",
    "outis.reset_index(drop = True, inplace = True)\n",
    "outis = outis[outis.str.match('[a-zA-Z0-9가-힣\\s]+$')]\n",
    "outis.drop_duplicates(inplace = True)\n",
    "outis.reset_index(drop = True, inplace = True)\n",
    "outis.shape\n",
    "\n",
    "outis[outis.isin(df1[0])].to_csv('./data/news/noun/nouns_include.txt',index=False, header = False, encoding = 'utf-8')\n",
    "outis[~outis.isin(df1[0])].to_csv('./data/news/noun/nouns_exclude.txt',index=False, header = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword extract from news big data in kinds.or.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_File2(file):\n",
    "    df = pd.read_csv(file,encoding = 'utf-8', engine = 'python')\n",
    "    targetColumns = list(filter(lambda x: x in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], df.columns.values))\n",
    "    if len(targetColumns) != 0:\n",
    "        return df[targetColumns]\n",
    "    else:\n",
    "        return df[targetColumns]\n",
    "def Extract_Keywords(path):\n",
    "    fileList = glob(path+'*.csv')\n",
    "    fileList.extend(glob(path+'*.CSV'))\n",
    "    outDf = pd.DataFrame()\n",
    "    for file in fileList:\n",
    "        df = Read_File2(file)\n",
    "        if len(outDf) == 0:\n",
    "            outDf = df\n",
    "        else:\n",
    "            outDf = pd.concat([outDf, df], axis = 0)\n",
    "    outDf.reset_index(drop=True, inplace = True)\n",
    "    outDf.astype(str)\n",
    "    outDf = outDf.to_dict('list')\n",
    "    outDict = dict()\n",
    "    for outKey in outDf:\n",
    "        outDict[outKey] = filter3(outDf[outKey])\n",
    "    return outDict\n",
    "def filter3(isList):\n",
    "    tmp1 = set(isList)\n",
    "    tmp1 = list(filter(lambda x: type(x)!=float, tmp1))\n",
    "    tmp2 = list(map(lambda x: x.split(','), tmp1))\n",
    "    tmp3 = set(map(lambda x: x.strip(), set(itertools.chain.from_iterable(tmp2))))\n",
    "    tmp3 = list(filter(lambda x: len(x)>1, tmp3))\n",
    "    tmp3 = list(filter(lambda x: x!='', tmp3))\n",
    "    tmp3 = list(map(lambda x: x.strip(), tmp3))\n",
    "    return tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathList1 = [path3, path4, path5, path6, path7, path8, path9, path10,path11]\n",
    "#pathList1 = [path1]\n",
    "#outpath = './data/news/'\n",
    "outpath = './data/news/keywords/'\n",
    "outSet2 =set()\n",
    "for path1 in pathList1:\n",
    "    ek = Extract_Keywords(path1)\n",
    "    for info2 in ek:\n",
    "        out2 = pd.DataFrame(ek[info2])\n",
    "        out2 = out2[out2[0].str.match('[a-zA-Z0-9가-힣\\s]+$')]\n",
    "        out2.reset_index(drop=True, inplace=True)\n",
    "        out2 = out2[0].apply(lambda x: filter2(x))\n",
    "        out2.apply(lambda x: outSet2.update(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436204,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outis2 = pd.Series(list(outSet2))\n",
    "outis2.shape\n",
    "\n",
    "outis2.reset_index(inplace = True, drop=True)\n",
    "outis2.drop_duplicates(inplace = True)\n",
    "outis2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfilename_1 = outpath+ 'keywords_from_news_bigdata_include_wiki_keywords.txt'\n",
    "outfilename_2 = outpath+ 'keywords_from_news_bigdata_exclude_wiki_keywords.txt'\n",
    "outis2[outis2.isin(df1[0])].to_csv(outfilename_1 ,index=False, header = False, encoding='utf-8')\n",
    "outis2[~outis2.isin(df1[0])].to_csv(outfilename_2 ,index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142966, 1)\n"
     ]
    }
   ],
   "source": [
    "inkeywords = pd.read_csv('./data/news/keywords/keywords_from_news_bigdata_include_wiki_keywords.txt', encoding = 'utf-8', engine = 'python', header=None)\n",
    "innouns = pd.read_csv('./data/news/noun/nouns_include.txt', encoding = 'utf-8', engine = 'python', header=None)\n",
    "tdf = pd.concat([inkeywords, innouns])\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.drop_duplicates(inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "print (tdf.shape)\n",
    "tdf.to_csv('./data/news/last/last_keywords_nouns.txt',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325794,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exkeywords = pd.read_csv('./data/news/keywords/keywords_from_news_bigdata_exclude_wiki_keywords.txt', \n",
    "                        encoding = 'utf-8', engine = 'python', header=None)\n",
    "exnouns = pd.read_csv('./data/news/noun/nouns_exclude.txt', \n",
    "                        encoding = 'utf-8', engine = 'python', header=None)\n",
    "tdf = pd.concat([exkeywords, exnouns])\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.drop_duplicates(inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.dropna(inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf = tdf[tdf[0].str.match('[^0-9a-zA-Z]+')][0]\n",
    "tdf.drop(tdf[tdf.str.match('[가-힣]{3,4}기자')].index, inplace = True)\n",
    "tdf.drop(tdf[tdf.str.match('[가-힣]+(신문)')].index, inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.drop(tdf[tdf.str.match('[가-힣]+(일보)')].index, inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.drop(tdf[tdf.str.match('연합뉴스')].index, inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.drop(tdf[tdf.str.match('뉴스')].index, inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.drop(tdf[tdf.str.match('신문')].index, inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.drop(tdf[tdf.str.match('시즌')].index, inplace = True)\n",
    "tdf.reset_index(drop=True, inplace = True)\n",
    "tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: 0, dtype: object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf[tdf.str.match('[a-zA-Z]+씨')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47,), (325747,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf1 = tdf [ tdf.str.len() > 20]\n",
    "tdf2 = tdf [ tdf.str.len() <= 20]\n",
    "tdf1.shape, tdf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tdf2.apply(ct.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x11 = x1[x1.str.len() ==1]\n",
    "x111 = tdf2[x11[x11.apply(lambda x: x[0][1]=='Noun')].index]\n",
    "x112 = tdf2[x11[x11.apply(lambda x: x[0][1]!='Noun')].index]\n",
    "x111.shape, x112.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x111.to_csv('./data/news/keywords2.txt',index=False, header = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x12 = x1[x1.str.len() !=1]\n",
    "x121 = x12[x12.apply(lambda x: (x[-1][1] !='Suffix') & (x[-1][1] !='Adverb') &\n",
    "                   (x[-1][1] !='Josa') & (x[-1][1] !='Eomi') & (x[-1][1] !='Number') &\n",
    "                   (x[-1][1] !='Alpha') & (x[-1][1] !='Verb'))]\n",
    "#x12.apply(lambda x: list(map(lambda y: y[0], x)))\n",
    "x121.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1211 = tdf2[x121.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = tdf1.apply(ct.pos)\n",
    "x21 = x2[x2.str.len() ==1]\n",
    "x22 = x2[x2.str.len() !=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x211 = tdf1[x21.index]\n",
    "x221 = tdf1[x22.index]\n",
    "x211.shape, x221.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = glob('./data/checkfile/a/*.txt')\n",
    "datadf = pd.DataFrame(columns=[0])\n",
    "for data in datalist:\n",
    "    df = pd.read_csv(data,encoding='utf-8', header=None)\n",
    "    if len(datadf) == 0:\n",
    "        datadf = df\n",
    "    else:\n",
    "        datadf = pd.concat([datadf, df])\n",
    "datadf = datadf[0]\n",
    "datadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.drop_duplicates(inplace = True)\n",
    "datadf.reset_index(drop=True, inplace = True)\n",
    "datadf.dropna(inplace = True)\n",
    "datadf.reset_index(drop=True, inplace = True)\n",
    "datadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf = datadf.apply(lambda x: re.sub(' ','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.drop(datadf[datadf.str.contains('[a-zA-Z]+[양씨군사의는은이가]+$')].index, inplace = True)\n",
    "datadf.reset_index(drop=True, inplace = True)\n",
    "datadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.drop(datadf[datadf.str.contains('.+[씨는은이가]+$')].index, inplace=True)\n",
    "datadf.reset_index(drop=True, inplace = True)\n",
    "datadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = datadf[datadf.str.match('[a-zA-Z0-9]+$')]\n",
    "eng.reset_index(drop=True,inplace = True)\n",
    "datadf.drop(datadf[datadf.str.match('[a-zA-Z0-9]+$')].index, inplace=True)\n",
    "datadf.reset_index(drop=True, inplace = True)\n",
    "datadf.drop(datadf[datadf.str.match('[a-zA-Z0-9가-힣]+으로$')].index, inplace = True)\n",
    "datadf.reset_index(drop=True, inplace = True)\n",
    "datadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf2 = datadf.apply(ct.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf3 = datadf.apply(mecab.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf3_1 = datadf3[datadf3.apply(lambda x: len(list(map(lambda y: y[1], x)))) == 1]\n",
    "datadf3_1.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf3_2 = datadf3[datadf3.apply(lambda x: len(list(map(lambda y: y[1], x)))) != 1]\n",
    "datadf3_2.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf3_2 = datadf3_2[datadf3_2.apply(lambda x: x[-1][-1].split('+')).str.len() ==1]\n",
    "datadf3_2.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf3_2 = datadf3_2[datadf3_2.apply(lambda x: x[-1][-1]!='EC')]\n",
    "datadf3_2.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf = datadf3_2.apply(lambda x: ''.join(list(map(lambda y: y[0], x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.to_csv('./data/checkfile/merged_file.txt',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종성유무확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jongsung(x):\n",
    "    if (ord(x[-1])-0xAC00)%28 == 0:\n",
    "        return 'F'\n",
    "    else:\n",
    "        return 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = datadf3_1.apply(lambda x: x[0][0])\n",
    "b = pd.Series()\n",
    "#c = datadf3_1.apply(lambda x: '+'.join(list(map(lambda y: y[1], x))))\n",
    "c =pd.Series(['NNP']*len(a))\n",
    "d = pd.Series(['*']*len(a))\n",
    "d1 = a.apply(lambda x: Jongsung(x))\n",
    "x1 = pd.concat([a,b,b,b,c,d,d1,a,c,d,d,d],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = datadf3_2.apply(lambda x: ''.join(list(map(lambda y: y[0], x))))\n",
    "b = pd.Series()\n",
    "c = pd.Series(['NNP']*len(a))\n",
    "d = pd.Series(['*']*len(a))\n",
    "d1 = a.apply(lambda x: Jongsung(x))\n",
    "e = pd.Series(['Compound']*len(a))\n",
    "f = pd.Series(['*']*len(a))\n",
    "g = pd.Series(['*']*len(a))\n",
    "h = datadf3_2.apply(lambda x: '+'.join(list(map(lambda y: y[0]+'/'+y[1]+'/*', x))))\n",
    "x2 = pd.concat([a,b,b,b,c,d,d1,a,e,f,g,h],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex) \n",
    "계몽전의,1,2,3,NNP,*,T,계몽전의,Compound,*,*,계몽/NNG/*+전의/NNG/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1.columns = list(range(len(x1.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.columns = list(range(len(x2.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.concat([x1,x2])\n",
    "m.reset_index(drop = True, inplace = True)\n",
    "m.to_csv('./data/checkfile/for_mecab_dic.csv',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
