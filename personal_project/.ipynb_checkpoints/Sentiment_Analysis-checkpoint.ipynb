{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import html\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "import multiprocessing\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import numpy as np\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "mecab = Mecab()\n",
    "ct = ctwitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsSentence = pd.read_csv('./data/sentiment_data/merged_sentiment_data.txt',encoding='utf-8', header=None)\n",
    "newsSentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:] # header\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = read_data('./data/sentiment_data/ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in ct.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_doc = [(tokenize(row[1]), row[2]) for row in rating]\n",
    "news_doc = [(tokenize(newsSentence.loc[idx][0]), newsSentence.loc[idx][1]) for idx in newsSentence.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rating_doc+news_doc, open('./data/pre_data/pre_data_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_run_docs = [TaggedDocument(d, [c]) for d, c in rating_doc+news_doc]\n",
    "pickle.dump(tagged_run_docs, open('./data/pre_data/pre_data_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_run_docs = pickle.load(open('./data/pre_data/pre_data_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(tagged_run_docs, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer = doc2vec.Doc2Vec(size=500, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    doc_vectorizer.train(train, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "    doc_vectorizer.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('안철수/Noun', 0.5092235803604126),\n",
      " ('손학규/Noun', 0.4781286120414734),\n",
      " ('문/Noun', 0.47694167494773865),\n",
      " ('김종인/Noun', 0.41631853580474854),\n",
      " ('박근혜/Noun', 0.40870070457458496),\n",
      " ('정세균/Noun', 0.39632225036621094),\n",
      " ('김무성/Noun', 0.3882656991481781),\n",
      " ('김한길/Noun', 0.36604562401771545),\n",
      " ('이태근/Noun', 0.3626297116279602),\n",
      " ('박/Noun', 0.3578115403652191)]\n",
      "[('이명박/Noun', 0.6439641118049622),\n",
      " ('박근혜/Noun', 0.6232602000236511),\n",
      " ('노/Noun', 0.6187499761581421),\n",
      " ('김영삼/Noun', 0.5640239715576172),\n",
      " ('박/Noun', 0.48870864510536194),\n",
      " ('김대중/Noun', 0.4885497987270355),\n",
      " ('박정희/Noun', 0.47176212072372437),\n",
      " ('노태우/Noun', 0.46269893646240234),\n",
      " ('盧/Foreign', 0.4458937346935272),\n",
      " ('이승만/Noun', 0.4335157871246338)]\n",
      "[('이명박/Noun', 0.7078977823257446),\n",
      " ('박/Noun', 0.670047402381897),\n",
      " ('노무현/Noun', 0.6232602000236511),\n",
      " ('김영삼/Noun', 0.5593474507331848),\n",
      " ('노/Noun', 0.534331202507019),\n",
      " ('김대중/Noun', 0.4972655177116394),\n",
      " ('김/Noun', 0.46960729360580444),\n",
      " ('노태우/Noun', 0.45434486865997314),\n",
      " ('朴/Foreign', 0.4491111636161804),\n",
      " ('이회창/Noun', 0.4375873804092407)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#To save\n",
    "doc_vectorizer.save('./model/doc2vec_size500_epoch25.model')\n",
    "pprint(doc_vectorizer.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizer.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('문/Noun', 0.3691737651824951),\n",
      " ('안철수/Noun', 0.32957664132118225),\n",
      " ('손학규/Noun', 0.32620835304260254),\n",
      " ('박근혜/Noun', 0.3075103461742401),\n",
      " ('이태근/Noun', 0.29577916860580444),\n",
      " ('정세균/Noun', 0.29367774724960327),\n",
      " ('추미애/Noun', 0.2902194559574127),\n",
      " ('박/Noun', 0.2826129198074341),\n",
      " ('전혜숙/Noun', 0.27465057373046875),\n",
      " ('조수용/Noun', 0.27053505182266235)]\n",
      "[('노/Noun', 0.5221443176269531),\n",
      " ('이명박/Noun', 0.5167340636253357),\n",
      " ('박근혜/Noun', 0.4996199607849121),\n",
      " ('김영삼/Noun', 0.4124770760536194),\n",
      " ('박/Noun', 0.3791698217391968),\n",
      " ('노태우/Noun', 0.352672815322876),\n",
      " ('김대중/Noun', 0.3470269441604614),\n",
      " ('박정희/Noun', 0.3383614420890808),\n",
      " ('盧/Foreign', 0.32480141520500183),\n",
      " ('全斗換/Foreign', 0.32205379009246826)]\n",
      "[('박/Noun', 0.6092792749404907),\n",
      " ('이명박/Noun', 0.5681461095809937),\n",
      " ('노무현/Noun', 0.4996200203895569),\n",
      " ('노/Noun', 0.4388248026371002),\n",
      " ('김영삼/Noun', 0.42257383465766907),\n",
      " ('김대중/Noun', 0.4170014262199402),\n",
      " ('김/Noun', 0.4114646911621094),\n",
      " ('朴/Foreign', 0.36898529529571533),\n",
      " ('노태우/Noun', 0.35113632678985596),\n",
      " ('金泳三/Foreign', 0.3457714319229126)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer2 = doc2vec.Doc2Vec(size=1000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer2.build_vocab(train)\n",
    "\n",
    "for epoch in range(25):\n",
    "    doc_vectorizer2.train(train, total_examples=doc_vectorizer2.corpus_count, epochs=doc_vectorizer2.iter)\n",
    "    doc_vectorizer2.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer2.min_alpha = doc_vectorizer2.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer2.save('./model/doc2vec_size1000_epoch25.model')\n",
    "pprint(doc_vectorizer2.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizer2.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizer2.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer3 = doc2vec.Doc2Vec(size=2000, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer3.build_vocab(train)\n",
    "\n",
    "for epoch in range(35):\n",
    "    doc_vectorizer3.train(train, total_examples=doc_vectorizer3.corpus_count, epochs=doc_vectorizer3.iter)\n",
    "    doc_vectorizer3.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer3.min_alpha = doc_vectorizer2.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#To save\n",
    "doc_vectorizer3.save('./model/doc2vec_size2000_epoch35.model')\n",
    "pprint(doc_vectorizer3.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizer3.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizer3.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_doc2 = [(tokenize2(row[1]), row[2]) for row in rating]\n",
    "news_doc2 = [(tokenize2(newsSentence.loc[idx][0]), newsSentence.loc[idx][1]) for idx in newsSentence.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rating_doc2+news_doc2, open('./data/pre_data/pre_data_by_mecab_for_sentiment_analysis.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_run_docs2 = [TaggedDocument(d, [c]) for d, c in rating_doc2+news_doc2]\n",
    "pickle.dump(tagged_run_docs2, open('./data/pre_data/pre_data_by_mecab_tagged_run_docs.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_run_docs2 = pickle.load(open('./data/pre_data/pre_data_by_mecab_tagged_run_docs.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train2, test2 = train_test_split(tagged_run_docs2, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizerx = doc2vec.Doc2Vec(size=1500, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizerx.build_vocab(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    doc_vectorizerx.train(train, total_examples=doc_vectorizerx.corpus_count, epochs=doc_vectorizerx.iter)\n",
    "    doc_vectorizerx.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizerx.min_alpha = doc_vectorizerx.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save\n",
    "doc_vectorizerx.save('./model/doc2vec_by_mecab_size1500_epoch25.model')\n",
    "pprint(doc_vectorizerx.most_similar('문재인/Noun'))\n",
    "pprint(doc_vectorizerx.most_similar('노무현/Noun'))\n",
    "pprint(doc_vectorizerx.most_similar('박근혜/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
