{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification Using Word2Vec\n",
    "> * 네이버의 뉴스 기사를 이용하여 모델을 만들고 평가를 실시한뒤, 다음의 뉴스 기사를 이용하여 분류해보도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1213504241553485166\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Roc_Curve(x, y, model1, model2, model3, model4):\n",
    "    print ('Logistic Regression')\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y, model1.predict(x))\n",
    "    print ('Random Forest')\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y, model2.predict(x))\n",
    "    print ('Kernel SVM')\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(y, model3.predict(x))\n",
    "    print ('XGBoost')\n",
    "    import xgboost as xgb\n",
    "    fpr4, tpr4, thresholds4 = roc_curve(y, model4.predict(xgb.DMatrix(x)))\n",
    "    plt.plot(fpr1, tpr1, label=\"Logistic Regression\")\n",
    "    plt.plot(fpr2, tpr2, label=\"RandomForest\")\n",
    "    plt.plot(fpr3, tpr3, label=\"Kernel SVM\")\n",
    "    plt.plot(fpr4, tpr4, label='XGBoost')\n",
    "    plt.legend()\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot model history after `fit()`.\n",
    "    \"\"\"\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = int(multiprocessing.cpu_count())\n",
    "#sg (int {1, 0}) – Defines the training algorithm. If 1, CBOW is used, otherwise, skip-gram is employed.\n",
    "def Make_Word2Vec_Model(modelPath, data, size, epoch, sg, window,min_count, cbow_mean, workers, negative, hs, tagger):\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\")\n",
    "    from datetime import datetime\n",
    "    from gensim.models import Word2Vec\n",
    "    start = datetime.now()\n",
    "    modelName = 'word2vec_size-{}_epoch-{}_window-{}_negative-{}_hs-{}_sg-{}_cbow_mean-{}_min_count-{}_by-{}.model'.format(\n",
    "        size, epoch, window, negative, hs, sg, cbow_mean, min_count, tagger)\n",
    "    modelName = modelPath+modelName\n",
    "    print (modelName)\n",
    "    w2v_model = Word2Vec(size = size, sg = sg, cbow_mean = cbow_mean, negative = negative,\n",
    "                                  hs = hs, window = window, workers = workers, iter=epoch, min_count = min_count)\n",
    "    w2v_model.build_vocab(tqdm(data))\n",
    "    w2v_model.train(tqdm(data),  total_examples=w2v_model.corpus_count, epochs=w2v_model.iter, compute_loss = True)\n",
    "    w2v_model.init_sims(replace = True)\n",
    "    w2v_model.save(modelName)\n",
    "    end = datetime.now()\n",
    "    print (\"Total running time: \", end-start)\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nav_tokenizer(tagger, corpus, stopwords):\n",
    "    pos = tagger.pos(corpus)\n",
    "    pos = [t[0] for t in pos if not t[0] in stopwords]\n",
    "    return pos\n",
    "\n",
    "def MakeTaggedData(df, taggedDoc, tagger, stopwords, labelEncoder):\n",
    "    w2v_docs = list()\n",
    "    for idx in tqdm(df.index):\n",
    "        text = df.loc[idx,'title']+'.\\n'+df.loc[idx,'mainText']\n",
    "        pos = nav_tokenizer(tagger, text, stopwords)\n",
    "        category = df.loc[idx, 'category']\n",
    "        encodeCategory = labelEncoder.transform([category])\n",
    "        label = ['news_'+str(idx)]\n",
    "        w2v_docs.append(TaggedDocument(pos, label, encodeCategory))\n",
    "    return w2v_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naver : (15120, 11)\n",
      "Daum : (9372, 11)\n"
     ]
    }
   ],
   "source": [
    "#Naver\n",
    "naverData = pickle.load(open('./data/pre_data/stastics/for_statistics_Naver_from_mongodb.pickled','rb'))\n",
    "naverData = pd.DataFrame.from_dict(naverData, orient = 'index')\n",
    "naverData.reset_index(inplace = True)\n",
    "naverData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "#Daum\n",
    "daumData = pickle.load(open('./data/pre_data/stastics/for_statistics_daum_from_mongodb.pickled','rb'))\n",
    "daumData = pd.DataFrame.from_dict(daumData, orient = 'index')\n",
    "daumData.reset_index(inplace = True)\n",
    "daumData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "\n",
    "print ('Naver : {}'.format(naverData.shape))\n",
    "print ('Daum : {}'.format(daumData.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * words : 기사에서 나온 단어들 or keywords\n",
    "> * tags : 문서 tag\n",
    "> * classes : category\n",
    ">> 기사분류가 daum보다 naver에서 더 세분화되어 있기 때문에 네이버의 category 분류를 이용하기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IT/과학', '경제', '사회', '생활/문화', '세계', '스포츠', '연예', '정치'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(naverData['category'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(le, open('./data/pre_data/news_tagged_data/pre_data_category_label_encoder_by_ct_for_word2vec_news_classification.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "ct = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 기본 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 5284/15120 [09:47<18:12,  9.00it/s]"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/pre_data/news_tagged_data/pre_data_by_ct_for_word2vec_news_classification.pickled'):\n",
    "    w2v_docs = pickle.load(open('./data/pre_data/news_tagged_data/pre_data_by_ct_for_word2vec_news_classification.pickled', 'rb'))\n",
    "else:\n",
    "    w2v_docs = MakeTaggedData(naverData, TaggedDocument, ct, stopwords, le)\n",
    "    pickle.dump(w2v_docs, open('./data/pre_data/news_tagged_data/pre_data_by_ct_for_word2vec_news_classification.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainName = './data/pre_data/news_train_test_Data/pre_data_word2vec_train_for_news_classification_by_ct.pickled'\n",
    "testName = './data/pre_data/news_train_test_Data/pre_data_word2vec_test_for_news_classification_by_ct.pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(trainName) & os.path.isfile(testName):\n",
    "    train = pickle.load(open(trainName, 'rb'))\n",
    "    test = pickle.load(open(testName, 'rb'))\n",
    "else:\n",
    "    train, test = train_test_split(w2v_docs, test_size = 0.15)\n",
    "    pickle.dump(train,open(trainName,'wb'))\n",
    "    pickle.dump(test,open(testName,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ x.words for x in tqdm(train)]\n",
    "x_test = [x.words for x in tqdm(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = './news_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(modelPath = modelPath, data = x_train, size = 1000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 1, hs = 0, tagger = 'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(modelPath = modelPath, data = x_train, size = 1000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(modelPath = modelPath, data = x_train, size = 1000, epoch = 20, \n",
    "                   sg = 1, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'ct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 기본 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3944/15120 [07:15<20:34,  9.06it/s]"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/pre_data/news_tagged_data/pre_data_by_mecab_for_word2vec_news_classification.pickled'):\n",
    "    w2v_docs = pickle.load(open('./data/pre_data/news_tagged_data/pre_data_by_mecab_for_word2vec_news_classification.pickled', 'rb'))\n",
    "else:\n",
    "    w2v_docs = MakeTaggedData(naverData, TaggedDocument, ct, stopwords, le)\n",
    "    pickle.dump(w2v_docs, open('./data/pre_data/news_tagged_data/pre_data_by_mecab_for_word2vec_news_classification.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainName = './data/pre_data/news_train_test_Data/pre_data_word2vec_train_for_news_classification_by_mecab.pickled'\n",
    "testName = './data/pre_data/news_train_test_Data/pre_data_word2vec_test_for_news_classification_by_mecab.pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(trainName) & os.path.isfile(testName):\n",
    "    train = pickle.load(open(trainName, 'rb'))\n",
    "    test = pickle.load(open(testName, 'rb'))\n",
    "else:\n",
    "    train, test = train_test_split(w2v_docs, test_size = 0.15)\n",
    "    pickle.dump(train,open(trainName,'wb'))\n",
    "    pickle.dump(test,open(testName,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ x.words for x in tqdm(train)]\n",
    "x_test = [x.words for x in tqdm(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = './news_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(modelPath = modelPath, data = x_train, size = 1000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 1, hs = 0, tagger = 'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(modelPath = modelPath, data = x_train, size = 1000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(modelPath = modelPath, data = x_train, size = 1000, epoch = 20, \n",
    "                   sg = 1, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'mecab')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
