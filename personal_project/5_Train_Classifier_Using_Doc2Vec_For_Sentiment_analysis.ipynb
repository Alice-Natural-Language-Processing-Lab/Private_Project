{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 만들어진 Doc2Vec model을 통한 감정분석 실시\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import doc2vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17427044518938657962\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4969044377\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17760644295380746286\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:09:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Roc_Curve(x, y, model1, model2, model3):\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y, model1.predict(x))\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y, model2.predict(x))\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(y, model3.predict(x))\n",
    "    plt.plot(fpr1, tpr1, label=\"Logistic Regression\")\n",
    "    plt.plot(fpr2, tpr2, label=\"RandomForest\")\n",
    "    plt.plot(fpr3, tpr3, label=\"Kernel SVM\")\n",
    "    plt.legend()\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot model history after `fit()`.\n",
    "    \"\"\"\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* windows에서 모델을 만들때 사용한 gensim의 버전이 3.3이었고, mac에서는 버전이 맞지 않아서 만들어둔 모델을 불러오지 못하였음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Infer_Vector(docs, model):\n",
    "    return [model.infer_vector(doc.words) for doc in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "saveTrainPath = './data/pre_data/train_test_Data2/'\n",
    "saveClassifierPath = './data/pre_data/classifier/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "cores = int(multiprocessing.cpu_count() )\n",
    "print (cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-ct.model')\n",
    "model2 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-ct.model')\n",
    "model3 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-ct.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_by_ct_train_for_doc2vec_sentiment_analysis.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name1):\n",
    "    train_x_by_m1 = Get_Infer_Vector(train, model1)\n",
    "    \n",
    "    pickle.dump(train_x_by_m1,open(saveTrainPath+'train_x_'+name1,'wb'))\n",
    "    del train_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name2):\n",
    "    train_x_by_m2 = Get_Infer_Vector(train, model2)\n",
    "\n",
    "    pickle.dump(train_x_by_m2,open(saveTrainPath+'train_x_'+name2,'wb'))\n",
    "    del train_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name3):\n",
    "    train_x_by_m3 = Get_Infer_Vector(train, model3)\n",
    "\n",
    "    pickle.dump(train_x_by_m3,open(saveTrainPath+'train_x_'+name3,'wb'))\n",
    "    del train_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'train_senti_y_ct_for_sentiment_analysis'):\n",
    "    train_senti_y = [doc.sentiment for doc in tqdm(train)]\n",
    "    \n",
    "    pickle.dump(train_senti_y, open(saveTrainPath+'train_senti_y_ct_for_sentiment_analysis','wb'))\n",
    "    del train_senti_y\n",
    "    \n",
    "if not os.path.isfile(saveTrainPath+'train_tags_y_ct_for_sentiment_analysis'):\n",
    "    train_tags_y = [doc.tags for doc in tqdm(train)]\n",
    "    \n",
    "    pickle.dump(train_tags_y, open(saveTrainPath+'train_tags_y_ct_for_sentiment_analysis','wb'))\n",
    "    del train_tags_y\n",
    "\n",
    "if 'train' in locals():\n",
    "    del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_by_ct_test_for_doc2vec_sentiment_analysis.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name1):\n",
    "    test_x_by_m1 = Get_Infer_Vector(test, model1)\n",
    "    \n",
    "    pickle.dump(test_x_by_m1,open(saveTrainPath+'test_x_'+name1,'wb'))\n",
    "    del test_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name2):\n",
    "    test_x_by_m2 = Get_Infer_Vector(test, model2)\n",
    "\n",
    "    pickle.dump(test_x_by_m2,open(saveTrainPath+'test_x_'+name2,'wb'))\n",
    "    del test_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name3):\n",
    "    test_x_by_m3 = Get_Infer_Vector(test, model3)\n",
    "\n",
    "    pickle.dump(test_x_by_m3,open(saveTrainPath+'test_x_'+name3,'wb'))\n",
    "    del test_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'test_senti_y_ct_for_sentiment_analysis'):\n",
    "    test_senti_y = [doc.sentiment for doc in tqdm(test)]\n",
    "    \n",
    "    pickle.dump(test_senti_y, open(saveTrainPath+'test_senti_y_ct_for_sentiment_analysis','wb'))\n",
    "    del test_senti_y\n",
    "    \n",
    "if not os.path.isfile(saveTrainPath+'test_tags_y_ct_for_sentiment_analysis'):\n",
    "    test_tags_y = [doc.tags for doc in tqdm(test)]\n",
    "    \n",
    "    pickle.dump(test_tags_y, open(saveTrainPath+'test_tags_y_ct_for_sentiment_analysis','wb'))\n",
    "    del test_tags_y\n",
    "\n",
    "if 'test' in locals():\n",
    "    del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1\n",
    "* Doc2Vec(dm/m,d1000,n7,w10,mc5,s0.001,t12)\n",
    "* size-1000\n",
    "* epoch-20\n",
    "* window-10\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-0\n",
    "* dm_mean-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m1_name = 'train_x_Doc2Vec-dm-m-d1000-n7-w10-mc5-s0.001-t12-ct'\n",
    "train_x_by_m1 = pickle.load(open(saveTrainPath+train_x_by_m1_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_senti_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m1_name = 'test_x_Doc2Vec-dm-m-d1000-n7-w10-mc5-s0.001-t12-ct'\n",
    "test_x_by_m1 = pickle.load(open(saveTrainPath+test_x_by_m1_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_senti_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1903760.78it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1691702.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_y2 = [y[0] for y in tqdm(train_y)]\n",
    "test_y2 = [y[0] for y in tqdm(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(random_state=1234, n_jobs=cores)\n",
    "classifier.fit(train_x_by_m1, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m1, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(random_state=1234, n_jobs=cores)\n",
    "classifier2.fit(train_x_by_m1, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m1, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_x_by_m1)\n",
    "train_x_by_m1_2 = scaling.transform(train_x_by_m1)\n",
    "test_x_by_m1_2 = scaling.transform(test_x_by_m1)\n",
    "classifier3 =  SVC(kernel = 'linear', \n",
    "        cache_size= 10000) \n",
    "classifier3.fit(train_x_by_m1_2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m1_2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_x_by_m1_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaling\n",
    "del train_x_by_m1_2\n",
    "del test_x_by_m1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m1, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 :  Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "442359it [00:00, 704456.41it/s]\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "49151it [00:00, 743592.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m1))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m1))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np.array(test_y)\n",
    "y_train = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf671c3c8ba4ddaa8b048d268b5747a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=1000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [1000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    \n",
    "                    epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m1\n",
    "del train_x_by_m1_name\n",
    "del test_x_by_m1\n",
    "del test_x_by_m1_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2\n",
    "* Doc2Vec(dm/c,d1000,n7,w5,mc5,s0.001,t12)\n",
    "* size-1000\n",
    "* epoch-20\n",
    "* window-5\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-1\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m2_name = 'train_x_Doc2Vec-dm-c-d1000-n7-w5-mc5-s0.001-t12-ct'\n",
    "train_x_by_m2 = pickle.load(open(saveTrainPath+train_x_by_m2_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_senti_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m2_name = 'test_x_Doc2Vec-dm-c-d1000-n7-w5-mc5-s0.001-t12-ct'\n",
    "test_x_by_m2 = pickle.load(open(saveTrainPath+test_x_by_m2_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_senti_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y2 = [y[0] for y in tqdm(train_y)]\n",
    "test_y2 = [y[0] for y in tqdm(test_y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m2, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m2, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_x_by_m2)\n",
    "train_x_by_m2_2 = scaling.transform(train_x_by_m2)\n",
    "test_x_by_m2_2 = scaling.transform(test_x_by_m2)\n",
    "classifier3 =  SVC(kernel = 'linear', \n",
    "        cache_size= 10000) \n",
    "classifier3.fit(train_x_by_m2_2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m2_2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_x_by_m2_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaling\n",
    "del train_x_by_m2_2\n",
    "del test_x_by_m2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m2, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m2))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m2))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np.array(test_y)\n",
    "y_train = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=1000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [1000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    \n",
    "                    epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m2\n",
    "del train_x_by_m2_name\n",
    "del test_x_by_m2\n",
    "del test_x_by_m2_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3\n",
    "* Doc2Vec(dbow,d1000,n7,mc5,s0.001,t12)\n",
    "* size-1000\n",
    "* epoch-20\n",
    "* window-None\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-0\n",
    "* dm_concat-0\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m3_name = 'train_x_Doc2Vec-dbow-d1000-n7-mc5-s0.001-t12-ct'\n",
    "train_x_by_m3 = pickle.load(open(saveTrainPath+train_x_by_m3_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_senti_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m3_name = 'test_x_Doc2Vec-dbow-d1000-n7-mc5-s0.001-t12-ct'\n",
    "test_x_by_m3 = pickle.load(open(saveTrainPath+test_x_by_m3_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_senti_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y2 = [y[0] for y in tqdm(train_y)]\n",
    "test_y2 = [y[0] for y in tqdm(test_y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m3, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m3, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m3, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m3, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_x_by_m3)\n",
    "train_x_by_m3_2 = scaling.transform(train_x_by_m3)\n",
    "test_x_by_m3_2 = scaling.transform(test_x_by_m3)\n",
    "classifier3 =  SVC(kernel = 'linear', \n",
    "        cache_size= 10000) \n",
    "classifier3.fit(train_x_by_m3_2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m3_2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_x_by_m3_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaling\n",
    "del train_x_by_m3_2\n",
    "del test_x_by_m3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m3, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m3))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m3))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np.array(test_y)\n",
    "y_train = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=1000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [1000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    \n",
    "                    epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m3\n",
    "del train_x_by_m3_name\n",
    "del test_x_by_m3\n",
    "del test_x_by_m3_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-mecab.model')\n",
    "model2 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-mecab.model')\n",
    "model3 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-1000_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-mecab.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_by_mecab_train_for_doc2vec_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name1):\n",
    "    train_x_by_m1 = Get_Infer_Vector(train, model1)\n",
    "    \n",
    "    pickle.dump(train_x_by_m1,open(saveTrainPath+'train_x_'+name1,'wb'))\n",
    "    del train_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name2):\n",
    "    train_x_by_m2 = Get_Infer_Vector(train, model2)\n",
    "\n",
    "    pickle.dump(train_x_by_m2,open(saveTrainPath+'train_x_'+name2,'wb'))\n",
    "    del train_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name3):\n",
    "    train_x_by_m3 = Get_Infer_Vector(train, model3)\n",
    "\n",
    "    pickle.dump(train_x_by_m3,open(saveTrainPath+'train_x_'+name3,'wb'))\n",
    "    del train_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'train_senti_y_mecab_for_sentiment_analysis'):\n",
    "    train_senti_y = [doc.sentiment for doc in tqdm(train)]\n",
    "    \n",
    "    pickle.dump(train_senti_y, open(saveTrainPath+'train_senti_y_mecab_for_sentiment_analysis','wb'))\n",
    "    del train_senti_y\n",
    "    \n",
    "if not os.path.isfile(saveTrainPath+'train_tags_y_mecab_for_sentiment_analysis'):\n",
    "    train_tags_y = [doc.tags for doc in tqdm(train)]\n",
    "    \n",
    "    pickle.dump(train_tags_y, open(saveTrainPath+'train_tags_y_mecab_for_sentiment_analysis','wb'))\n",
    "    del train_tags_y\n",
    "\n",
    "if 'train' in locals():\n",
    "    del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_by_mecab_test_for_doc2vec_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name1):\n",
    "    test_x_by_m1 = Get_Infer_Vector(test, model1)\n",
    "    \n",
    "    pickle.dump(test_x_by_m1,open(saveTrainPath+'test_x_'+name1,'wb'))\n",
    "    del test_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name2):\n",
    "    test_x_by_m2 = Get_Infer_Vector(test, model2)\n",
    "\n",
    "    pickle.dump(test_x_by_m2,open(saveTrainPath+'test_x_'+name2,'wb'))\n",
    "    del test_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name3):\n",
    "    test_x_by_m3 = Get_Infer_Vector(test, model3)\n",
    "\n",
    "    pickle.dump(test_x_by_m3,open(saveTrainPath+'test_x_'+name3,'wb'))\n",
    "    del test_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'test_senti_y_mecab_for_sentiment_analysis'):\n",
    "    test_senti_y = [doc.sentiment for doc in tqdm(test)]\n",
    "    \n",
    "    pickle.dump(test_senti_y, open(saveTrainPath+'test_senti_y_mecab_for_sentiment_analysis','wb'))\n",
    "    del test_senti_y\n",
    "    \n",
    "if not os.path.isfile(saveTrainPath+'test_tags_y_mecab_for_sentiment_analysis'):\n",
    "    test_tags_y = [doc.tags for doc in tqdm(test)]\n",
    "    \n",
    "    pickle.dump(test_tags_y, open(saveTrainPath+'test_tags_y_mecab_for_sentiment_analysis','wb'))\n",
    "    del test_tags_y\n",
    "\n",
    "if 'test' in locals():\n",
    "    del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1\n",
    "* Doc2Vec(dm/m,d1000,n7,w10,mc5,s0.001,t12)\n",
    "* size-1000\n",
    "* epoch-20\n",
    "* window-10\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-0\n",
    "* dm_mean-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m1_name = 'train_x_Doc2Vec-dm-m-d1000-n7-w10-mc5-s0.001-t12-mecab'\n",
    "train_x_by_m1 = pickle.load(open(saveTrainPath+train_x_by_m1_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_senti_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m1_name = 'test_x_Doc2Vec-dm-m-d1000-n7-w10-mc5-s0.001-t12-mecab'\n",
    "test_x_by_m1 = pickle.load(open(saveTrainPath+test_x_by_m1_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_senti_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y2 = [y[0] for y in tqdm(train_y)]\n",
    "test_y2 = [y[0] for y in tqdm(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m1, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m1, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m1, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m1, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_x_by_m1)\n",
    "train_x_by_m1_2 = scaling.transform(train_x_by_m1)\n",
    "test_x_by_m1_2 = scaling.transform(test_x_by_m1)\n",
    "classifier3 =  SVC(kernel = 'linear', \n",
    "        cache_size= 10000) \n",
    "classifier3.fit(train_x_by_m1_2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m1_2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_x_by_m1_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaling\n",
    "del train_x_by_m1_2\n",
    "del test_x_by_m1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m1, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m1))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m1))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np.array(test_y)\n",
    "y_train = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=1000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [1000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    \n",
    "                    epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m1\n",
    "del train_x_by_m1_name\n",
    "del test_x_by_m1\n",
    "del test_x_by_m1_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2\n",
    "* Doc2Vec(dm/c,d1000,n7,w5,mc5,s0.001,t12)\n",
    "* size-1000\n",
    "* epoch-20\n",
    "* window-5\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-1\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m2_name = 'train_x_Doc2Vec-dm-c-d1000-n7-w5-mc5-s0.001-t12-mecab'\n",
    "train_x_by_m2 = pickle.load(open(saveTrainPath+train_x_by_m2_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_senti_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m2_name = 'test_x_Doc2Vec-dm-c-d1000-n7-w5-mc5-s0.001-t12-mecab'\n",
    "test_x_by_m2 = pickle.load(open(saveTrainPath+test_x_by_m2_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_senti_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y2 = [y[0] for y in tqdm(train_y)]\n",
    "test_y2 = [y[0] for y in tqdm(test_y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m2, train_y2)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m2, train_y2)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier2.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_x_by_m2)\n",
    "train_x_by_m2_2 = scaling.transform(train_x_by_m2)\n",
    "test_x_by_m2_2 = scaling.transform(test_x_by_m2)\n",
    "classifier3 =  SVC(kernel = 'linear', \n",
    "        cache_size= 10000) \n",
    "classifier3.fit(train_x_by_m2_2, train_y2)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m2_2, test_y2)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y2, classifier3.predict(test_x_by_m2_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaling\n",
    "del train_x_by_m2_2\n",
    "del test_x_by_m2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m2, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m2))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m2))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np.array(test_y)\n",
    "y_train = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=1000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [1000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    \n",
    "                    epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m2\n",
    "del train_x_by_m2_name\n",
    "del test_x_by_m2\n",
    "del test_x_by_m2_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3\n",
    "* Doc2Vec(dbow,d1000,n7,mc5,s0.001,t12)\n",
    "* size-1000\n",
    "* epoch-20\n",
    "* window-None\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-0\n",
    "* dm_concat-0\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m3_name = 'train_x_Doc2Vec-dbow-d1000-n7-mc5-s0.001-t12-mecab'\n",
    "train_x_by_m3 = pickle.load(open(saveTrainPath+train_x_by_m3_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_senti_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m3_name = 'test_x_Doc2Vec-dbow-d1000-n7-mc5-s0.001-t12-mecab'\n",
    "test_x_by_m3 = pickle.load(open(saveTrainPath+test_x_by_m3_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_senti_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m3, train_y)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m3, train_y)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier2.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_x_by_m3)\n",
    "train_x_by_m3_2 = scaling.transform(train_x_by_m3)\n",
    "test_x_by_m3_2 = scaling.transform(test_x_by_m3)\n",
    "classifier3 =  SVC(kernel = 'linear', \n",
    "        cache_size= 10000) \n",
    "classifier3.fit(train_x_by_m3_2, train_y)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m3_2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier3.predict(test_x_by_m3_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaling\n",
    "del train_x_by_m3_2\n",
    "del test_x_by_m3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m3, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m3))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m3))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np.array(test_y)\n",
    "y_train = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=200, batch_size = 250000,  verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=1000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode2 = Sequential()\n",
    "mode2.add(Dense(64, activation='relu', input_dim=1000))\n",
    "mode2.add(Dropout(0.25))\n",
    "mode2.add(Dense(32, activation='relu'))\n",
    "mode2.add(Dropout(0.125))\n",
    "mode2.add(Dense(1, activation='softmax'))\n",
    "mode2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = mode2.fit(train_vecs_w2v, y_train,epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = mode2.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [1000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    \n",
    "                    epochs=200, batch_size = 250000,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m3\n",
    "del train_x_by_m3_name\n",
    "del test_x_by_m3\n",
    "del test_x_by_m3_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
