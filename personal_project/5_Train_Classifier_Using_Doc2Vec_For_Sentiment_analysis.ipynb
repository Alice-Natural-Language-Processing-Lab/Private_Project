{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 만들어진 Doc2Vec model을 통한 감정분석 실시\n",
    "> * Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import doc2vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12943643292237712204\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Roc_Curve(x, y, model1, model2, model3):\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y, model1.predict(x))\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y, model2.predict(x))\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(y, model3.predict(x))\n",
    "    plt.plot(fpr1, tpr1, label=\"Logistic Regression\")\n",
    "    plt.plot(fpr2, tpr2, label=\"RandomForest\")\n",
    "    plt.plot(fpr3, tpr3, label=\"Kernel SVM\")\n",
    "    plt.legend()\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot model history after `fit()`.\n",
    "    \"\"\"\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* windows에서 모델을 만들때 사용한 gensim의 버전이 3.3이었고, mac에서는 버전이 맞지 않아서 만들어둔 모델을 불러오지 못하였음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Infer_Vector(docs, model):\n",
    "    return [model.infer_vector(doc.words) for doc in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "saveTrainPath = './data/pre_data/train_test_Data2/'\n",
    "saveClassifierPath = './data/pre_data/classifier/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "cores = int(multiprocessing.cpu_count() / 2)\n",
    "print (cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-2000_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-ct.model')\n",
    "model2 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-2000_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-ct.model')\n",
    "model3 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-2000_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-ct.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_by_ct_train.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name1):\n",
    "    train_x_by_m1 = Get_Infer_Vector(train, model1)\n",
    "    \n",
    "    pickle.dump(train_x_by_m1,open(saveTrainPath+'train_x_'+name1,'wb'))\n",
    "    del train_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name2):\n",
    "    train_x_by_m2 = Get_Infer_Vector(train, model2)\n",
    "\n",
    "    pickle.dump(train_x_by_m2,open(saveTrainPath+'train_x_'+name2,'wb'))\n",
    "    del train_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name3):\n",
    "    train_x_by_m3 = Get_Infer_Vector(train, model3)\n",
    "\n",
    "    pickle.dump(train_x_by_m3,open(saveTrainPath+'train_x_'+name3,'wb'))\n",
    "    del train_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'train_y_ct_for_sentiment_analysis'):\n",
    "    train_y = [doc.tags[0] for doc in tqdm(train)]\n",
    "\n",
    "    pickle.dump(train_y, open(saveTrainPath+'train_y_ct_for_sentiment_analysis','wb'))\n",
    "    del train_y\n",
    "\n",
    "if 'train' in locals():\n",
    "    del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_by_ct_test.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name1):\n",
    "    test_x_by_m1 = Get_Infer_Vector(test, model1)\n",
    "    \n",
    "    pickle.dump(test_x_by_m1,open(saveTrainPath+'test_x_'+name1,'wb'))\n",
    "    del test_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name2):\n",
    "    test_x_by_m2 = Get_Infer_Vector(test, model2)\n",
    "\n",
    "    pickle.dump(test_x_by_m2,open(saveTrainPath+'test_x_'+name2,'wb'))\n",
    "    del test_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'ct'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name3):\n",
    "    test_x_by_m3 = Get_Infer_Vector(test, model3)\n",
    "\n",
    "    pickle.dump(test_x_by_m3,open(saveTrainPath+'test_x_'+name3,'wb'))\n",
    "    del test_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'test_y_ct_for_sentiment_analysis'):\n",
    "    test_y = [doc.tags[0] for doc in tqdm(test)]\n",
    "\n",
    "    pickle.dump(test_y, open(saveTrainPath+'test_y_ct_for_sentiment_analysis','wb'))\n",
    "    del test_y\n",
    "\n",
    "if 'test' in locals():\n",
    "    del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1\n",
    "* Doc2Vec(dm/m,d2000,n7,w10,mc5,s0.001,t12)\n",
    "* size-2000\n",
    "* epoch-20\n",
    "* window-10\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-0\n",
    "* dm_mean-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m1_name = 'train_x_Doc2Vec-dm-m-d2000-n7-w10-mc5-s0.001-t12-ct'\n",
    "train_x_by_m1 = pickle.load(open(saveTrainPath+train_x_by_m1_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m1_name = 'test_x_Doc2Vec-dm-m-d2000-n7-w10-mc5-s0.001-t12-ct'\n",
    "test_x_by_m1 = pickle.load(open(saveTrainPath+test_x_by_m1_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(random_state=1234, n_jobs=cores)\n",
    "classifier.fit(train_x_by_m1, train_y)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m1, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier2 = RandomForestClassifier(random_state=1234, n_jobs=cores)\n",
    "classifier2.fit(train_x_by_m1, train_y)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m1, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier2.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_x_by_m1)\n",
    "train_x_by_m1_2 = scaling.transform(train_x_by_m1)\n",
    "test_x_by_m1_2 = scaling.transform(test_x_by_m1)\n",
    "classifier3 =  SVC(kernel = 'linear', \n",
    "        cache_size= 10000) \n",
    "classifier3.fit(train_x_by_m1_2, train_y)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m1_2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier3.predict(test_x_by_m1_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaling\n",
    "del train_x_by_m1_2\n",
    "del test_x_by_m1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m1, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m1))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m1))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np_utils.to_categorical(test_y,2)\n",
    "y_train = np_utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = model1.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "print ('Result embedding shape : {}'.format(pretrained_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = embedding_size,\n",
    "                   weights = [pretrained_weights]))\n",
    "model.add(LSTM(units = embedding_size))\n",
    "model.add(Dense(units= vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_vecs_w2v, y_train,\n",
    "                    epochs=2,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=0,\n",
    "                    callbacks=[TQDMNotebookCallback(show_inner=True)])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=2000))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=2000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [2000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    batch_size=200000,\n",
    "                    epochs=100,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m1\n",
    "del train_x_by_m1_name\n",
    "del test_x_by_m1\n",
    "del test_x_by_m1_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2\n",
    "* Doc2Vec(dm/c,d2000,n7,w5,mc5,s0.001,t12)\n",
    "* size-2000\n",
    "* epoch-20\n",
    "* window-5\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-1\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m2_name = 'train_x_Doc2Vec-dm-c-d2000-n7-w5-mc5-s0.001-t12-ct'\n",
    "train_x_by_m2 = pickle.load(open(saveTrainPath+train_x_by_m2_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m2_name = 'test_x_Doc2Vec-dm-c-d2000-n7-w5-mc5-s0.001-t12-ct'\n",
    "test_x_by_m2 = pickle.load(open(saveTrainPath+test_x_by_m2_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m2, train_y)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m2, train_y)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier2.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SVC(kernel = 'linear', gamma = 2)\n",
    "classifier3.fit(train_x_by_m2, train_y)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier3.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m2, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m2))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m2))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np_utils.to_categorical(test_y,2)\n",
    "y_train = np_utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=2000))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=2000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [2000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    batch_size=200000,\n",
    "                    epochs=100,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m2\n",
    "del train_x_by_m2_name\n",
    "del test_x_by_m2\n",
    "del test_x_by_m2_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3\n",
    "* Doc2Vec(dbow,d2000,n7,mc5,s0.001,t12)\n",
    "* size-2000\n",
    "* epoch-20\n",
    "* window-None\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-0\n",
    "* dm_concat-0\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m3_name = 'train_x_Doc2Vec-dbow-d2000-n7-mc5-s0.001-t12-ct'\n",
    "train_x_by_m3 = pickle.load(open(saveTrainPath+train_x_by_m3_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m3_name = 'test_x_Doc2Vec-dbow-d2000-n7-mc5-s0.001-t12-ct'\n",
    "test_x_by_m3 = pickle.load(open(saveTrainPath+test_x_by_m3_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_y_ct_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m3, train_y)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m3, train_y)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier2.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SVC(kernel = 'linear', gamma = 2)\n",
    "classifier3.fit(train_x_by_m3, train_y)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier3.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m3, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m3))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m3))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np_utils.to_categorical(test_y,2)\n",
    "y_train = np_utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=2000))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=2000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [2000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    batch_size=200000,\n",
    "                    epochs=100,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m3\n",
    "del train_x_by_m3_name\n",
    "del test_x_by_m3\n",
    "del test_x_by_m3_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-2000_epoch-20_window-10_negative-7_hs-0_dm-1_dm_concat-0_dm_mean-1_by-mecab.model')\n",
    "model2 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-2000_epoch-20_window-5_negative-7_hs-0_dm-1_dm_concat-1_dm_mean-0_by-mecab.model')\n",
    "model3 = doc2vec.Doc2Vec.load(loadModelPath+'doc2vec_size-2000_epoch-20_window-None_negative-7_hs-0_dm-0_dm_concat-0_dm_mean-0_by-mecab.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_by_mecab_train.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name1):\n",
    "    train_x_by_m1 = Get_Infer_Vector(train, model1)\n",
    "    \n",
    "    pickle.dump(train_x_by_m1,open(saveTrainPath+'train_x_'+name1,'wb'))\n",
    "    del train_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name2):\n",
    "    train_x_by_m2 = Get_Infer_Vector(train, model2)\n",
    "\n",
    "    pickle.dump(train_x_by_m2,open(saveTrainPath+'train_x_'+name2,'wb'))\n",
    "    del train_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'train_x_'+name3):\n",
    "    train_x_by_m3 = Get_Infer_Vector(train, model3)\n",
    "\n",
    "    pickle.dump(train_x_by_m3,open(saveTrainPath+'train_x_'+name3,'wb'))\n",
    "    del train_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'train_y_mecab_for_sentiment_analysis'):\n",
    "    train_y = [doc.tags[0] for doc in tqdm(train)]\n",
    "\n",
    "    pickle.dump(train_y, open(saveTrainPath+'train_y_mecab_for_sentiment_analysis','wb'))\n",
    "    del train_y\n",
    "\n",
    "if 'train' in locals():\n",
    "    del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_by_mecab_test.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = '-'.join(re.split('[\\(\\),\\/]',str(model1)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name1):\n",
    "    test_x_by_m1 = Get_Infer_Vector(test, model1)\n",
    "    \n",
    "    pickle.dump(test_x_by_m1,open(saveTrainPath+'test_x_'+name1,'wb'))\n",
    "    del test_x_by_m1\n",
    "    del name1\n",
    "    \n",
    "name2 = '-'.join(re.split('[\\(\\),\\/]',str(model2)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name2):\n",
    "    test_x_by_m2 = Get_Infer_Vector(test, model2)\n",
    "\n",
    "    pickle.dump(test_x_by_m2,open(saveTrainPath+'test_x_'+name2,'wb'))\n",
    "    del test_x_by_m2\n",
    "    del name2\n",
    "\n",
    "name3 = '-'.join(re.split('[\\(\\),\\/]',str(model3)))+'mecab'\n",
    "if not os.path.isfile(saveTrainPath+'test_x_'+name3):\n",
    "    test_x_by_m3 = Get_Infer_Vector(test, model3)\n",
    "\n",
    "    pickle.dump(test_x_by_m3,open(saveTrainPath+'test_x_'+name3,'wb'))\n",
    "    del test_x_by_m3\n",
    "    del name3\n",
    "\n",
    "if not os.path.isfile(saveTrainPath+'test_y_mecab_for_sentiment_analysis'):\n",
    "    test_y = [doc.tags[0] for doc in tqdm(test)]\n",
    "\n",
    "    pickle.dump(test_y, open(saveTrainPath+'test_y_mecab_for_sentiment_analysis','wb'))\n",
    "    del test_y\n",
    "\n",
    "if 'test' in locals():\n",
    "    del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1\n",
    "* Doc2Vec(dm/m,d2000,n7,w10,mc5,s0.001,t12)\n",
    "* size-2000\n",
    "* epoch-20\n",
    "* window-10\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-0\n",
    "* dm_mean-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m1_name = 'train_x_Doc2Vec-dm-m-d2000-n7-w10-mc5-s0.001-t12-mecab'\n",
    "train_x_by_m1 = pickle.load(open(saveTrainPath+train_x_by_m1_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m1_name = 'test_x_Doc2Vec-dm-m-d2000-n7-w10-mc5-s0.001-t12-mecab'\n",
    "test_x_by_m1 = pickle.load(open(saveTrainPath+test_x_by_m1_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m1, train_y)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m1, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m1, train_y)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m1, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier2.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SVC(kernel = 'linear', gamma = 2)\n",
    "classifier3.fit(train_x_by_m1, train_y)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m1, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier3.predict(test_x_by_m1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m1_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m1, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m1))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m1))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np_utils.to_categorical(test_y,2)\n",
    "y_train = np_utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=2000))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=2000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [2000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    batch_size=200000,\n",
    "                    epochs=100,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m1\n",
    "del train_x_by_m1_name\n",
    "del test_x_by_m1\n",
    "del test_x_by_m1_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2\n",
    "* Doc2Vec(dm/c,d2000,n7,w5,mc5,s0.001,t12)\n",
    "* size-2000\n",
    "* epoch-20\n",
    "* window-5\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-1\n",
    "* dm_concat-1\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m2_name = 'train_x_Doc2Vec-dm-c-d2000-n7-w5-mc5-s0.001-t12-mecab'\n",
    "train_x_by_m2 = pickle.load(open(saveTrainPath+train_x_by_m2_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m2_name = 'test_x_Doc2Vec-dm-c-d2000-n7-w5-mc5-s0.001-t12-mecab'\n",
    "test_x_by_m2 = pickle.load(open(saveTrainPath+test_x_by_m2_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m2, train_y)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m2, train_y)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier2.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SVC(kernel = 'linear', gamma = 2)\n",
    "classifier3.fit(train_x_by_m2, train_y)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m2, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier3.predict(test_x_by_m2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m2_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m2, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m2))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m2))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np_utils.to_categorical(test_y,2)\n",
    "y_train = np_utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=2000))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_1_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=2000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_2_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.125))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_3_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [2000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    batch_size=200000,\n",
    "                    epochs=100,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m2\n",
    "del train_x_by_m2_name\n",
    "del test_x_by_m2\n",
    "del test_x_by_m2_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3\n",
    "* Doc2Vec(dbow,d2000,n7,mc5,s0.001,t12)\n",
    "* size-2000\n",
    "* epoch-20\n",
    "* window-None\n",
    "* negative-7\n",
    "* hs-0\n",
    "* dm-0\n",
    "* dm_concat-0\n",
    "* dm_mean-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_by_m3_name = 'train_x_Doc2Vec-dbow-d2000-n7-mc5-s0.001-t12-mecab'\n",
    "train_x_by_m3 = pickle.load(open(saveTrainPath+train_x_by_m3_name,'rb'))\n",
    "train_y = pickle.load(open(saveTrainPath+'train_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_by_m3_name = 'test_x_Doc2Vec-dbow-d2000-n7-mc5-s0.001-t12-mecab'\n",
    "test_x_by_m3 = pickle.load(open(saveTrainPath+test_x_by_m3_name,'rb'))\n",
    "test_y = pickle.load(open(saveTrainPath+'test_y_mecab_for_sentiment_analysis','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x_by_m3, train_y)\n",
    "print (classifier.get_params())\n",
    "print( 'score : {}'.format(classifier.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(saveClassifierPath+'LogisticRegression_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(random_state=1234)\n",
    "classifier2.fit(train_x_by_m3, train_y)\n",
    "print (classifier2.get_params())\n",
    "print( 'score : {}'.format(classifier2.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier2.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier2,open(saveClassifierPath+'RandomForestClassifier_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SVC(kernel = 'linear', gamma = 2)\n",
    "classifier3.fit(train_x_by_m3, train_y)\n",
    "print (classifier3.get_params())\n",
    "print( 'score : {}'.format(classifier3.score(test_x_by_m3, test_y)))\n",
    "print ('classification report')\n",
    "print (classification_report(test_y, classifier3.predict(test_x_by_m3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier3,open(saveClassifierPath+'SVC_'+train_x_by_m3_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make_Roc_Curve(test_x_by_m3, test_y, classifier, classifier2, classifier3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del classifier2\n",
    "del classifier3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류모델 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x,train_x_by_m3))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([z.reshape(1,-1) for z in tqdm(map(lambda x: x, test_x_by_m3))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "y_test = np_utils.to_categorical(test_y,2)\n",
    "y_train = np_utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=2000))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=200000, verbose=0,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=2000))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v, y_train,epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode2 = Sequential()\n",
    "mode2.add(Dense(64, activation='relu', input_dim=2000))\n",
    "mode2.add(Dropout(0.25))\n",
    "mode2.add(Dense(32, activation='relu'))\n",
    "mode2.add(Dropout(0.125))\n",
    "mode2.add(Dense(2, activation='softmax'))\n",
    "mode2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = mode2.fit(train_vecs_w2v, y_train,epochs=100, batch_size=200000, verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)] )\n",
    "# Evaluate model\n",
    "score, acc = mode2.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_2 = train_vecs_w2v.reshape(train_vecs_w2v.shape[0], train_vecs_w2v.shape[1], 1)\n",
    "test_vecs_w2v_2 = test_vecs_w2v.reshape(test_vecs_w2v.shape[0], test_vecs_w2v.shape[1], 1)\n",
    "\n",
    "print (train_vecs_w2v_2.shape, test_vecs_w2v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 5, kernel_size = 3,\n",
    "                 activation='relu', input_shape = [2000, 1]\n",
    "                ))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=5, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_vecs_w2v_2, y_train,\n",
    "                    batch_size=200000,\n",
    "                    epochs=100,  verbose=0, validation_split=0.2,\n",
    "          callbacks=[TQDMNotebookCallback(show_inner=False)])\n",
    "score, acc = model.evaluate(test_vecs_w2v_2, y_test, verbose=0)\n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveClassifierPath+'NeuralNetwork_4_'+train_x_by_m3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_by_m3\n",
    "del train_x_by_m3_name\n",
    "del test_x_by_m3\n",
    "del test_x_by_m3_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
