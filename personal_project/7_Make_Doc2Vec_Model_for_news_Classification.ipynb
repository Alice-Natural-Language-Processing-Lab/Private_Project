{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스기사 Classification Using Doc2Vec\n",
    "> * 네이버의 뉴스 기사를 이용하여 모델을 만들고 평가를 실시한뒤, 다으므이 뉴스 기사를 이용하여 분류해보도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import doc2vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9804077932699667221\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Roc_Curve(x, y, model1, model2, model3, model4):\n",
    "    print ('Logistic Regression')\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y, model1.predict(x))\n",
    "    print ('Random Forest')\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y, model2.predict(x))\n",
    "    print ('Kernel SVM')\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(y, model3.predict(x))\n",
    "    print ('XGBoost')\n",
    "    import xgboost as xgb\n",
    "    fpr4, tpr4, thresholds4 = roc_curve(y, model4.predict(xgb.DMatrix(x)))\n",
    "    plt.plot(fpr1, tpr1, label=\"Logistic Regression\")\n",
    "    plt.plot(fpr2, tpr2, label=\"RandomForest\")\n",
    "    plt.plot(fpr3, tpr3, label=\"Kernel SVM\")\n",
    "    plt.plot(fpr4, tpr4, label='XGBoost')\n",
    "    plt.legend()\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot model history after `fit()`.\n",
    "    \"\"\"\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naver : (15120, 11)\n",
      "Daum : (9372, 11)\n"
     ]
    }
   ],
   "source": [
    "#Naver\n",
    "naverData = pickle.load(open('./data/pre_data/stastics/for_statistics_Naver_from_mongodb.pickled','rb'))\n",
    "naverData = pd.DataFrame.from_dict(naverData, orient = 'index')\n",
    "naverData.reset_index(inplace = True)\n",
    "naverData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "#Daum\n",
    "daumData = pickle.load(open('./data/pre_data/stastics/for_statistics_daum_from_mongodb.pickled','rb'))\n",
    "daumData = pd.DataFrame.from_dict(daumData, orient = 'index')\n",
    "daumData.reset_index(inplace = True)\n",
    "daumData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "\n",
    "print ('Naver : {}'.format(naverData.shape))\n",
    "print ('Daum : {}'.format(daumData.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * words : 기사에서 나온 단어들 or keywords\n",
    "> * tags : 문서 tag\n",
    "> * classes : category\n",
    ">> 기사분류가 daum보다 naver에서 더 세분화되어 있기 때문에 네이버의 category 분류를 이용하기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IT/과학', '경제', '사회', '생활/문화', '세계', '스포츠', '연예', '정치'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(naverData['category'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "ct = Twitter()\n",
    "mecab = Mecab()\n",
    "def nav_tokenizer(tagger, corpus, stopwords):\n",
    "    pos = tagger.pos(corpus)\n",
    "    pos = ['/'.join(t) for t in pos if not t[0] in stopwords]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeTaggedData(df, taggedDoc, tagger, stopwords, labelEncoder):\n",
    "    w2v_docs = list()\n",
    "    for idx in tqdm(df.index):\n",
    "        text = df.loc[idx,'title']+'.\\n'+df.loc[idx,'mainText']\n",
    "        pos = nav_tokenizer(tagger, text, stopwords)\n",
    "        category = df.loc[idx, 'category']\n",
    "        encodeCategory = labelEncoder.transform([category])\n",
    "        label = ['news_'+str(idx)]\n",
    "        w2v_docs.append(TaggedDocument(pos, label, encodeCategory))\n",
    "    return w2v_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 345/15120 [00:39<28:28,  8.65it/s] "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_docs = MakeTaggedData(naverData, TaggedDocument, ct, stopwords, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(w2v_docs, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train,open('./data/pre_data/kc_train_test_Data/pre_data_doc2vec_train_for_keywords_classification_by_ct.pickled','wb'))\n",
    "pickle.dump(test,open('./data/pre_data/kc_train_test_Data/pre_data_doc2vec_test_for_keywords_classification_by_ct.pickled','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(alpha=0.025, min_alpha=0.025, iter=10)\n",
    "d2v_model.build_vocab(tqdm(train))\n",
    "for epoch in tqdm(range(10)):\n",
    "    d2v_model.train(tqdm(train),total_examples=d2v_model.corpus_count, epochs=d2v_model.iter)\n",
    "    d2v_model.alpha -= 0.002 \n",
    "    d2v_model.min_alpha = d2v_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.save('./model/keywords_classification_naver_by_doc2vec_size100_window5_iter10_by_mecab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec.load('./model/keywords_classification_naver_by_doc2vec_size100_window5_iter10_by_mecab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_doc2vec_train_for_keywords_classification_by_mecab.pickled','rb'))\n",
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_doc2vec_test_for_keywords_classification_by_mecab.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x.words for x in train]\n",
    "y_train = [x.tags for x in train]\n",
    "x_test = [x.words for x in test]\n",
    "y_test = [x.tags for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([d2v_model.infer_vector(z).reshape(1,-1) for z in tqdm(map(lambda x: x, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([d2v_model.infer_vector(z).reshape(1,-1) for z in tqdm(map(lambda x: x, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_test2 = np_utils.to_categorical(y_test,8)\n",
    "y_train2 = np_utils.to_categorical(y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense, Embedding, embeddings, merge, Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.compile(#optimizer='rmsprop',\n",
    "    optimizer='adadelta',\n",
    "              #loss='binary_crossentropy',\n",
    "    #optimizer=SGD(lr=0.2), \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train2, epochs=100, verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=True, leave_outer=True)])\n",
    "score = model.evaluate(test_vecs_w2v, y_test2, verbose=2)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, activation='relu', input_dim=100))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.125))\n",
    "model2.add(Dense(8, activation='softmax'))\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "model2.fit(train_vecs_w2v, y_train2,\n",
    "          epochs=100, verbose=0,\n",
    "          callbacks=[TQDMNotebookCallback(leave_inner=True, leave_outer=True)])\n",
    "score = model2.evaluate(test_vecs_w2v, y_test2)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDict2 = pickle.load(open('./data/pre_data/stastics/for_statistics_daum_from_mongodb.pickled','rb'))\n",
    "print (len(dataDict2))\n",
    "\n",
    "keywordsDict2 = pickle.load(open('./data/pre_data/keywords/keywords_daum.pickled','rb'))\n",
    "\n",
    "for idx in dataDict2:\n",
    "    dataDict2[idx]['extracted_keywords'] = keywordsDict2[idx]\n",
    "\n",
    "df2 = pd.DataFrame.from_dict(dataDict2, orient='index')\n",
    "df2['date'] = pd.to_datetime(df2['date']).dt.date\n",
    "df2.reset_index(inplace = True)\n",
    "df2.rename(columns={'index':'id'}, inplace=True)\n",
    "print (df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daum_w2v = np.concatenate([d2v_model.infer_vector(z).reshape(1,-1) for z in tqdm(map(lambda x: x, df2.title.values+'\\.n'+df2.mainText.values))])\n",
    "daum_w2v = scale(daum_w2v)\n",
    "\n",
    "model_pre = model.predict_classes(daum_w2v)\n",
    "model2_pre = model2.predict_classes(daum_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = le.inverse_transform(model_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp2 = le.inverse_transform(model2_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mp1==mp2).sum() / len(daum_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(d2v_model.wv.vocab), 100))\n",
    "for i in range(len(d2v_model.wv.vocab)):\n",
    "    embedding_vector = d2v_model.wv[d2v_model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 50  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "# input words - in this case we do sample by sample evaluations of the similarity\n",
    "valid_word = Input((1,), dtype='int32')\n",
    "other_word = Input((1,), dtype='int32')\n",
    "# setup the embedding layer\n",
    "embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "                      weights=[embedding_matrix])\n",
    "embedded_a = embeddings(valid_word)\n",
    "embedded_b = embeddings(other_word)\n",
    "similarity = merge([embedded_a, embedded_b], mode='cos', dot_axes=2)\n",
    "# create the Keras model\n",
    "k_model = Model(input=[valid_word, other_word], output=similarity)\n",
    "\n",
    "def get_sim(valid_word_idx, vocab_size):\n",
    "    sim = np.zeros((vocab_size,))\n",
    "    in_arr1 = np.zeros((1,))\n",
    "    in_arr2 = np.zeros((1,))\n",
    "    in_arr1[0,] = valid_word_idx\n",
    "    for i in range(vocab_size):\n",
    "        in_arr2[0,] = i\n",
    "        out = k_model.predict_on_batch([in_arr1, in_arr2])\n",
    "        sim[i] = out\n",
    "    return sim\n",
    "\n",
    "# now run the model and get the closest words to the valid examples\n",
    "for i in range(valid_size):\n",
    "    valid_word = d2v_model.wv.index2word[valid_examples[i]]\n",
    "    top_k = 8  # number of nearest neighbors\n",
    "    sim = get_sim(valid_examples[i], len(d2v_model.wv.vocab))\n",
    "    nearest = (-sim).argsort()[1:top_k + 1]\n",
    "    log_str = 'Nearest to %s:' % valid_word\n",
    "    for k in range(top_k):\n",
    "        close_word = d2v_model.wv.index2word[nearest[k]]\n",
    "        log_str = '%s %s,' % (log_str, close_word)\n",
    "    print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
