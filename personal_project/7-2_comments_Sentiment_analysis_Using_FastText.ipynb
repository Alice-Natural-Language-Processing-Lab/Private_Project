{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 뉴스 기사 및 댓글에 대한 감정 분석\n",
    "## * FastText\n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 2월 1일까지 63일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import FastText, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Database_Handler as dh\n",
    "import Basic_Module as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "ct = Twitter()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naver : (15120, 11)\n",
      "Daum : (9372, 11)\n"
     ]
    }
   ],
   "source": [
    "#Naver\n",
    "naverData = pickle.load(open('./data/pre_data/stastics/for_statistics_Naver_from_mongodb.pickled','rb'))\n",
    "naverData = pd.DataFrame.from_dict(naverData, orient = 'index')\n",
    "naverData.reset_index(inplace = True)\n",
    "naverData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "#Daum\n",
    "daumData = pickle.load(open('./data/pre_data/stastics/for_statistics_daum_from_mongodb.pickled','rb'))\n",
    "daumData = pd.DataFrame.from_dict(daumData, orient = 'index')\n",
    "daumData.reset_index(inplace = True)\n",
    "daumData.rename(columns = {'index' : 'id'}, inplace = True)\n",
    "\n",
    "print ('Naver : {}'.format(naverData.shape))\n",
    "print ('Daum : {}'.format(daumData.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extNaverData = naverData.loc[:, ['id', 'title', 'date', 'press', 'rank']].copy()\n",
    "extNaverData['site'] = pd.Series(['Naver']*extNaverData.shape[0])\n",
    "extDaumData = daumData.loc[:, ['id', 'title', 'date', 'press', 'rank']].copy()\n",
    "extDaumData['site'] = pd.Series(['daum']*extDaumData.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "    classifierPath = '/Volumes/disk1/data/pre_data/classifier/'\n",
    "    #newsPath = '/Volumes/data/pre_data/news_sentiment/'\n",
    "    newsPath = './data/pre_data/news_sentiment/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "    classifierPath = 'd:/data/pre_data/classifier/'\n",
    "    newsPath = './data/pre_data/news_sentiment/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaggedDocument For FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName = 'ct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1334417.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442359, 159010)\n",
      "vocab size : 159010\n"
     ]
    }
   ],
   "source": [
    "trainName = './data/pre_data/train_test_Data/pre_data_train_for_fastText_sentiment_by_ct.pickled'\n",
    "train = pickle.load(open(trainName, 'rb'))\n",
    "tfidf = bm.Build_tfidf(train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedDaumData = pickle.load(open('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedDaumData = bm.MakeTaggedDataDAUM2(daumData, TaggedDocument, ct, stopwords, 'daum')\n",
    "    pickle.dump(taggedDaumData, open('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_fastText_sentiment_analysis.pickled', 'wb'))\n",
    "\n",
    "    \n",
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedNaverData = pickle.load(open('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedNaverData = bm.MakeTaggedDataDAUM2(naverData, TaggedDocument, ct, stopwords, 'naver')\n",
    "    pickle.dump(taggedNaverData, open('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_fastText_sentiment_analysis.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model1\n",
      "Load Model2\n",
      "Load Model3\n"
     ]
    }
   ],
   "source": [
    "print ('Load Model1')\n",
    "model1 = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-ct.model')\n",
    "print ('Load Model2')\n",
    "model2 = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-ct.model')\n",
    "print ('Load Model3')\n",
    "model3 = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-ct.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model1,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_1\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:07<00:00, 20787.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:07.914689\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [16:48,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:16:56.839356\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623853.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 40 - ETA: 0 - ETA:  - ETA:  - 0s 33us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 44971.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 1: - ETA: 1s - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 57us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 176563.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:02<00:00, 70060.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:02.325316\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [11:31, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:11:35.294469\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 350855.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA: 2: - ETA: 3s - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 64us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629008.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA: 49 - ETA: 1 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 31us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629058.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model2,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:05<00:00, 27980.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:05.853044\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [05:33, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:05:39.259452\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623823.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 32 - ETA: 0 - ETA:  - ETA:  - 0s 29us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 222804.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 16 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 36us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 584877.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:01<00:00, 105873.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.539473\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [03:05, 81.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:03:07.277000\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 308099.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 19us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 603847.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 24us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 628790.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model3,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:02<00:00, 57702.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:02.822298\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:00, 39.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:03.101711\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 359929.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 26 - ETA: 0 - ETA:  - ETA:  - 0s 27us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623883.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 25 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 33us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623853.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:01<00:00, 114794.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.421138\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [03:25, 73.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:03:26.819750\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629076.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 18us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 628796.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 25us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 628790.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName = 'mecab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1482191.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442359, 162003)\n",
      "vocab size : 162003\n"
     ]
    }
   ],
   "source": [
    "trainName = './data/pre_data/train_test_Data/pre_data_train_for_fastText_sentiment_by_mecab.pickled'\n",
    "train = pickle.load(open(trainName, 'rb'))\n",
    "tfidf = bm.Build_tfidf(train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedDaumData = pickle.load(open('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedDaumData = bm.MakeTaggedDataDAUM2(daumData, TaggedDocument, mecab, stopwords, 'daum')\n",
    "    pickle.dump(taggedDaumData, open('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'wb'))\n",
    "\n",
    "    \n",
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedNaverData = pickle.load(open('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedNaverData = bm.MakeTaggedDataDAUM2(naverData, TaggedDocument, mecab, stopwords, 'naver')\n",
    "    pickle.dump(taggedNaverData, open('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model1\n",
      "Load Model2\n",
      "Load Model3\n"
     ]
    }
   ],
   "source": [
    "print ('Load Model1')\n",
    "model1 = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "print ('Load Model2')\n",
    "model2 = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-mecab.model')\n",
    "print ('Load Model3')\n",
    "model3 = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-mecab.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model1,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_1\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:05<00:00, 30233.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:05.661371\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [17:47,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:17:53.867761\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 343331.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 37 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 36us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 143966.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 41 - ETA: 3 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 54us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 119787.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:03<00:00, 51421.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:03.228807\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [07:09, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:07:13.156170\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629070.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 603657.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 35us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 126679.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model2,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:03<00:00, 46049.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:03.605978\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [06:48, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:06:51.856900\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 641078.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 22 - ETA: 0 - ETA:  - ETA:  - 0s 26us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623893.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 15 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 29us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 668486.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:01<00:00, 107998.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.539425\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [05:40, 44.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:05:42.461675\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 656425.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 656418.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 26us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629083.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model3,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:04<00:00, 39179.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:04.236385\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:50, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:54.642550\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 584885.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 22 - ETA: 0 - ETA:  - ETA:  - 0s 25us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 668520.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 23 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 33us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623883.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:01<00:00, 121210.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.372066\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [04:15, 59.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:17.519591\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629045.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 603657.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 25us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 503232.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
