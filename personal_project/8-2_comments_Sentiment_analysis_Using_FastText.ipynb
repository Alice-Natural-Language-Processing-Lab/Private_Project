{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 뉴스 기사 및 댓글에 대한 감정 분석\n",
    "## * FastText\n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 2월 1일까지 63일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunyoun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import FastText, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Database_Handler as dh\n",
    "import Basic_Module as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "ct = Twitter()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaggedDocument For FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "    classifierPath = '/Volumes/disk1/data/pre_data/classifier/'\n",
    "    news_senti_outcome = '/Volumes/disk1/outcome_for_News_sentiment_analysis/'\n",
    "    daumCommentsPath = '/Volumes/disk1/data/daum_Comments/'\n",
    "    naverCommentsPath = '/Volumes/disk1/data/naver_Comments/'\n",
    "    outcomeDaumCommentsPath = '/Volumes/disk1/outcome_comments_for_daum/'\n",
    "    outcomeNaverCommentsPath = '/Volumes/disk1/outcome_comments_for_naver/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "    classifierPath = 'd:/data/pre_data/classifier/'\n",
    "    newsPath = './data/pre_data/news_sentiment/'\n",
    "    news_senti_outcome = './outcome_for_News_sentiment_analysis/'\n",
    "    daumCommentsPath = 'd:/data/daum_Comments/'\n",
    "    naverCommentsPath = 'd:/data/naver_Comments/'\n",
    "    outcomeDaumCommentsPath = 'd:/outcome_comments_for_daum/'\n",
    "    outcomeNaverCommentsPath = 'd:/outcome_comments_for_naver/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naver_news_sentiment_analysis.csv', 'daum_news_sentiment_analysis.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(news_senti_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15015, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>press</th>\n",
       "      <th>number_of_comment</th>\n",
       "      <th>number_of_crawled_comment</th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>mainText</th>\n",
       "      <th>keywords</th>\n",
       "      <th>extracted_keywords</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>Decision</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5a29c445588c132954d1973a</th>\n",
       "      <td>정치</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>1713</td>\n",
       "      <td>1465</td>\n",
       "      <td>1</td>\n",
       "      <td>北외무성 \"전쟁 바라지 않지만 결코 피하지 않을 것\"</td>\n",
       "      <td>美고위인사 대북언급 비난하며 \"전쟁 기정사실화\" 위협  며칠 새 이어지는 북한 군민...</td>\n",
       "      <td>['외무성', '핵전쟁', '대변인']</td>\n",
       "      <td>{'미국', '조선반도', '핵전쟁', '북한', '중앙', '도화선', '고위',...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>Naver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a29c445588c132954d1973b</th>\n",
       "      <td>정치</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>한국일보</td>\n",
       "      <td>2551</td>\n",
       "      <td>2062</td>\n",
       "      <td>2</td>\n",
       "      <td>예산전쟁, 예결위 간사ㆍ호남이 웃었다</td>\n",
       "      <td>예결위 간사들이 최대 수혜자..당 지도부 내 몫 챙기기도 여전   황주홍ㆍ김도읍 등...</td>\n",
       "      <td>['예산', '예결위', 'soc']</td>\n",
       "      <td>{'호남', '예산안', '의원', '정부안', '증액', '지역구', '국민의당'}</td>\n",
       "      <td>46.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Naver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a29c445588c132954d1973c</th>\n",
       "      <td>정치</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>610</td>\n",
       "      <td>536</td>\n",
       "      <td>3</td>\n",
       "      <td>혐의 부인에 20시간 조사…檢, 최경환 구속 카드 꺼내나</td>\n",
       "      <td>【서울=뉴시스】 최진석 기자 = 박근혜 정부 시절 국가정보원 특수활동비 수수 의혹 ...</td>\n",
       "      <td>['최경환', '구속영장', '국가정보원']</td>\n",
       "      <td>{'구속영장 청구', '조사', '의원', '정기국회', '국정원장', '검찰', ...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>Naver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a29c445588c132954d1973d</th>\n",
       "      <td>정치</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>145</td>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>최재형 감사원장 후보자 \"독립성 강화는 임명권자의 뜻\"</td>\n",
       "      <td>감사원장에 내정된 최재형 사법연수원장(고양=연합뉴스) 이희열 기자 = 7일 감사원장...</td>\n",
       "      <td>['이슈 · 최재형 감사원장 내정', '감사원장', '최재형', '감사원']</td>\n",
       "      <td>{'공직 사회', '법관', '생활', '감사원장', '지명', '후보자'}</td>\n",
       "      <td>39.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Naver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a29c445588c132954d1973e</th>\n",
       "      <td>정치</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>동아일보</td>\n",
       "      <td>1074</td>\n",
       "      <td>932</td>\n",
       "      <td>5</td>\n",
       "      <td>B-1B 한반도에 뜨자, 평양 비운 김정은</td>\n",
       "      <td>[동아일보] 북중 접경지 양강도 삼지연 시찰… 방북 유엔 사무차장 면담 안할듯 B-...</td>\n",
       "      <td>['김정은', 'b-1b', '한반도']</td>\n",
       "      <td>{'공장', '양강도', '사무차장', '삼지연', '시찰', '훈련', '접경',...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>Naver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         category        date press  number_of_comment  \\\n",
       "id                                                                       \n",
       "5a29c445588c132954d1973a       정치  2017.12.07  연합뉴스               1713   \n",
       "5a29c445588c132954d1973b       정치  2017.12.07  한국일보               2551   \n",
       "5a29c445588c132954d1973c       정치  2017.12.07   뉴시스                610   \n",
       "5a29c445588c132954d1973d       정치  2017.12.07  연합뉴스                145   \n",
       "5a29c445588c132954d1973e       정치  2017.12.07  동아일보               1074   \n",
       "\n",
       "                          number_of_crawled_comment  rank  \\\n",
       "id                                                          \n",
       "5a29c445588c132954d1973a                       1465     1   \n",
       "5a29c445588c132954d1973b                       2062     2   \n",
       "5a29c445588c132954d1973c                        536     3   \n",
       "5a29c445588c132954d1973d                        133     4   \n",
       "5a29c445588c132954d1973e                        932     5   \n",
       "\n",
       "                                                    title  \\\n",
       "id                                                          \n",
       "5a29c445588c132954d1973a    北외무성 \"전쟁 바라지 않지만 결코 피하지 않을 것\"   \n",
       "5a29c445588c132954d1973b             예산전쟁, 예결위 간사ㆍ호남이 웃었다   \n",
       "5a29c445588c132954d1973c  혐의 부인에 20시간 조사…檢, 최경환 구속 카드 꺼내나   \n",
       "5a29c445588c132954d1973d   최재형 감사원장 후보자 \"독립성 강화는 임명권자의 뜻\"   \n",
       "5a29c445588c132954d1973e          B-1B 한반도에 뜨자, 평양 비운 김정은   \n",
       "\n",
       "                                                                   mainText  \\\n",
       "id                                                                            \n",
       "5a29c445588c132954d1973a  美고위인사 대북언급 비난하며 \"전쟁 기정사실화\" 위협  며칠 새 이어지는 북한 군민...   \n",
       "5a29c445588c132954d1973b  예결위 간사들이 최대 수혜자..당 지도부 내 몫 챙기기도 여전   황주홍ㆍ김도읍 등...   \n",
       "5a29c445588c132954d1973c  【서울=뉴시스】 최진석 기자 = 박근혜 정부 시절 국가정보원 특수활동비 수수 의혹 ...   \n",
       "5a29c445588c132954d1973d  감사원장에 내정된 최재형 사법연수원장(고양=연합뉴스) 이희열 기자 = 7일 감사원장...   \n",
       "5a29c445588c132954d1973e  [동아일보] 북중 접경지 양강도 삼지연 시찰… 방북 유엔 사무차장 면담 안할듯 B-...   \n",
       "\n",
       "                                                            keywords  \\\n",
       "id                                                                     \n",
       "5a29c445588c132954d1973a                       ['외무성', '핵전쟁', '대변인']   \n",
       "5a29c445588c132954d1973b                        ['예산', '예결위', 'soc']   \n",
       "5a29c445588c132954d1973c                    ['최경환', '구속영장', '국가정보원']   \n",
       "5a29c445588c132954d1973d  ['이슈 · 최재형 감사원장 내정', '감사원장', '최재형', '감사원']   \n",
       "5a29c445588c132954d1973e                      ['김정은', 'b-1b', '한반도']   \n",
       "\n",
       "                                                         extracted_keywords  \\\n",
       "id                                                                            \n",
       "5a29c445588c132954d1973a  {'미국', '조선반도', '핵전쟁', '북한', '중앙', '도화선', '고위',...   \n",
       "5a29c445588c132954d1973b    {'호남', '예산안', '의원', '정부안', '증액', '지역구', '국민의당'}   \n",
       "5a29c445588c132954d1973c  {'구속영장 청구', '조사', '의원', '정기국회', '국정원장', '검찰', ...   \n",
       "5a29c445588c132954d1973d         {'공직 사회', '법관', '생활', '감사원장', '지명', '후보자'}   \n",
       "5a29c445588c132954d1973e  {'공장', '양강도', '사무차장', '삼지연', '시찰', '훈련', '접경',...   \n",
       "\n",
       "                          negative  positive  Decision   site  \n",
       "id                                                             \n",
       "5a29c445588c132954d1973a      86.0      22.0  negative  Naver  \n",
       "5a29c445588c132954d1973b      46.0      62.0  positive  Naver  \n",
       "5a29c445588c132954d1973c      77.0      31.0  negative  Naver  \n",
       "5a29c445588c132954d1973d      39.0      69.0  positive  Naver  \n",
       "5a29c445588c132954d1973e      72.0      36.0  negative  Naver  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naver\n",
    "naverData = pd.read_csv(os.path.join(news_senti_outcome, 'naver_news_sentiment_analysis.csv'), index_col=0, header= 0, encoding = 'utf-8')\n",
    "naverData['site'] = ['Naver'] * naverData.shape[0]\n",
    "reNaverData = naverData[naverData.number_of_crawled_comment != 0]\n",
    "print (reNaverData.shape)\n",
    "reNaverData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9369, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>press</th>\n",
       "      <th>number_of_comment</th>\n",
       "      <th>number_of_crawled_comment</th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>mainText</th>\n",
       "      <th>keywords</th>\n",
       "      <th>extracted_keywords</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>Decision</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5a2a61bf588c13481c229d1e</th>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>1093</td>\n",
       "      <td>903</td>\n",
       "      <td>1</td>\n",
       "      <td>\"밤이 무섭다\"..비아그라 공장 연기에 남성들 부작용 호소</td>\n",
       "      <td>주민들은 공장에서 배출된 연기가 '남성이 매우 건강해지는 부작용'을 일으킨다며, ...</td>\n",
       "      <td>['부작용', '비아그라', '아일랜드']</td>\n",
       "      <td>{'건강', '부작용', '공장', '남성들', '지역', '세보 효과', '연기'}</td>\n",
       "      <td>50.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>daum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a2a61bf588c13481c229d1f</th>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>헬스조선</td>\n",
       "      <td>603</td>\n",
       "      <td>386</td>\n",
       "      <td>2</td>\n",
       "      <td>식후 커피·늦은 양치질..점심식사 후 하면 안 좋은 습관 3가지</td>\n",
       "      <td>점심식사를 마친 후 후식으로 커피를 마시는 사람들이 많다. 실제로 직장이 밀집돼 ...</td>\n",
       "      <td>['커피', '낮잠', '음식물']</td>\n",
       "      <td>{'건강', '커피', '자세', '치아', '철분', '디스크', '점심 식사',...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>daum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a2a61bf588c13481c229d20</th>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>1067</td>\n",
       "      <td>807</td>\n",
       "      <td>3</td>\n",
       "      <td>'십년지기 생매장' 진짜 이유는..\"'청부 통정' 알려질까 봐\"</td>\n",
       "      <td>(성남=연합뉴스) 최해민 기자 = 십년지기 지인을 산 채로 묻어 살해한 50대 여...</td>\n",
       "      <td>['살인혐의', '철원', '검찰송치']</td>\n",
       "      <td>{'앙심', '남편', '성관계', '주변', '진술', '지인', '범행', '아...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>daum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a2a61bf588c13481c229d21</th>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>헤럴드경제</td>\n",
       "      <td>418</td>\n",
       "      <td>368</td>\n",
       "      <td>4</td>\n",
       "      <td>신영자, 억 소리나는 갑질</td>\n",
       "      <td>신영자, 적용안된 혐의→검찰 상고에서 인정\\n신영자, 얼마를 어떻게 받았나  [헤럴...</td>\n",
       "      <td>['신영자', '갑질', '롯데백화점']</td>\n",
       "      <td>{'네이처리퍼블릭', '롯데', '징역', '유통업체', '신영자 이사장', '검찰...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>daum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5a2a61bf588c13481c229d22</th>\n",
       "      <td>뉴스</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>434</td>\n",
       "      <td>367</td>\n",
       "      <td>5</td>\n",
       "      <td>\"배신하지마\" 20대女 살인 피의자 유치장서 공범 남친에 쪽지</td>\n",
       "      <td>(청주=연합뉴스) 이승민 기자 = 지난 9월 청주의 한 하천에서 20대 여성을 둔기...</td>\n",
       "      <td>['공범', '살인', '과자']</td>\n",
       "      <td>{'폭행', '혐의', '과자', '범행', '쪽지', '남자친구', '경찰', '...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>daum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         category        date  press  number_of_comment  \\\n",
       "id                                                                        \n",
       "5a2a61bf588c13481c229d1e       뉴스  2017.12.07   세계일보               1093   \n",
       "5a2a61bf588c13481c229d1f       뉴스  2017.12.07   헬스조선                603   \n",
       "5a2a61bf588c13481c229d20       뉴스  2017.12.07   연합뉴스               1067   \n",
       "5a2a61bf588c13481c229d21       뉴스  2017.12.07  헤럴드경제                418   \n",
       "5a2a61bf588c13481c229d22       뉴스  2017.12.07   연합뉴스                434   \n",
       "\n",
       "                          number_of_crawled_comment  rank  \\\n",
       "id                                                          \n",
       "5a2a61bf588c13481c229d1e                        903     1   \n",
       "5a2a61bf588c13481c229d1f                        386     2   \n",
       "5a2a61bf588c13481c229d20                        807     3   \n",
       "5a2a61bf588c13481c229d21                        368     4   \n",
       "5a2a61bf588c13481c229d22                        367     5   \n",
       "\n",
       "                                                        title  \\\n",
       "id                                                              \n",
       "5a2a61bf588c13481c229d1e     \"밤이 무섭다\"..비아그라 공장 연기에 남성들 부작용 호소   \n",
       "5a2a61bf588c13481c229d1f  식후 커피·늦은 양치질..점심식사 후 하면 안 좋은 습관 3가지   \n",
       "5a2a61bf588c13481c229d20  '십년지기 생매장' 진짜 이유는..\"'청부 통정' 알려질까 봐\"   \n",
       "5a2a61bf588c13481c229d21                       신영자, 억 소리나는 갑질   \n",
       "5a2a61bf588c13481c229d22   \"배신하지마\" 20대女 살인 피의자 유치장서 공범 남친에 쪽지   \n",
       "\n",
       "                                                                   mainText  \\\n",
       "id                                                                            \n",
       "5a2a61bf588c13481c229d1e   주민들은 공장에서 배출된 연기가 '남성이 매우 건강해지는 부작용'을 일으킨다며, ...   \n",
       "5a2a61bf588c13481c229d1f   점심식사를 마친 후 후식으로 커피를 마시는 사람들이 많다. 실제로 직장이 밀집돼 ...   \n",
       "5a2a61bf588c13481c229d20   (성남=연합뉴스) 최해민 기자 = 십년지기 지인을 산 채로 묻어 살해한 50대 여...   \n",
       "5a2a61bf588c13481c229d21  신영자, 적용안된 혐의→검찰 상고에서 인정\\n신영자, 얼마를 어떻게 받았나  [헤럴...   \n",
       "5a2a61bf588c13481c229d22  (청주=연합뉴스) 이승민 기자 = 지난 9월 청주의 한 하천에서 20대 여성을 둔기...   \n",
       "\n",
       "                                         keywords  \\\n",
       "id                                                  \n",
       "5a2a61bf588c13481c229d1e  ['부작용', '비아그라', '아일랜드']   \n",
       "5a2a61bf588c13481c229d1f      ['커피', '낮잠', '음식물']   \n",
       "5a2a61bf588c13481c229d20   ['살인혐의', '철원', '검찰송치']   \n",
       "5a2a61bf588c13481c229d21   ['신영자', '갑질', '롯데백화점']   \n",
       "5a2a61bf588c13481c229d22       ['공범', '살인', '과자']   \n",
       "\n",
       "                                                         extracted_keywords  \\\n",
       "id                                                                            \n",
       "5a2a61bf588c13481c229d1e    {'건강', '부작용', '공장', '남성들', '지역', '세보 효과', '연기'}   \n",
       "5a2a61bf588c13481c229d1f  {'건강', '커피', '자세', '치아', '철분', '디스크', '점심 식사',...   \n",
       "5a2a61bf588c13481c229d20  {'앙심', '남편', '성관계', '주변', '진술', '지인', '범행', '아...   \n",
       "5a2a61bf588c13481c229d21  {'네이처리퍼블릭', '롯데', '징역', '유통업체', '신영자 이사장', '검찰...   \n",
       "5a2a61bf588c13481c229d22  {'폭행', '혐의', '과자', '범행', '쪽지', '남자친구', '경찰', '...   \n",
       "\n",
       "                          negative  positive  Decision  site  \n",
       "id                                                            \n",
       "5a2a61bf588c13481c229d1e      50.0      58.0  positive  daum  \n",
       "5a2a61bf588c13481c229d1f      81.0      27.0  negative  daum  \n",
       "5a2a61bf588c13481c229d20      99.0       9.0  negative  daum  \n",
       "5a2a61bf588c13481c229d21      77.0      31.0  negative  daum  \n",
       "5a2a61bf588c13481c229d22     102.0       6.0  negative  daum  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daum\n",
    "daumData = pd.read_csv(os.path.join(news_senti_outcome, 'daum_news_sentiment_analysis.csv'), index_col=0, header= 0, encoding = 'utf-8')\n",
    "daumData['site'] = ['daum'] * daumData.shape[0]\n",
    "reDaumData = daumData[daumData.number_of_crawled_comment != 0]\n",
    "print (reDaumData.shape)\n",
    "reDaumData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 댓글\n",
    "* 예시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>date</th>\n",
       "      <th>rank</th>\n",
       "      <th>site</th>\n",
       "      <th>공감</th>\n",
       "      <th>비공감</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a29c449588c132954d19ba1</td>\n",
       "      <td>정치</td>\n",
       "      <td>6.25때와는 확실히 틀릴것이다.</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>1</td>\n",
       "      <td>Naver</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a29c449588c132954d19daa</td>\n",
       "      <td>정치</td>\n",
       "      <td>60대 알바들 오늘도 수고가 많다. 근데 니들 이제 조사들어가겠더라? 업보다 업보야...</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>1</td>\n",
       "      <td>Naver</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a29c448588c132954d1988a</td>\n",
       "      <td>정치</td>\n",
       "      <td>625때 중국에 핵폭탄만 투하했다면 통일 됐을건데, 트루먼이 말려서 통일도 못하고 ...</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>1</td>\n",
       "      <td>Naver</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a29c448588c132954d199b6</td>\n",
       "      <td>정치</td>\n",
       "      <td>625때에도 인천상륙작전실시하기 전에 서해안 여러곳을 가짜로 상륙하려고 시도를 했었...</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>1</td>\n",
       "      <td>Naver</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a29c449588c132954d19ac7</td>\n",
       "      <td>정치</td>\n",
       "      <td>9.11테러때 납작 엎드려서 부지한 목숨이 아깝지 않은가보네 ㅋㅋㅋㅋㅋ</td>\n",
       "      <td>2017.12.07</td>\n",
       "      <td>1</td>\n",
       "      <td>Naver</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id category  \\\n",
       "0  5a29c449588c132954d19ba1       정치   \n",
       "1  5a29c449588c132954d19daa       정치   \n",
       "2  5a29c448588c132954d1988a       정치   \n",
       "3  5a29c448588c132954d199b6       정치   \n",
       "4  5a29c449588c132954d19ac7       정치   \n",
       "\n",
       "                                            comments        date  rank   site  \\\n",
       "0                                 6.25때와는 확실히 틀릴것이다.  2017.12.07     1  Naver   \n",
       "1  60대 알바들 오늘도 수고가 많다. 근데 니들 이제 조사들어가겠더라? 업보다 업보야...  2017.12.07     1  Naver   \n",
       "2  625때 중국에 핵폭탄만 투하했다면 통일 됐을건데, 트루먼이 말려서 통일도 못하고 ...  2017.12.07     1  Naver   \n",
       "3  625때에도 인천상륙작전실시하기 전에 서해안 여러곳을 가짜로 상륙하려고 시도를 했었...  2017.12.07     1  Naver   \n",
       "4            9.11테러때 납작 엎드려서 부지한 목숨이 아깝지 않은가보네 ㅋㅋㅋㅋㅋ  2017.12.07     1  Naver   \n",
       "\n",
       "   공감  비공감  \n",
       "0   1    0  \n",
       "1   1    4  \n",
       "2   7    0  \n",
       "3   3    0  \n",
       "4   2    0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(naverCommentsPath, '5a29c445588c132954d1973a'+'.csv'), encoding='utf-8', index_col=None, header = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter & Mecab\n",
    "> * 메모리, 시간상의 문제점\n",
    ">> * 메모리는 dataframe apply로 하면 kernel이 죽어버림.\n",
    ">> * 시간 for 문으로 할 수 있는건 한번에 해버리는게 낫겠지? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1334417.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442359, 159010)\n",
      "vocab size : 159010\n"
     ]
    }
   ],
   "source": [
    "trainName_ct = './data/pre_data/train_test_Data/pre_data_train_for_fastText_sentiment_by_ct.pickled'\n",
    "train_ct = pickle.load(open(trainName_ct, 'rb'))\n",
    "tfidf_ct = bm.Build_tfidf(train_ct)\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1482191.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442359, 162003)\n",
      "vocab size : 162003\n"
     ]
    }
   ],
   "source": [
    "trainName_mecab = './data/pre_data/train_test_Data/pre_data_train_for_fastText_sentiment_by_mecab.pickled'\n",
    "train_mecab = pickle.load(open(trainName_mecab, 'rb'))\n",
    "tfidf_mecab = bm.Build_tfidf(train_mecab)\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model1\n",
      "Load Model2\n",
      "Load Model3\n"
     ]
    }
   ],
   "source": [
    "taggerName_ct = 'ct'\n",
    "print ( '{}'.format(taggerName_ct))\n",
    "model1_ct = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-ct.model')\n",
    "model1_ct_Name = bm.Return_ModelName('fastText', model1_ct,'ct')\n",
    "print ('Load Model')\n",
    "model2_ct = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-ct.model')\n",
    "model2_ct_Name = bm.Return_ModelName('fastText', model2_ct,'ct')\n",
    "print ('Load Model')\n",
    "model3_ct = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-ct.model')\n",
    "model3_ct_Name = bm.Return_ModelName('fastText', model3_ct,'ct')\n",
    "print ('Load Model')\n",
    "\n",
    "taggerName_mecab = 'mecab'\n",
    "print ( '{}'.format(taggerName_mecab))\n",
    "model1_mecab = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "model1_mecab_Name = bm.Return_ModelName('fastText', model1_mecab,'mecab')\n",
    "print ('Load Model')\n",
    "model2_mecab = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-mecab.model')\n",
    "model2_mecab_Name = bm.Return_ModelName('fastText', model2_mecab,'mecab')\n",
    "print ('Load Model')\n",
    "model3_mecab = FastText.load(loadModelPath+'fastText_size-1000_epoch-20_ngrams-3_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "model3_mecab_Name = bm.Return_ModelName('fastText', model3_mecab,'mecab')\n",
    "print ('Load Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in reNaverData.index:\n",
    "    commentFile = os.path.join(naverCommentsPath, idx+'.csv')\n",
    "    if os.path.isfile(commentFile):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedDaumData = pickle.load(open('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedDaumData = bm.MakeTaggedDataDAUM2(daumData, TaggedDocument, ct, stopwords, 'daum')\n",
    "    pickle.dump(taggedDaumData, open('./data/pre_data/tagged_data/pre_data_daum_news_by_ct_for_fastText_sentiment_analysis.pickled', 'wb'))\n",
    "\n",
    "    \n",
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedNaverData = pickle.load(open('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedNaverData = bm.MakeTaggedDataDAUM2(naverData, TaggedDocument, ct, stopwords, 'naver')\n",
    "    pickle.dump(taggedNaverData, open('./data/pre_data/tagged_data/pre_data_naver_news_by_ct_for_fastText_sentiment_analysis.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model1,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_1\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:07<00:00, 20787.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:07.914689\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [16:48,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:16:56.839356\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623853.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 40 - ETA: 0 - ETA:  - ETA:  - 0s 33us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 44971.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 1: - ETA: 1s - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 57us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 176563.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:02<00:00, 70060.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:02.325316\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [11:31, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:11:35.294469\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 350855.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA: 2: - ETA: 3s - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 64us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629008.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA: 49 - ETA: 1 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 31us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629058.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model2,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:05<00:00, 27980.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:05.853044\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [05:33, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:05:39.259452\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623823.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 32 - ETA: 0 - ETA:  - ETA:  - 0s 29us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 222804.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 16 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 36us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 584877.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:01<00:00, 105873.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.539473\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [03:05, 81.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:03:07.277000\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 308099.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 19us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 603847.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 24us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 628790.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model3,'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:02<00:00, 57702.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:02.822298\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:00, 39.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:03.101711\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 359929.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 26 - ETA: 0 - ETA:  - ETA:  - 0s 27us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623883.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 25 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 33us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623853.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=162564, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162564/162564 [00:01<00:00, 114794.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.421138\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [03:25, 73.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:03:26.819750\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629076.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 18us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 628796.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 25us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 628790.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedDaumData = pickle.load(open('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedDaumData = bm.MakeTaggedDataDAUM2(daumData, TaggedDocument, mecab, stopwords, 'daum')\n",
    "    pickle.dump(taggedDaumData, open('./data/pre_data/tagged_data/pre_data_daum_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'wb'))\n",
    "\n",
    "    \n",
    "if os.path.isfile('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_fastText_sentiment_analysis.pickled'):\n",
    "    taggedNaverData = pickle.load(open('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'rb'))\n",
    "else:\n",
    "    taggedNaverData = bm.MakeTaggedDataDAUM2(naverData, TaggedDocument, mecab, stopwords, 'naver')\n",
    "    pickle.dump(taggedNaverData, open('./data/pre_data/tagged_data/pre_data_naver_news_by_mecab_for_fastText_sentiment_analysis.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model1,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_1\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:05<00:00, 30233.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:05.661371\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [17:47,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:17:53.867761\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 343331.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 37 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 36us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 143966.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 41 - ETA: 3 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 54us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 119787.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:03<00:00, 51421.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:03.228807\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [07:09, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:07:13.156170\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model1, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629070.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 603657.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 35us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 126679.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model2,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:03<00:00, 46049.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:03.605978\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [06:48, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:06:51.856900\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 641078.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 22 - ETA: 0 - ETA:  - ETA:  - 0s 26us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623893.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 15 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 29us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 668486.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:01<00:00, 107998.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.539425\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [05:40, 44.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:05:42.461675\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model2, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 656425.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 656418.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 26us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629083.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = bm.Return_ModelName('fastText', model3,'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierList = glob(classifierPath+'*'+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "NeuralNetwork_1\n",
      "NeuralNetwork_2\n",
      "RandomForestClassifier\n",
      "SVC\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "loadClassifierDict = dict(map(lambda x:bm.LoadClassifier(x), classifierList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:04<00:00, 39179.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:04.236385\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [04:50, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:54.642550\n"
     ]
    }
   ],
   "source": [
    "wv1, daum_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedDaumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 584885.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 22 - ETA: 0 - ETA:  - ETA:  - 0s 25us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 668520.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372/9372 [==============================] - ETA: 23 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 33us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9372it [00:00, 623883.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_daum = pd.DataFrame.from_dict(predictOutcome_daum)\n",
    "predictOutcome_daum = extDaumData.merge(predictOutcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_daum.to_csv('./outcome/outcome_news_sentiment_analysis_daum_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=165823, size=1000, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165823/165823 [00:01<00:00, 121210.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time : 0:00:01.372066\n",
      "Vectorizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [04:15, 59.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling Data\n",
      "total running time : 0:04:17.519591\n"
     ]
    }
   ],
   "source": [
    "wv1, naver_vecs_w2v = bm.Make_Pre_Data_For_DAUM(model3, tfidf, 1000, taggedNaverData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 629045.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 603657.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120/15120 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 25us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15120it [00:00, 503232.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predictOutcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_w2v, x, loadClassifierDict[x]), loadClassifierDict))\n",
    "predictOutcome_naver = pd.DataFrame.from_dict(predictOutcome_naver)\n",
    "predictOutcome_naver = extNaverData.merge(predictOutcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predictOutcome_naver.to_csv('./outcome/outcome_news_sentiment_analysis_naver_'+modelName,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelName\n",
    "del predictOutcome_daum\n",
    "del predictOutcome_naver\n",
    "del daum_vecs_w2v\n",
    "del naver_vecs_w2v\n",
    "del wv1\n",
    "del loadClassifierDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
