{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 데이터에 대한 통계 분석\n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 1월 1일까지 32일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import sys\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import html\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import functools\n",
    "from konlpy.tag import Mecab\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    binList = []\n",
    "    collection = 'newsDaum'\n",
    "elif site.lower() == 'naver':\n",
    "    binList = []\n",
    "    collection = 'newsNaver'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "useCollection = dh.Use_Collection(useDb, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "def tokenize_pos2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None, encoding='utf-8')\n",
    "rawdata.head()\n",
    "X = list(rawdata[0])\n",
    "y = np.array(list(rawdata[1]), dtype=int)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mecab_log1 = Pipeline([\n",
    "            ('vect', TfidfVectorizer(tokenizer=tokenize_pos2, ngram_range=(1,2))), \n",
    "            ('log', LogisticRegression()),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model_mecab_log1.fit(train_X, train_y)\n",
    "print(classification_report(test_y, model_mecab_log1.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_mecab_log1, open('./data/pre_data/classifier_log_classifier_ngram12_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mecab_log1 = pickle.load(open('./data/pre_data/classifier_log_classifier_ngram12_by_mecab.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load('./model/doc2vec_size2000_epoch20_by_mecab.model')\n",
    "classifier = pickle.load(open('./data/pre_data/classifier_by_mecab_from_doc2vec_size2000_epoch20.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주의 \n",
    "> gensim의 doc2vec 모델의 infer_vector 함수를 이용해 벡터를 생성하면, 같은 단어열(문서)에 대해서도 여러번 호출하면 각기 다른 벡터가 나온다  \n",
    "> 이는 문서 벡터 추론 과정에서 랜덤하게 초기화하는 과정이 있기 때문인데, 문제는 결과로 나오는 벡터의 차이가 꽤 크다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cls(text, model, num):\n",
    "    mecab = Mecab()\n",
    "    def tokenize_pos2(doc):\n",
    "        return ['/'.join(t) for t in mecab.pos(doc)]\n",
    "    TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "    x2 = TaggedDocument(tokenize_pos2(text), 0.5)\n",
    "    out = list()\n",
    "    for idx in range(num):\n",
    "        x = model.infer_vector(x2.words)\n",
    "        cl = classifier.predict(x.reshape(1,-1))\n",
    "        out.append(cl)\n",
    "    out = list(itertools.chain.from_iterable(out))\n",
    "    cls2 = int(round(sum(out)/len(out)))\n",
    "    return cls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsDict = pickle.load(open('./data/pre_data/keywords_daum.pickled','rb'))\n",
    "#keywordsDict = pickle.load(open('./data/pre_data/keywords_Naver.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김국진·노홍철·한혜진 '한명회', 오늘(5일) 9회만에 종영 0\n",
      "{'한명회', '특집', '이름', '녹화', '출연자', '방송'}\n",
      "댓글수 : 120, 댓글 판단 : 0.53\n",
      "'몸 만드는 게 우선' 류중일 감독, 캠프 명단에 신인 없다 0\n",
      "{'신인', '류중일 감독', '캠프 명단', '오키나와 캠프', '선수들'}\n",
      "댓글수 : 27, 댓글 판단 : 0.70\n",
      "[어제TV]첫방 '저글러스' 男보스 女비서, 시대착오적 설정 '불편' 0\n",
      "{'아내', '비서', '보스', '불륜', '설정', '무릎', '수습', '봉상무', '좌윤이'}\n",
      "댓글수 : 125, 댓글 판단 : 0.50\n",
      "[Oh! 무비] '기억의 밤' 장항준 감독은 어떻게 골리앗을 무찔렀나 1\n",
      "{'장항준 감독', '시나리오', '기억의', '드라마', '영화', '스릴러'}\n",
      "댓글수 : 39, 댓글 판단 : 0.59\n",
      "[RE:TV]'섬총사' 강호동X정상훈, 80cm 대어 낚아 '낚시TV 따로없네' 0\n",
      "{'농어', '강호동과 정상훈', '어청도', '총사', '낚시', '조세호'}\n",
      "댓글수 : 23, 댓글 판단 : 0.74\n",
      "'범죄도시'는 왜 불법 다운과의 전쟁을 선포했나 0\n",
      "{'유포 자들', '처벌', '행위', '범죄 도시', '불법 유포', '영화'}\n",
      "댓글수 : 81, 댓글 판단 : 0.58\n",
      "'다둥이 아빠' 박지헌, 여섯째 출산 앞둔 아내에 \"정말 고마워\" 애틋 0\n",
      "{'아내', '인스타그램', '사랑', '여섯째 출산', '박지헌'}\n",
      "댓글수 : 609, 댓글 판단 : 0.60\n",
      "[SC이슈] \"무서운 팬덤 화력\"..강다니엘,  뉴욕 타임스퀘어 전광판 장식 1\n",
      "{'타임스퀘어 전광판', '팬들', '뉴욕 타임스퀘어', '아이돌', '광고', '강다니엘'}\n",
      "댓글수 : 87, 댓글 판단 : 0.80\n",
      "반찬 먹는 순서만 바꿔도 살이 빠지고 당뇨병이 예방된다? 0\n",
      "{'식이섬유', '생선', '순서', '혈당', '단백질 탄수화물', '먹는', '식사', '육류'}\n",
      "댓글수 : 484, 댓글 판단 : 0.54\n",
      "\"지금에 감사하자♡\"..김가연-임요환, 딸 부자 가족사진 1\n",
      "{'사진', '김가연과 임요환 부부'}\n",
      "댓글수 : 63, 댓글 판단 : 0.68\n",
      "이창명 \"검찰 상고? 가족들 고통에 죽고싶다\" 심경 0\n",
      "{'가족', '상고', '이창명 항소심', '연예 한밤', '검찰'}\n",
      "댓글수 : 823, 댓글 판단 : 0.52\n",
      "[단독] '윤식당2' 윤여정X이서진X정유미X박서준, 오늘(5일) 동반귀국 1\n",
      "{'정유미', '박서준', '촬영', '제작진', '윤식당2', '스페인', '프로그램', '리포트', '배우'}\n",
      "댓글수 : 58, 댓글 판단 : 0.69\n",
      "가희 \"둘째 건강히 잘 크고 있다, 응원과 격려 부탁\" (공식) 1\n",
      "{'둘째 임신 소식', '가희', '세상', '격려', '감사', '건강히', '응원'}\n",
      "댓글수 : 113, 댓글 판단 : 0.65\n",
      "[스한초점] \"온유 상품 안 사요\"..샤이니 팬들은 왜 등을 돌렸나 0\n",
      "{'그리팅', '성추행', '운동', '사과문', '온유', 'SM', '논란', '샤이니'}\n",
      "댓글수 : 85, 댓글 판단 : 0.67\n",
      "'비행소녀' 김지민, 서운함 폭발한 母 돌직구 \"데이트 할 사람 없냐\" 0\n",
      "{'웃음', '어머니', '오빠', '비행 소녀', '김지민 엄마', '돌직구'}\n",
      "댓글수 : 33, 댓글 판단 : 0.42\n",
      "'코빅' 측 \"정인영 결혼? 개인 사생활..하차는 없다\" [공식입장] 1\n",
      "{'하차', '코미디 빅리그', '개인', '정인영의 결혼'}\n",
      "댓글수 : 54, 댓글 판단 : 0.56\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "collection = useCollection.find({'site':site})\n",
    "for data in collection:\n",
    "    idis = data['_id']._ObjectId__id.hex()\n",
    "    \n",
    "    text = data['title']+'. '+data['mainText']\n",
    "    print (data['title'], Cls(text, model, 50))\n",
    "    commentCollection = dh.Use_Collection(useDb, 'comments')\n",
    "    comments = commentCollection.find({'site':site, 'category':data['category'], 'date':data['date'], 'rank':data['rank'] })\n",
    "    commEstimation = list()\n",
    "    for comment in comments:\n",
    "        comm = comment['comments']\n",
    "        commEstimation.append(Cls(comm, model, 30))\n",
    "    cls = sum(commEstimation)/ len(commEstimation)\n",
    "    print (keywordsDict[idis])\n",
    "    print ('댓글수 : {}, 댓글 판단 : {:0.2f}'.format(comments.count(), cls))\n",
    "    if i==15:break\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
