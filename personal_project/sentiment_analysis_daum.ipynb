{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 데이터에 대한 통계 분석\n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 1월 1일까지 32일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import sys\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import html\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import functools\n",
    "from konlpy.tag import Mecab\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    binList = []\n",
    "    collection = 'newsDaum'\n",
    "elif site.lower() == 'naver':\n",
    "    binList = []\n",
    "    collection = 'newsNaver'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "useCollection = dh.Use_Collection(useDb, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "def tokenize_pos2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None, encoding='utf-8')\n",
    "rawdata.head()\n",
    "X = list(rawdata[0])\n",
    "y = np.array(list(rawdata[1]), dtype=int)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mecab_log1 = Pipeline([\n",
    "            ('vect', TfidfVectorizer(tokenizer=tokenize_pos2, ngram_range=(1,2))), \n",
    "            ('log', LogisticRegression()),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model_mecab_log1.fit(train_X, train_y)\n",
    "print(classification_report(test_y, model_mecab_log1.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_mecab_log1, open('./data/pre_data/classifier_log_classifier_ngram12_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mecab_log1 = pickle.load(open('./data/pre_data/classifier_log_classifier_ngram12_by_mecab.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load('./model/doc2vec_size2000_epoch20_by_mecab.model')\n",
    "classifier = pickle.load(open('./data/pre_data/classifier_by_mecab_from_doc2vec_size2000_epoch20.pickled','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주의 \n",
    "> gensim의 doc2vec 모델의 infer_vector 함수를 이용해 벡터를 생성하면, 같은 단어열(문서)에 대해서도 여러번 호출하면 각기 다른 벡터가 나온다  \n",
    "> 이는 문서 벡터 추론 과정에서 랜덤하게 초기화하는 과정이 있기 때문인데, 문제는 결과로 나오는 벡터의 차이가 꽤 크다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cls(text, model, num):\n",
    "    mecab = Mecab()\n",
    "    def tokenize_pos2(doc):\n",
    "        return ['/'.join(t) for t in mecab.pos(doc)]\n",
    "    TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "    x2 = TaggedDocument(tokenize_pos2(text), 0.5)\n",
    "    out = list()\n",
    "    for idx in range(num):\n",
    "        x = model.infer_vector(x2.words)\n",
    "        cl = classifier.predict(x.reshape(1,-1))\n",
    "        out.append(cl)\n",
    "    out = list(itertools.chain.from_iterable(out))\n",
    "    cls2 = int(round(sum(out)/len(out)))\n",
    "    return cls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsDict = pickle.load(open('./data/pre_data/keywords_daum.pickled','rb'))\n",
    "#keywordsDict = pickle.load(open('./data/pre_data/keywords_Naver.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "outputDict  =dict()\n",
    "collection = useCollection.find({'site':site})\n",
    "for data in collection:\n",
    "    idis = data['_id']._ObjectId__id.hex()\n",
    "    text = data['title']+'. '+data['mainText']    \n",
    "    commentCollection = dh.Use_Collection(useDb, 'comments')\n",
    "    comments = commentCollection.find({'site':site, 'category':data['category'], 'date':data['date'], 'rank':data['rank'] })\n",
    "    commEstimation = list()\n",
    "    for comment in comments:\n",
    "        comm = comment['comments']\n",
    "        commEstimation.append(Cls(comm, model, 30))\n",
    "    cls = sum(commEstimation)/ len(commEstimation)    \n",
    "    outputDict[idis] = tmpdict = dict()\n",
    "    tmpdict['title'] = data['title']\n",
    "    tmpdict['maintextestimation'] = Cls(text, model, 50)\n",
    "    tmpdict['keywordsindaum'] = data['keywords']\n",
    "    tmpdict['analyzedkeywords'] = keywordsDict[idis]\n",
    "    tmpdict['number_crawledcomments'] = comments.count()\n",
    "    tmpdict['commentsestimation'] = cls\n",
    "    tmpdict['press'] = data['press']\n",
    "    tmpdict['number_of_comment'] = data['number_of_comment']\n",
    "pickle.dump(outputDict, open('./outcome/outcome_daum_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
