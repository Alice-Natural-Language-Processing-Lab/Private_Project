{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 뉴스 기사 및 댓글에 대한 감정 분석\n",
    "## * Word2Vec\n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 2월 1일까지 63일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import html\n",
    "import multiprocessing\n",
    "from collections import namedtuple, OrderedDict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "from numba import jit\n",
    "import warnings\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,  accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, embeddings, merge, Dropout, Activation,  LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers.merge import dot\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Database_Handler as dh\n",
    "import Basic_Module as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "ct = Twitter()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform =='darwin':\n",
    "    loadModelPath = '/Volumes/disk1/model/'\n",
    "    classifierPath = '/Volumes/disk1/data/pre_data/classifier/'\n",
    "    news_senti_outcome = '/Volumes/disk1/outcome_for_News_sentiment_analysis/'\n",
    "    daumCommentsPath = '/Volumes/disk1/data/daum_Comments/'\n",
    "    naverCommentsPath = '/Volumes/disk1/data/naver_Comments/'\n",
    "    outcomeDaumCommentsPath = '/Volumes/disk1/outcome_comments_for_daum/'\n",
    "    outcomeNaverCommentsPath = '/Volumes/disk1/outcome_comments_for_naver/'\n",
    "    outcome_predata = '/Volumes/disk1/pre_data_for_comments/'\n",
    "    outcome_tagged_data = '/Volumes/disk1/pre_data_for_comments2/'\n",
    "    outcome_vectorized_data = '/Volumes/disk1/pre_data_for_comments3/'\n",
    "elif sys.platform =='win32':\n",
    "    loadModelPath = 'd:/model/'\n",
    "    classifierPath = 'd:/data/pre_data/classifier/'\n",
    "    newsPath = './data/pre_data/news_sentiment/'\n",
    "    news_senti_outcome = './outcome_for_News_sentiment_analysis/'\n",
    "    daumCommentsPath = 'd:/data/daum_Comments/'\n",
    "    naverCommentsPath = 'd:/data/naver_Comments/'\n",
    "    outcomeDaumCommentsPath = 'd:/outcome_comments_for_daum/'\n",
    "    outcomeNaverCommentsPath = 'd:/outcome_comments_for_naver/'\n",
    "    outcome_predata = 'd:/pre_data_for_comments/'\n",
    "    outcome_tagged_data = 'd:/pre_data_for_comments2/'\n",
    "    outcome_vectorized_data = 'd:/pre_data_for_comments3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(news_senti_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naver\n",
    "naverData = pd.read_csv(os.path.join(news_senti_outcome, 'naver_news_sentiment_analysis.csv'), index_col=0, header= 0, encoding = 'utf-8')\n",
    "naverData['site'] = ['Naver'] * naverData.shape[0]\n",
    "reNaverData = naverData[naverData.number_of_crawled_comment != 0]\n",
    "print (reNaverData.shape)\n",
    "reNaverData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daum\n",
    "daumData = pd.read_csv(os.path.join(news_senti_outcome, 'daum_news_sentiment_analysis.csv'), index_col=0, header= 0, encoding = 'utf-8')\n",
    "daumData['site'] = ['daum'] * daumData.shape[0]\n",
    "reDaumData = daumData[daumData.number_of_crawled_comment != 0]\n",
    "print (reDaumData.shape)\n",
    "reDaumData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 댓글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(outcome_predata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predata_naver = outcome_predata +  'filtered_predata_for_naver_news_comment.csv'\n",
    "dfNaver = pd.read_csv(predata_naver, header = 0, index_col = 0, encoding = 'utf-8')\n",
    "print (dfNaver.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfNaver = dfNaver[dfNaver._id == '5a29c445588c132954d1973a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extDfNaver = dfNaver.loc[:,['_id', 'category', 'date', 'rank', 'site', '공감', '비공감']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predata_daum = outcome_predata +  'filtered_predata_for_daum_news_comment.csv'\n",
    "dfDaum = pd.read_csv(predata_daum, header = 0, index_col = 0, encoding = 'utf-8')\n",
    "print (dfDaum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfDaum = dfDaum[dfDaum._id =='5a2a61bf588c13481c229d1e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extDfDaum = dfDaum.loc[:,['_id', 'category', 'date', 'rank', 'site', '공감', '비공감']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tagged_by_ct_daum_file = outcome_tagged_data+'word2vec_tagged_data_by_ct_for_daum_news_comment.pickled'\n",
    "if os.path.isfile(tagged_by_ct_daum_file):\n",
    "    tagged_daum_ct = pickle.load(open(tagged_by_ct_daum_file, 'rb'))\n",
    "else:\n",
    "    tagged_daum_ct = bm.MakeTaggedData_For_Comments(dfDaum, TaggedDocument, ct, stopwords)\n",
    "    pickle.dump(tagged_daum_ct, open(tagged_by_ct_daum_file, 'wb'))\n",
    "\n",
    "tagged_by_ct_naver_file = outcome_tagged_data+'word2vec_tagged_data_by_ct_for_naver_news_comment.pickled' \n",
    "if os.path.isfile(tagged_by_ct_naver_file):\n",
    "    tagged_naver_ct = pickle.load(open(tagged_by_ct_naver_file, 'rb'))\n",
    "else:\n",
    "    tagged_naver_ct = bm.MakeTaggedData_For_Comments(dfNaver, TaggedDocument, ct, stopwords)\n",
    "    pickle.dump(tagged_naver_ct, open(tagged_by_ct_naver_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainName_ct = './data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_ct.pickled'\n",
    "train_ct = pickle.load(open(trainName_ct, 'rb'))\n",
    "tfidf_ct = bm.Build_tfidf(train_ct)\n",
    "del train_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName_ct = 'ct'\n",
    "print ( '{}'.format(taggerName_ct))\n",
    "model1_ct = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-ct.model')\n",
    "model1_ct_Name = bm.Return_ModelName('word2vec', model1_ct, taggerName_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model1_ct_Name+'-daum'):\n",
    "    daum_vecs_by_model1 = pickle.load(open(outcome_vectorized_data+model1_ct_Name+'-daum', 'rb'))\n",
    "else:\n",
    "    wv1, daum_vecs_by_model1 = bm.Make_Pre_Data_For_DAUM(model1_ct, tfidf_ct, 1000, tagged_daum_ct)\n",
    "    pickle.dump(daum_vecs_by_model1, open(outcome_vectorized_data+model1_ct_Name+'-daum', 'wb'))\n",
    "    del wv1#, daum_vecs_by_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model1_ct_Name+'-naver'):\n",
    "    naver_vecs_by_model1 = pickle.load(open(outcome_vectorized_data+model1_ct_Name+'-naver', 'rb'))\n",
    "else:\n",
    "    wv1, naver_vecs_by_model1 = bm.Make_Pre_Data_For_DAUM(model1_ct, tfidf_ct, 1000, tagged_naver_ct)\n",
    "    pickle.dump(naver_vecs_by_model1, open(outcome_vectorized_data+model1_ct_Name+'-naver', 'wb'))\n",
    "    del wv1#, naver_vecs_by_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_by_model1 = glob(classifierPath+'*'+model1_ct_Name)\n",
    "load_Classifier_by_model1_Dict = dict(map(lambda x:bm.LoadClassifier(x), classifier_by_model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_by_model1, x, load_Classifier_by_model1_Dict[x]), load_Classifier_by_model1_Dict))\n",
    "predict_Outcome_daum = pd.DataFrame.from_dict(predict_Outcome_daum)\n",
    "predict_Outcome_daum = extDfDaum.merge(predict_Outcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_daum.to_csv(outcomeDaumCommentsPath+'outcome_comments_sentiment_daum_'+model1_ct_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_by_model1, x, load_Classifier_by_model1_Dict[x]), load_Classifier_by_model1_Dict))\n",
    "predict_Outcome_naver = pd.DataFrame.from_dict(predict_Outcome_naver)\n",
    "predict_Outcome_naver = extDfNaver.merge(predict_Outcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_naver.to_csv(outcomeNaverCommentsPath+'outcome_comments_sentiment_naver_'+model1_ct_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model1_ct\n",
    "del model1_ct_Name\n",
    "del classifier_by_model1\n",
    "del load_Classifier_by_model1_Dict\n",
    "del daum_vecs_by_model1\n",
    "del naver_vecs_by_model1\n",
    "del predict_Outcome_naver\n",
    "del predict_Outcome_daum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName_ct = 'ct'\n",
    "print ( '{}'.format(taggerName_ct))\n",
    "model2_ct = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-ct.model')\n",
    "model2_ct_Name = bm.Return_ModelName('word2vec', model2_ct, taggerName_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model2_ct_Name+'-daum'):\n",
    "    daum_vecs_by_model2 = pickle.load(open(outcome_vectorized_data+model2_ct_Name+'-daum', 'rb'))\n",
    "else:\n",
    "    wv1, daum_vecs_by_model2 = bm.Make_Pre_Data_For_DAUM(model2_ct, tfidf_ct, 1000, tagged_daum_ct)\n",
    "    pickle.dump(daum_vecs_by_model2, open(outcome_vectorized_data+model2_ct_Name+'-daum', 'wb'))\n",
    "    del wv1#, daum_vecs_by_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model2_ct_Name+'-naver'):\n",
    "    naver_vecs_by_model2 = pickle.load(open(outcome_vectorized_data+model2_ct_Name+'-naver', 'rb'))\n",
    "else:\n",
    "    wv1, naver_vecs_by_model2 = bm.Make_Pre_Data_For_DAUM(model2_ct, tfidf_ct, 1000, tagged_naver_ct)\n",
    "    pickle.dump(naver_vecs_by_model2, open(outcome_vectorized_data+model2_ct_Name+'-naver', 'wb'))\n",
    "    del wv1#, naver_vecs_by_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_by_model2 = glob(classifierPath+'*'+model2_ct_Name)\n",
    "load_Classifier_by_model2_Dict = dict(map(lambda x:bm.LoadClassifier(x), classifier_by_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_by_model2, x, load_Classifier_by_model2_Dict[x]), load_Classifier_by_model2_Dict))\n",
    "predict_Outcome_daum = pd.DataFrame.from_dict(predict_Outcome_daum)\n",
    "predict_Outcome_daum = extDfDaum.merge(predict_Outcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_daum.to_csv(outcomeDaumCommentsPath+'outcome_comments_sentiment_daum_'+model2_ct_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_by_model2, x, load_Classifier_by_model2_Dict[x]), load_Classifier_by_model2_Dict))\n",
    "predict_Outcome_naver = pd.DataFrame.from_dict(predict_Outcome_naver)\n",
    "predict_Outcome_naver = extDfNaver.merge(predict_Outcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_naver.to_csv(outcomeNaverCommentsPath+'outcome_comments_sentiment_naver_'+model2_ct_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model2_ct\n",
    "del model2_ct_Name\n",
    "del classifier_by_model2\n",
    "del load_Classifier_by_model2_Dict\n",
    "del daum_vecs_by_model2\n",
    "del naver_vecs_by_model2\n",
    "del predict_Outcome_naver\n",
    "del predict_Outcome_daum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName_ct = 'ct'\n",
    "print ( '{}'.format(taggerName_ct))\n",
    "model3_ct = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-ct.model')\n",
    "model3_ct_Name = bm.Return_ModelName('word2vec', model3_ct, taggerName_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model3_ct_Name+'-daum'):\n",
    "    daum_vecs_by_model3 = pickle.load(open(outcome_vectorized_data+model3_ct_Name+'-daum', 'rb'))\n",
    "else:\n",
    "    wv1, daum_vecs_by_model3 = bm.Make_Pre_Data_For_DAUM(model3_ct, tfidf_ct, 1000, tagged_daum_ct)\n",
    "    pickle.dump(daum_vecs_by_model3, open(outcome_vectorized_data+model3_ct_Name+'-daum', 'wb'))\n",
    "    del wv1#, daum_vecs_by_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model3_ct_Name+'-naver'):\n",
    "    naver_vecs_by_model3 = pickle.load(open(outcome_vectorized_data+model3_ct_Name+'-naver', 'rb'))\n",
    "else:\n",
    "    wv1, naver_vecs_by_model3 = bm.Make_Pre_Data_For_DAUM(model3_ct, tfidf_ct, 1000, tagged_naver_ct)\n",
    "    pickle.dump(naver_vecs_by_model3, open(outcome_vectorized_data+model3_ct_Name+'-naver', 'wb'))\n",
    "    del wv1#, naver_vecs_by_model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_by_model3 = glob(classifierPath+'*'+model3_ct_Name)\n",
    "load_Classifier_by_model3_Dict = dict(map(lambda x:bm.LoadClassifier(x), classifier_by_model3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_by_model3, x, load_Classifier_by_model3_Dict[x]), load_Classifier_by_model3_Dict))\n",
    "predict_Outcome_daum = pd.DataFrame.from_dict(predict_Outcome_daum)\n",
    "predict_Outcome_daum = extDfDaum.merge(predict_Outcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_daum.to_csv(outcomeDaumCommentsPath+'outcome_comments_sentiment_daum_'+model3_ct_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_by_model3, x, load_Classifier_by_model3_Dict[x]), load_Classifier_by_model3_Dict))\n",
    "predict_Outcome_naver = pd.DataFrame.from_dict(predict_Outcome_naver)\n",
    "predict_Outcome_naver = extDfNaver.merge(predict_Outcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_naver.to_csv(outcomeNaverCommentsPath+'outcome_comments_sentiment_naver_'+model3_ct_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model3_ct\n",
    "del model3_ct_Name\n",
    "del classifier_by_model3\n",
    "del load_Classifier_by_model3_Dict\n",
    "del daum_vecs_by_model3\n",
    "del naver_vecs_by_model3\n",
    "del predict_Outcome_naver\n",
    "del predict_Outcome_daum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News to tagged Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tagged_by_mecab_daum_file = outcome_tagged_data+'word2vec_tagged_data_by_mecab_for_daum_news_comment.pickled'\n",
    "if os.path.isfile(tagged_by_mecab_daum_file):\n",
    "    tagged_daum_mecab = pickle.load(open(tagged_by_mecab_daum_file, 'rb'))\n",
    "else:\n",
    "    tagged_daum_mecab = bm.MakeTaggedData_For_Comments(dfDaum, TaggedDocument, mecab, stopwords)\n",
    "    pickle.dump(tagged_daum_mecab, open(tagged_by_mecab_daum_file, 'wb'))\n",
    "\n",
    "tagged_by_mecab_naver_file = outcome_tagged_data+'word2vec_tagged_data_by_mecab_for_naver_news_comment.pickled' \n",
    "if os.path.isfile(tagged_by_mecab_naver_file):\n",
    "    tagged_naver_mecab = pickle.load(open(tagged_by_mecab_naver_file, 'rb'))\n",
    "else:\n",
    "    tagged_naver_mecab = bm.MakeTaggedData_For_Comments(dfNaver, TaggedDocument, mecab, stopwords)\n",
    "    pickle.dump(tagged_naver_mecab, open(tagged_by_mecab_naver_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set으로부터 TF-IDF Vectorizer을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainName = './data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_mecab.pickled'\n",
    "train_mecab = pickle.load(open(trainName, 'rb'))\n",
    "tfidf_mecab = bm.Build_tfidf(train_mecab)\n",
    "del train_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName_mecab = 'mecab'\n",
    "print ( '{}'.format(taggerName_mecab))\n",
    "model1_mecab = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "model1_mecab_Name = bm.Return_ModelName('word2vec', model1_mecab, taggerName_mecab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model1_mecab_Name+'-daum'):\n",
    "    daum_vecs_by_model1 = pickle.load(open(outcome_vectorized_data+model1_mecab_Name+'-daum', 'rb'))\n",
    "else:\n",
    "    wv1, daum_vecs_by_model1 = bm.Make_Pre_Data_For_DAUM(model1_mecab, tfidf_mecab, 1000, tagged_daum_mecab)\n",
    "    pickle.dump(daum_vecs_by_model1, open(outcome_vectorized_data+model1_mecab_Name+'-daum', 'wb'))\n",
    "    del wv1#, daum_vecs_by_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model1_mecab_Name+'-naver'):\n",
    "    naver_vecs_by_model1 = pickle.load(open(outcome_vectorized_data+model1_mecab_Name+'-naver', 'rb'))\n",
    "else:\n",
    "    wv1, naver_vecs_by_model1 = bm.Make_Pre_Data_For_DAUM(model1_mecab, tfidf_mecab, 1000, tagged_naver_mecab)\n",
    "    pickle.dump(naver_vecs_by_model1, open(outcome_vectorized_data+model1_mecab_Name+'-naver', 'wb'))\n",
    "    del wv1#, naver_vecs_by_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_by_model1 = glob(classifierPath+'*'+model1_mecab_Name)\n",
    "load_Classifier_by_model1_Dict = dict(map(lambda x:bm.LoadClassifier(x), classifier_by_model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_by_model1, x, load_Classifier_by_model1_Dict[x]), load_Classifier_by_model1_Dict))\n",
    "predict_Outcome_daum = pd.DataFrame.from_dict(predict_Outcome_daum)\n",
    "predict_Outcome_daum = extDfDaum.merge(predict_Outcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_daum.to_csv(outcomeDaumCommentsPath+'outcome_comments_sentiment_daum_'+model1_mecab_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_by_model1, x, load_Classifier_by_model1_Dict[x]), load_Classifier_by_model1_Dict))\n",
    "predict_Outcome_naver = pd.DataFrame.from_dict(predict_Outcome_naver)\n",
    "predict_Outcome_naver = extDfNaver.merge(predict_Outcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_naver.to_csv(outcomeNaverCommentsPath+'outcome_comments_sentiment_naver_'+model1_mecab_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model1_mecab\n",
    "del model1_mecab_Name\n",
    "del classifier_by_model1\n",
    "del load_Classifier_by_model1_Dict\n",
    "del daum_vecs_by_model1\n",
    "del naver_vecs_by_model1\n",
    "del predict_Outcome_naver\n",
    "del predict_Outcome_daum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName_mecab = 'mecab'\n",
    "print ( '{}'.format(taggerName_mecab))\n",
    "model2_mecab = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-mecab.model')\n",
    "model2_mecab_Name = bm.Return_ModelName('word2vec', model2_mecab, taggerName_mecab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model2_mecab_Name+'-daum'):\n",
    "    daum_vecs_by_model2 = pickle.load(open(outcome_vectorized_data+model2_mecab_Name+'-daum', 'rb'))\n",
    "else:\n",
    "    wv1, daum_vecs_by_model2 = bm.Make_Pre_Data_For_DAUM(model2_mecab, tfidf_mecab, 1000, tagged_daum_mecab)\n",
    "    pickle.dump(daum_vecs_by_model2, open(outcome_vectorized_data+model2_mecab_Name+'-daum', 'wb'))\n",
    "    del wv1#, daum_vecs_by_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model2_mecab_Name+'-naver'):\n",
    "    naver_vecs_by_model2 = pickle.load(open(outcome_vectorized_data+model2_mecab_Name+'-naver', 'rb'))\n",
    "else:\n",
    "    wv1, naver_vecs_by_model2 = bm.Make_Pre_Data_For_DAUM(model2_mecab, tfidf_mecab, 1000, tagged_naver_mecab)\n",
    "    pickle.dump(naver_vecs_by_model2, open(outcome_vectorized_data+model2_mecab_Name+'-naver', 'wb'))\n",
    "    del wv1#, naver_vecs_by_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_by_model2 = glob(classifierPath+'*'+model2_mecab_Name)\n",
    "load_Classifier_by_model2_Dict = dict(map(lambda x:bm.LoadClassifier(x), classifier_by_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_by_model2, x, load_Classifier_by_model2_Dict[x]), load_Classifier_by_model2_Dict))\n",
    "predict_Outcome_daum = pd.DataFrame.from_dict(predict_Outcome_daum)\n",
    "predict_Outcome_daum = extDfDaum.merge(predict_Outcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_daum.to_csv(outcomeDaumCommentsPath+'outcome_comments_sentiment_daum_'+model2_mecab_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_by_model2, x, load_Classifier_by_model2_Dict[x]), load_Classifier_by_model2_Dict))\n",
    "predict_Outcome_naver = pd.DataFrame.from_dict(predict_Outcome_naver)\n",
    "predict_Outcome_naver = extDfNaver.merge(predict_Outcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_naver.to_csv(outcomeNaverCommentsPath+'outcome_comments_sentiment_naver_'+model2_mecab_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model2_mecab\n",
    "del model2_mecab_Name\n",
    "del classifier_by_model2\n",
    "del load_Classifier_by_model2_Dict\n",
    "del daum_vecs_by_model2\n",
    "del naver_vecs_by_model2\n",
    "del predict_Outcome_naver\n",
    "del predict_Outcome_daum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerName_mecab = 'mecab'\n",
    "print ( '{}'.format(taggerName_mecab))\n",
    "model3_mecab = Word2Vec.load(loadModelPath+'word2vec_size-1000_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-mecab.model')\n",
    "model3_mecab_Name = bm.Return_ModelName('word2vec', model3_mecab, taggerName_mecab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model3_mecab_Name+'-daum'):\n",
    "    daum_vecs_by_model3 = pickle.load(open(outcome_vectorized_data+model3_mecab_Name+'-daum', 'rb'))\n",
    "else:\n",
    "    wv1, daum_vecs_by_model3 = bm.Make_Pre_Data_For_DAUM(model3_mecab, tfidf_mecab, 1000, tagged_daum_mecab)\n",
    "    pickle.dump(daum_vecs_by_model3, open(outcome_vectorized_data+model3_mecab_Name+'-daum', 'wb'))\n",
    "    del wv1#, daum_vecs_by_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(outcome_vectorized_data+model3_mecab_Name+'-naver'):\n",
    "    naver_vecs_by_model3 = pickle.load(open(outcome_vectorized_data+model3_mecab_Name+'-naver', 'rb'))\n",
    "else:\n",
    "    wv1, naver_vecs_by_model3 = bm.Make_Pre_Data_For_DAUM(model3_mecab, tfidf_mecab, 1000, tagged_naver_mecab)\n",
    "    pickle.dump(naver_vecs_by_model3, open(outcome_vectorized_data+model3_mecab_Name+'-naver', 'wb'))\n",
    "    del wv1#, naver_vecs_by_model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_by_model3 = glob(classifierPath+'*'+model3_mecab_Name)\n",
    "load_Classifier_by_model3_Dict = dict(map(lambda x:bm.LoadClassifier(x), classifier_by_model3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_daum = dict(map(lambda x: bm.PredictSentiment(daum_vecs_by_model3, x, load_Classifier_by_model3_Dict[x]), load_Classifier_by_model3_Dict))\n",
    "predict_Outcome_daum = pd.DataFrame.from_dict(predict_Outcome_daum)\n",
    "predict_Outcome_daum = extDfDaum.merge(predict_Outcome_daum,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_daum.to_csv(outcomeDaumCommentsPath+'outcome_comments_sentiment_daum_'+model3_mecab_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "predict_Outcome_naver = dict(map(lambda x: bm.PredictSentiment(naver_vecs_by_model3, x, load_Classifier_by_model3_Dict[x]), load_Classifier_by_model3_Dict))\n",
    "predict_Outcome_naver = pd.DataFrame.from_dict(predict_Outcome_naver)\n",
    "predict_Outcome_naver = extDfNaver.merge(predict_Outcome_naver,\n",
    "                                   left_index = True, right_index = True)\n",
    "predict_Outcome_naver.to_csv(outcomeNaverCommentsPath+'outcome_comments_sentiment_naver_'+model3_mecab_Name,index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model3_mecab\n",
    "del model3_mecab_Name\n",
    "del classifier_by_model3\n",
    "del load_Classifier_by_model3_Dict\n",
    "del daum_vecs_by_model3\n",
    "del naver_vecs_by_model3\n",
    "del predict_Outcome_naver\n",
    "del predict_Outcome_daum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
