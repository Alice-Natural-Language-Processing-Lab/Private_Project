{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model\n",
    "> * 단어를 vector로 바꿔주는 알고리즘 : Embedding\n",
    "> * Neural Network Language Model(NNLM)을 계승하면서도 학습 속도와 성능을 끌어올림\n",
    "> * 단어를 vectorization할 때 단어의 문맥적 의미를 보존함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이론 참고 \n",
    "\n",
    "> [ratsgo's blog](https://ratsgo.github.io/)  \n",
    ">> 1. [Word2Vec으로 문장 분류하기](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)  \n",
    ">> 2. [빈도수 세기의 놀라운 마법 Word2Vec, Glove, Fasttext](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/)  \n",
    ">> 3. [Neural Network Language Model](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/)  \n",
    ">> 4. [Word2Vec의 학습 방식](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/)  \n",
    "\n",
    "> [data scienceschool](https://datascienceschool.net)  \n",
    ">> 1. [단어 임베딩과 word2vec](https://datascienceschool.net/view-notebook/6927b0906f884a67b0da9310d3a581ee/)  \n",
    ">> 2. [Scikit-Learn의 문서 전처리 기능](https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/)  \n",
    ">> 3. [확률론적 언어 모형](https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/)  \n",
    "\n",
    "> [BEOMSU KIM](https://shuuki4.wordpress.com/category/deep-learning/)\n",
    ">>  1. [word2vec 관련 이론 정리](https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Language Model (NNLM)\n",
    "> * Feed-Forward Neural Network Language Model\n",
    "> * 단어를 vector로 바꾸는 neural network 기반 방법론\n",
    "> * 컴퓨터는 단어를 숫자로 바꿔서 입력해야 컴퓨터는 연산을 수행\n",
    "> * Input Layer, Projection Layer, Hidden Layer, Output Layer로 이우러진 Neural Network\n",
    "> ![image](./nnlm.png)\n",
    "\n",
    ">> 1. 현재 보고 있는 단어 이전의 단어들 N개를 one-hot encoding으로 벡터화\n",
    ">> 2. 사전의 크기를 V라고 하고 Projection Layer의 크기를 P라고 했을 때, 각각의 벡터들은 V*P 크기의 Projection Matrix에 의해 다음 layer로 넘어가게 된다.\n",
    ">> 3. Projection Layer의 값을 input이라고 생각하고, 크기 H인 hidden layer를 거쳐서 output layer에서 **각 단어들이 나올 확률**을 계산\n",
    ">> 4. 실제 단어의 one-hot encoding 벡터와 비교하여 에러를 계산하고, 이를 back-propagation해서 네트워크의 weight들을 최적화해나가는 것\n",
    "\n",
    "* 단점\n",
    "> * 몇 개의 단어를 볼 건지에 대한 parameter N이 고정되어 있고, 정해주어야 한다. \n",
    "> * 이전의 단어들에 대해서만 신경쓸 수 있고, 현재 보고 있는 단어 앞에 있는 단어들을 고려하지 못한다. \n",
    "> * **느리다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Language Model ( RNNLM)\n",
    "> * NNLM을 Recurrent Neural Network의 형태로 변형한 것\n",
    "> * Projection Layer없이 input, hidden, output layer로만 구성\n",
    "> * **hidden layer에 recurrent한 연결이 있어 이전 시간의 Hidden Layer의 입력이 다시 입력되는 형식으로 구성**\n",
    "> ![image](./rnnlm.png)\n",
    "> * 그림에서 U라고 나타나 있는 부분이 Word Embedding으로 사용되며, **Hidden layer의 크기를 H라고 할 때 각 단어는 길이 H의 벡터로 표현**\n",
    "> * NNLM과 달리 몇 개의 단어인지에 대해 정해줄 필요가 없이, 학습시켜줄 단어를 순차적으로 입력해주는 방식으로 학습\n",
    "> * NNLM보다 연산량이 적다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Continuous Bag of Words (CBOW)  \n",
    "* 주변에 있는 단어들을 가지고 중심에 있는 단어를 맞추는 방식\n",
    "> ex) 나는 _에 간다.\n",
    "\n",
    "![cbow](./cbow.png)\n",
    "* 주어진 단어에 대해 앞 뒤로 c/2개 씩 총 C개의 단어를 Input으로 사용하여, 주어진 단어를 맞추기 위한 네트워크를 만든다. \n",
    "* Input Layer, Projection Layer, Output Layer로 구성\n",
    "* Input Layer에서 중간 레이어로 가는 과정이 weight를 곱해주는 것이라기 보다는 단순히 Projection하는 과정에 가까움\n",
    "* Input Layer에서 Projection Layer로 갈 때는 모든 단어들이 공통적으로 사용하는 V*N 크기의 Projection Matrix W가 있다. (N은 Projection Layer의 길이 = 사용할 벡터의 길이)\n",
    "* Projection Layer에서 Output Layer로 갈 때는 N*V 크기의 Weight Matrix W'가 있다. \n",
    "* Input layer에서는 NNLM모델과 똑같이 단어를 One-hot encoding으로 넣어주고, 여러 개의 단어를 각각 Projection시킨 후 그 벡터들의 평균을 구해서 Projection Layer에 보낸다. \n",
    "* 그 뒤는 여기에 Weight Matrix를 곱해서 Output Layer로 보내고 softmax계산을 한후, 이 결과를 진짜 단어의 one-hot encoding과 비교하여 에러를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Skip-Gram\n",
    "* 중심에 있는 단어로 주변 단어를 예측하는 방법\n",
    "> ex) _ 외나무다리 __\n",
    "\n",
    "![skip-gram](./skip-gram.png)\n",
    "* 예측하는 단어들의 경우 현재 단어 주위에서 샘플링하는데, '가까이 위치해있는 단어일수록 현재 단어와 관련이 더 많은 단어일것이다'라는 생각을 적용하기 위해 멀리 떨어져 있는 단어수록 낮은 확률로 택하는 방법을 사용한다. \n",
    "* 나머지 구조는 CBOW와 방향만 반대일 뿐 굉장히 유사하"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고  \n",
    "> [Gensim](https://radimrehurek.com/gensim/index.html)  \n",
    "> [Gensim Tutorial](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model 생성을 위한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() \n",
    "#sg (int {1, 0}) – Defines the training algorithm. If 1, CBOW is used, otherwise, skip-gram is employed.\n",
    "def Make_Word2Vec_Model(data, size, epoch, sg, window,min_count, cbow_mean, workers, negative, hs, tagger):\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\")\n",
    "    from datetime import datetime\n",
    "    from gensim.models import Word2Vec\n",
    "    start = datetime.now()\n",
    "    modelPath = './model/'\n",
    "    modelName = 'word2vec_size-{}_epoch-{}_window-{}_negative-{}_hs-{}_sg-{}_cbow_mean-{}_min_count-{}_by-{}.model'.format(\n",
    "        size, epoch, window, negative, hs, sg, cbow_mean, min_count, tagger)\n",
    "    modelName = modelPath+modelName\n",
    "    print (modelName)\n",
    "    w2v_model = Word2Vec(size = size, sg = sg, cbow_mean = cbow_mean, negative = negative,\n",
    "                                  hs = hs, window = window, workers = workers, iter=epoch, min_count = min_count)\n",
    "    w2v_model.build_vocab(tqdm(data))\n",
    "    w2v_model.train(tqdm(data),  total_examples=w2v_model.corpus_count, epochs=w2v_model.iter, compute_loss = True)\n",
    "    w2v_model.init_sims(replace = True)\n",
    "    w2v_model.save(modelName)\n",
    "    end = datetime.now()\n",
    "    print (\"Total running time: \", end-start)\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('./data/sentiment_data/raw_data_for_sentiment.txt',header=None,encoding='utf-8')\n",
    "print (rawdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOPWORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('./data/stopwordsList.txt',encoding='utf-8').readlines()\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 포맷으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tagging(sentence, tagger, stopwords):\n",
    "    pos = tagger.pos(sentence)\n",
    "    pos = [x[0] for x in pos]\n",
    "    pos = [x for x in pos if not x in stopwords]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def Tagging2(sentence, tagger, stopwords):\n",
    "    pos = pd.Series(tagger.pos(sentence)).str[0]\n",
    "    pos = pos[~pos.isin(stopwords)]\n",
    "    return pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Pre_Data_Sub(series, tagger, stopwords):\n",
    "    from gensim.models.doc2vec import TaggedDocument\n",
    "    pos = Tagging2(series[0], tagger, stopwords)\n",
    "    label = series[1]\n",
    "    return TaggedDocument(pos, [label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def Make_Pre_Data(rawdata, tagger, stopwords):\n",
    "    outList = list()\n",
    "    for idx in tqdm(rawdata.index):\n",
    "        outList.append([Tagging2(rawdata.loc[idx, 0], tagger, stopwords), rawdata.loc[idx, 1]])\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "ct = ctwitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas(desc=\"progress\")\n",
    "pre_data = rawdata.progress_apply(lambda x: Make_Pre_Data_Sub(x, ct, stopwords), axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pre_data,open('./data/pre_data/tagged_data/pre_data__for_word2vec_sentiment_by_ct.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(pre_data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train,open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_ct.pickled','wb'))\n",
    "pickle.dump(test,open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_ct.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_ct.pickled','rb'))\n",
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_ct.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1484524.27it/s]\n",
      "100%|██████████| 442359/442359 [00:00<00:00, 1512929.06it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1291482.82it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1363264.60it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = [ x.words for x in tqdm(train)] \n",
    "y_train = [ x.tags for x in tqdm(train)] \n",
    "x_test = [ x.words for x in tqdm(test)] \n",
    "y_test = [ x.tags for x in tqdm(test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/word2vec_size-2000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-ct.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:10<00:00, 41323.88it/s]\n",
      "  0%|          | 0/442359 [00:00<?, ?it/s]C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "100%|██████████| 442359/442359 [03:19<00:00, 2220.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time:  1:01:02.137368\n",
      "Wall time: 1h 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(data = x_train, size = 2000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 1, hs = 0, tagger = 'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/word2vec_size-2000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-ct.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:09<00:00, 46835.69it/s]\n",
      "  0%|          | 0/442359 [00:00<?, ?it/s]C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "100%|██████████| 442359/442359 [02:49<00:00, 2607.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time:  0:43:09.908259\n",
      "Wall time: 43min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(data = x_train, size = 2000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/word2vec_size-2000_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-ct.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:09<00:00, 45301.04it/s]\n",
      "  0%|          | 0/442359 [00:00<?, ?it/s]C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "100%|██████████| 442359/442359 [25:00<00:00, 294.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time:  7:49:37.103990\n",
      "Wall time: 7h 49min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(data = x_train, size = 2000, epoch = 20, \n",
    "                   sg = 1, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'ct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pre_data = rawdata.progress_apply(lambda x: Make_Pre_Data_Sub(x, mecab, stopwords), axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pre_data = Make_Pre_Data(rawdata, mecab, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pre_data,open('./data/pre_data/tagged_data/pre_data__for_word2vec_sentiment_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(pre_data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train,open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_mecab.pickled','wb'))\n",
    "pickle.dump(test,open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/pre_data/train_test_Data/pre_data_train_for_word2vec_sentiment_by_mecab.pickled','rb'))\n",
    "test = pickle.load(open('./data/pre_data/train_test_Data/pre_data_test_for_word2vec_sentiment_by_mecab.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:00<00:00, 1457736.54it/s]\n",
      "100%|██████████| 442359/442359 [00:00<00:00, 1528298.60it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1362913.10it/s]\n",
      "100%|██████████| 49151/49151 [00:00<00:00, 1486739.24it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = [ x.words for x in tqdm(train)] \n",
    "y_train = [ x.tags for x in tqdm(train)] \n",
    "x_test = [ x.words for x in tqdm(test)] \n",
    "y_test = [ x.tags for x in tqdm(test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/word2vec_size-2000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-1_min_count-2_by-mecab.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:10<00:00, 40336.77it/s]\n",
      "  0%|          | 0/442359 [00:00<?, ?it/s]C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "100%|██████████| 442359/442359 [03:02<00:00, 2421.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time:  0:58:07.911579\n",
      "Wall time: 58min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(data = x_train, size = 2000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 1, hs = 0, tagger = 'mecab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/word2vec_size-2000_epoch-20_window-10_negative-7_hs-0_sg-0_cbow_mean-0_min_count-2_by-mecab.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:09<00:00, 44282.71it/s]\n",
      "  0%|          | 0/442359 [00:00<?, ?it/s]C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "100%|██████████| 442359/442359 [02:43<00:00, 2710.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time:  0:41:33.832084\n",
      "Wall time: 41min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(data = x_train, size = 2000, epoch = 20, \n",
    "                   sg = 0, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'mecab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/word2vec_size-2000_epoch-20_window-10_negative-7_hs-0_sg-1_cbow_mean-0_min_count-2_by-mecab.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442359/442359 [00:09<00:00, 44309.46it/s]\n",
      "  0%|          | 0/442359 [00:00<?, ?it/s]C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "100%|██████████| 442359/442359 [22:25<00:00, 328.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time:  7:12:20.963696\n",
      "Wall time: 7h 12min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Make_Word2Vec_Model(data = x_train, size = 2000, epoch = 20, \n",
    "                   sg = 1, window = 10, workers = cores, negative = 7, min_count = 2,\n",
    "                    cbow_mean = 0, hs = 0, tagger = 'mecab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
