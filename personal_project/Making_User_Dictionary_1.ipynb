{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 ( Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 197504, 5532]\n",
      "[('이', 'Noun'), ('것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n",
      "best: [('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "import re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import html\n",
    "\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter, Mecab\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "\n",
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "mecab = Mecab()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "print (list(map(lambda x: len(xxxx[x]), xxxx)))\n",
    "\n",
    "score_weights = {\n",
    "    'num_nouns': -0.1,\n",
    "    'num_words': -0.2,\n",
    "    'no_noun': -1\n",
    "}\n",
    "\n",
    "def my_score(candidate):\n",
    "    num_nouns = len([w for w,t in candidate if t == 'Noun'])\n",
    "    num_words = len(candidate)\n",
    "    no_noun = 1 if num_nouns == 0 else 0\n",
    "    \n",
    "    score = (num_nouns * score_weights['num_nouns'] \n",
    "             + num_words * score_weights['num_words']\n",
    "             + no_noun * score_weights['no_noun'])\n",
    "    return score\n",
    "\n",
    "ct.set_selector(score_weights, my_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 사전 추가 및 Stopwords 를 위한 전처리 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언론사 목록 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> stopwords를 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressList = pd.read_excel('./data/presslist.xlsx')\n",
    "\n",
    "with(open('./data/newspress.txt','w',encoding='utf-8')) as f:\n",
    "    f.write('\\n'.join(set(pressList['언론사'].tolist())))\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwordList = Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIA Dictionary 파일의 태그 변환을 위한 Twitter 태그와 Hannanum(KAIST) 태그의 매칭파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagFile = pd.read_excel('./data/Korean_POS_tags_comparison.xlsx',sheet_name='chart3',dtype = str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagDict = dict()\n",
    "for idx in tagFile.index:\n",
    "    if tagFile.loc[idx]['Hannanum'][:3] =='XSN':\n",
    "        tag = 'XSN'\n",
    "    else:\n",
    "        tag = tagFile.loc[idx]['Hannanum']\n",
    "    tagDict[tag]=tagFile.loc[idx]['Twitter_Korean_Text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TagChange(tag):\n",
    "    return tagDict[tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIA Dictionary파일로부터 Twitter 태그를 가진 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "niaDictFile = pd.read_excel('./data/NIADic.xlsx')\n",
    "\n",
    "niaDictFile = niaDictFile[niaDictFile['tag']!='xp']\n",
    "niaDictFile.loc[:,'tag'] = niaDictFile['tag'].map(lambda x: TagChange(x.upper()))\n",
    "\n",
    "for idx in niaDictFile.tag.unique():\n",
    "    outfile = ('./data/'+'twitter_tagger_' + idx +'_TagChange_from_NIADic.txt')\n",
    "    with open(outfile,'w',encoding='utf-8') as f:\n",
    "        outdata = niaDictFile[niaDictFile['tag']==idx]['term']\n",
    "        outdata.to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위키피디아 dump 파일로부터 title 정보만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikifile = './data/first_title_from_wikidumps.txt'\n",
    "if not os.path.isfile(wikifile):\n",
    "    outfile = open(wikifile,'w',encoding='utf-8')\n",
    "    wikiTextList = glob('./data/wikidumps/text/A*')\n",
    "    for folder in wikiTextList:\n",
    "        fileList = glob(folder+'/*')\n",
    "        for file in fileList:\n",
    "            with open(file, 'r',encoding ='utf-8') as f:\n",
    "                while 1:\n",
    "                    line = f.readline()\n",
    "                    if not line:break\n",
    "                    try:\n",
    "                        x1 = re.search('title',line).group()\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        outfile.write(line)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mecab과 customized-konlpy의 사용자사전 추가를 위한 namu wiki와 위키피디아 title 정보 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325438\n",
      "1157924\n",
      "885658\n",
      "882163\n",
      "882163\n",
      "882135\n",
      "882063\n",
      "882017\n",
      "881619\n",
      "702748\n",
      "616503\n"
     ]
    }
   ],
   "source": [
    "outSet = set()\n",
    "with open('./data/first_title_from_wikidumps.txt','r',encoding='utf-8') as f:\n",
    "    while 1:\n",
    "        line = f.readline()\n",
    "        if not line:break\n",
    "        try:\n",
    "            tmp1 = re.split('title=',line)[1].strip()\n",
    "        except:pass\n",
    "        else:\n",
    "            tmp2 = re.sub('[\"]','',tmp1)[:-1]\n",
    "            tmp2 = html.unescape(tmp2).strip()\n",
    "            outSet.add(tmp2)\n",
    "    f.close()\n",
    "arrayOut = pd.DataFrame(list(outSet))\n",
    "outSet = set()\n",
    "with open('./data/out_namu_wiki.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = html.unescape(line).strip()\n",
    "        outSet.add(line)\n",
    "    f.close()\n",
    "arrayOut2 = pd.DataFrame(list(outSet))\n",
    "\n",
    "df1 = pd.concat([arrayOut, arrayOut2])\n",
    "print (len(df1))\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop_duplicates(subset = [0], keep = False, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1 = pd.DataFrame(df1[0].str.split('/').str.get(0))\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1 = pd.DataFrame(df1[0].str.split(':').str.get(0))\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1 = pd.DataFrame(df1[0].str.split('\\([a-zA-Z0-9가-힣\\s\\W\\w\\S]+\\)').str.get(0))\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop_duplicates(inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[^a-zA-Z0-9가-힣\\s]+$')].index, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('\\([a-zA-Z]+\\)')].index, inplace=True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('\\([0-9]\\)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('\\([가-힣\\s]\\)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+\\.[0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+\\.[0-9]+[a-zA-Z]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+\\.[0-9].[a-zA-Z]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[\\.][0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[\\.][0-9]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('[\\.][a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('^(\\.[\\.a-zA-Z0-9])+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+png[.]+)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+jpg[.]+)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('(.+\\.+jpg+$)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+\\.+png+$)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('(.+\\.+[a-zA-Z]+$)')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9a-zA-Z\\./]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[\\W]+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600477\n",
      "586771\n",
      "577580\n",
      "577397\n",
      "577383\n",
      "575476\n"
     ]
    }
   ],
   "source": [
    "df1.drop(df1[df1[0].str.match('^[a-zA-Z ]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.\\([a-zA-Z0-9가-힣\\s]\\)+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.\\([^a-zA-Z0-9가-힣\\s]\\)+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('.[^a-zA-Z0-9가-힣\\s]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[^a-zA-Z0-9가-힣\\s]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[^가-힣]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('^[0-9]+년')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[0-9]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[\\(]+[a-zA-Z]+[\\)]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('.[0-9]+.+[a-zA-Z]+$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.[~].')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+[-]+[0-9]+')].index,inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[0-9]+[-]+[0-9]')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('^[0-9 ]+\\.+[0-9 ]+[a-zA-Z]+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.\\([a-zA-Z0-9가-힣\\s]+\\)+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0].str.match('.+-[0-9]{4,}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('[a-zA-Z0-9가-힣\\s]+[0-9]{0,4}-[0-9]{1,2}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.*[0-9]{0,4}-[0-9]{1,2}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561700\n",
      "561427\n",
      "561322\n",
      "559890\n",
      "559197\n",
      "531241\n",
      "530612\n",
      "524465\n",
      "524464\n",
      "523685\n",
      "523684\n",
      "520667\n"
     ]
    }
   ],
   "source": [
    "df1.drop(df1[df1[0].str.match('.*-$')].index, inplace =True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('.+[0-9]{1,4}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "df1.drop(df1[df1[0].str.match('^[0-9]{1,}[a-zA-Z]{1,}')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+II$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+I$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[^a-zA-Z]+[\\-a-zA-Z]{1,1}$')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+및.+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+의 .+')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+,[ ]{1,}')].index, inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1 = df1[df1[0].str.split(' ').str.len() < 5]\n",
    "print (len(df1))\n",
    "df1.drop(df1[df1[0] ==''].index, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('[a-zA-Z0-9가-힣\\s]{1,1}[^a-zA-Z0-9가-힣][a-zA-Z0-9가-힣\\s]{1,1}$')].index, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+imf.*')].index, inplace = True)\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "\n",
    "df1.drop(df1[df1[0].str.match('.+[~`\\!@#\\$%\\^\\&\\*\\(\\)\\_\\+=\\{\\}\\[\\]\\\\\\|;:\\'\\\"<,>\\?/].*')].index, inplace = True)\n",
    "\n",
    "df1.dropna(inplace = True)\n",
    "df1.reset_index(drop=True, inplace = True)\n",
    "print (len(df1))\n",
    "df1.to_csv('./data/second_title_from_wikidumps.txt',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIADic와 NAMUWIKI& WIKIPEDIA의 TITLE로 부터 추출된 명사 데이터 정제후 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFilter(dataPath):\n",
    "    print (dataPath)\n",
    "    datalist = glob(dataPath+'*.txt')\n",
    "    datalist.extend(glob(dataPath+'*.csv'))\n",
    "    datalist.extend(glob(dataPath+'*.CSV'))\n",
    "    datadf = pd.DataFrame(columns=[0])\n",
    "    for data in datalist:\n",
    "        df = pd.read_csv(data,encoding='utf-8', header=None)\n",
    "        if len(datadf) == 0:\n",
    "            datadf = df\n",
    "        else:\n",
    "            datadf = pd.concat([datadf, df])\n",
    "    datadf = datadf[0]\n",
    "    print (datadf.shape)\n",
    "\n",
    "    datadf.drop_duplicates(inplace = True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    datadf.dropna(inplace = True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    print (datadf.shape)\n",
    "\n",
    "    datadf = datadf.apply(lambda x: re.sub(' ','',x))\n",
    "    print (datadf.shape)\n",
    "\n",
    "    datadf.drop(datadf[datadf.str.contains('[a-zA-Z]+[양씨군사의는은이가히]+$')].index, inplace = True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    print (datadf.shape)\n",
    "\n",
    "    datadf.drop(datadf[datadf.str.contains('[a-zA-Z가-힣]+[양씨군사의는은이가히]+$')].index, inplace = True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    print (datadf.shape)\n",
    "\n",
    "    datadf.drop(datadf[datadf.str.contains('.+[씨는은이가]+$')].index, inplace=True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    print (datadf.shape)\n",
    "\n",
    "    eng = datadf[datadf.str.match('[a-zA-Z0-9]+$')]\n",
    "    eng.reset_index(drop=True,inplace = True)\n",
    "    datadf.drop(datadf[datadf.str.match('[a-zA-Z0-9]+$')].index, inplace=True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    datadf.drop(datadf[datadf.str.match('[a-zA-Z0-9가-힣]+으로$')].index, inplace = True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    datadf.dropna(inplace = True)\n",
    "    datadf.drop_duplicates(inplace = True)\n",
    "    datadf.reset_index(drop=True, inplace = True)\n",
    "    print (datadf.shape)\n",
    "    return datadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/checkfile/a/\n",
      "(1313179,)\n",
      "(1135138,)\n",
      "(1135138,)\n",
      "(1135122,)\n",
      "(1076509,)\n",
      "(1076419,)\n",
      "(1016015,)\n",
      "(1016015,)\n"
     ]
    }
   ],
   "source": [
    "datapath = './data/checkfile/a/'\n",
    "datadf = DataFilter(datapath)\n",
    "print (datadf.shape)\n",
    "datadf.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab & twitter를 활용한 품사 태깅으로 명사만 선별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf2 = datadf.apply(ct.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf3 = datadf.apply(mecab.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 제거 \n",
    "* ex) 엄마를말하다, 폼은일시적이지만클래스는영원하다 같은 끝이 ~다로 끝나는 TITLE 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232843,)\n",
      "(0,)\n",
      "(230911,)\n"
     ]
    }
   ],
   "source": [
    "datadf2_1 = datadf2[datadf2.apply(lambda x: len(list(map(lambda y: y[1], x)))) == 1]\n",
    "print (datadf2_1.shape)\n",
    "datadf2_1 = datadf2_1[datadf2_1.apply(lambda x: x[0][1]=='Noun')]\n",
    "datadf2_2 = datadf2_1[datadf2_1.apply(lambda x: x[0][1]!='Noun')]\n",
    "print (datadf2_2.shape)\n",
    "datadf2_2 = datadf2_2.apply(lambda x: x[0][0])\n",
    "#datadf2_2.reset_index(drop=True, inplace = True)\n",
    "print (datadf2_1.shape)\n",
    "datadf2_1 = datadf2_1.apply(lambda x: x[0][0])\n",
    "#datadf2_1.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318891,)\n",
      "(313737,)\n"
     ]
    }
   ],
   "source": [
    "datadf3_1 = datadf3[datadf3.apply(lambda x: len(list(map(lambda y: y[1], x)))) == 1]\n",
    "#datadf3_1.reset_index(drop=True, inplace = True)\n",
    "print (datadf3_1.shape)\n",
    "datadf3_2 = datadf3_1[datadf3_1.apply(lambda x: x[0][1] in ['NNG','NNP'])]\n",
    "print (datadf3_2.shape)\n",
    "datadf3_2 = datadf3_2.apply(lambda x: x[0][0])\n",
    "#datadf3_2.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5154,)\n"
     ]
    }
   ],
   "source": [
    "datadf3_3 = datadf3_1[datadf3_1.apply(lambda x: not x[0][1] in ['NNG','NNP'])]\n",
    "print (datadf3_3.shape)\n",
    "#datadf3_3.reset_index(drop= True, inplace = True)\n",
    "datadf3_3 = datadf3_3.apply(lambda x: x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010861,)\n"
     ]
    }
   ],
   "source": [
    "datadf.drop(datadf[datadf3_3.index].index, inplace = True)\n",
    "datadf.dropna(inplace = True)\n",
    "datadf.drop_duplicates(inplace = True)\n",
    "datadf.reset_index(drop=True, inplace = True)\n",
    "print (datadf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 태깅이 하나만 된 것 중에서 명사가 아닌 것은 다 제외하기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 추가\n",
    "* 사전 획득 soynlp님 github page [github](https://github.com/lovit/sharing_korean_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import abc\n",
    "import json\n",
    "\n",
    "class Dictionary:\n",
    "    def __init__(self, json_path):\n",
    "        self._load_dictionary(json_path)\n",
    "    \n",
    "    def _load_dictionary(self, json_path):\n",
    "        with open(json_path, encoding='utf-8') as f:\n",
    "            json_object = json.load(f)\n",
    "        self.meta = json_object.get('meta', {})\n",
    "        self.hierarchy = json_object.get('dictionary', {})\n",
    "        self.hierarchy = self._nested_dict_to_set(self.hierarchy)\n",
    "        \n",
    "    def _nested_dict_to_set(self, nested):\n",
    "        return {key:set(value) if not isinstance(value, abc.Mapping) else self._nested_dict_to_set(value) for key, value in nested.items()}\n",
    "    \n",
    "    def show_hierarchy(self, nested=None, depth=0):\n",
    "        if nested == None:\n",
    "            nested = self.hierarchy\n",
    "        for key, value in nested.items():\n",
    "            print('{}|--{}'.format('   '*depth, key))\n",
    "            if isinstance(value, abc.Mapping):\n",
    "                self.show_hierarchy(value,depth+1)\n",
    "    \n",
    "    def get_words(self, categories=None):\n",
    "        wordset = set()\n",
    "        root = self.hierarchy\n",
    "        if categories:\n",
    "            if type(categories) == str:\n",
    "                categories = categories.split()\n",
    "            for category in categories:\n",
    "                root = root.get(category, {})\n",
    "        if not isinstance(root, dict):\n",
    "            return set(root)\n",
    "        for _, words in self._nested_dict_iter(root):\n",
    "            wordset.update(words)\n",
    "        return wordset\n",
    "    \n",
    "    def _nested_dict_iter(self, nested):\n",
    "        for key, value in nested.items():\n",
    "            if isinstance(value, abc.Mapping):\n",
    "                yield from self._nested_dict_iter(value)\n",
    "            else:\n",
    "                yield key, value\n",
    "    \n",
    "    def find_categories(self, word):\n",
    "        categories = set()\n",
    "        for key, words in self._nested_dict_iter(self.hierarchy):\n",
    "            if word in words:\n",
    "                categories.add(key)\n",
    "        return categories\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Dictionary:{}'.format(self.meta.get('name', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = pd.Series(list(Dictionary('./data/checkfile/dictionary_with_info_GICS.json').get_words()))\n",
    "x2 = pd.Series(list(Dictionary('./data/checkfile/nouns_from_economy_news.json').get_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128758,)\n"
     ]
    }
   ],
   "source": [
    "x1 = x1[~x1.str.match('[a-zA-Z0-9\\W]')]\n",
    "x2 = x2[~x2.str.match('[a-zA-Z0-9\\W]')]\n",
    "wordsList = pd.concat([x1,x2])\n",
    "wordsList.reset_index(drop=True, inplace = True)\n",
    "wordsList.drop_duplicates(inplace = True)\n",
    "wordsList.reset_index(drop=True, inplace = True)\n",
    "wordsList.dropna(inplace = True)\n",
    "print (wordsList.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsl2_ct = wordsList.apply(ct.pos)\n",
    "wsl2_mecab = wordsList.apply(mecab.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128758,)\n",
      "(128758,)\n"
     ]
    }
   ],
   "source": [
    "wsl2_ct.dropna(inplace = True)\n",
    "print (wsl2_ct.shape)\n",
    "wsl2_mecab.dropna(inplace = True)\n",
    "print (wsl2_mecab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = wsl2_ct[wsl2_ct.apply(lambda x: x[-1][-1] in ['Josa','Eomi', 'Suffix'])].index | wsl2_mecab[wsl2_mecab.apply(lambda x: x[-1][-1] in ['EC','EP','EF','ETM','ETN','JC','JKB','JKB',\n",
    " 'JKC','JKG','JKO','JKQ','JKS','JKV','XSA','XSV'])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = wordsList[x]\n",
    "words2 = wordsList[pd.Int64Index(np.arange(len(wordsList))).difference(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstWordList = pd.concat([words2, words1[~words1.str.match('.+[써씨더서은만랑큼음는인에이게가으로기다을를들지측]$')]])\n",
    "lstWordList.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120635,)\n"
     ]
    }
   ],
   "source": [
    "print (lstWordList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1131496,)\n",
      "(1091993,)\n"
     ]
    }
   ],
   "source": [
    "lstOutcomeWords = pd.concat([datadf, lstWordList])\n",
    "print (lstOutcomeWords.shape)\n",
    "lstOutcomeWords.dropna(inplace=True)\n",
    "lstOutcomeWords.reset_index(drop=True, inplace = True)\n",
    "lstOutcomeWords.drop_duplicates(inplace = True)\n",
    "lstOutcomeWords.reset_index(drop=True, inplace = True)\n",
    "print (lstOutcomeWords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstOutcome = lstOutcomeWords.apply(mecab.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstOutcome.to_csv('./data/checkfile/merged_file.txt',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstOutcome1 = lstOutcome[lstOutcome.apply(lambda x: len(list(map(lambda y: y[1], x)))) == 1]\n",
    "lstOutcome1.reset_index(drop=True, inplace = True)\n",
    "lstOutcome2 = lstOutcome[lstOutcome.apply(lambda x: len(list(map(lambda y: y[1], x)))) != 1]\n",
    "lstOutcome2.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mecab용 사용자사전 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jongsung(x):\n",
    "    if (ord(x[-1])-0xAC00)%28 == 0:\n",
    "        return 'F'\n",
    "    else:\n",
    "        return 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = lstOutcome1.apply(lambda x: x[0][0])\n",
    "b = pd.Series()\n",
    "#c = datadf3_1.apply(lambda x: '+'.join(list(map(lambda y: y[1], x))))\n",
    "c =pd.Series(['NNP']*len(a))\n",
    "d = pd.Series(['*']*len(a))\n",
    "d1 = a.apply(lambda x: Jongsung(x))\n",
    "x1 = pd.concat([a,b,b,b,c,d,d1,a,c,d,d,d],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lstOutcome2.apply(lambda x: ''.join(list(map(lambda y: y[0], x))))\n",
    "b = pd.Series()\n",
    "c = pd.Series(['NNP']*len(a))\n",
    "d = pd.Series(['*']*len(a))\n",
    "d1 = a.apply(lambda x: Jongsung(x))\n",
    "e = pd.Series(['Compound']*len(a))\n",
    "f = pd.Series(['*']*len(a))\n",
    "g = pd.Series(['*']*len(a))\n",
    "h = lstOutcome2.apply(lambda x: '+'.join(list(map(lambda y: y[0]+'/'+y[1]+'/*', x))))\n",
    "x2 = pd.concat([a,b,b,b,c,d,d1,a,e,f,g,h],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex) \n",
    "계몽전의,1,2,3,NNP,*,T,계몽전의,Compound,*,*,계몽/NNG/*+전의/NNG/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1.columns = list(range(len(x1.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.columns = list(range(len(x2.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.concat([x1,x2])\n",
    "m.reset_index(drop = True, inplace = True)\n",
    "m.to_csv('./data/checkfile/for_mecab_dic.csv',index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>가온</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>T</td>\n",
       "      <td>가온</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>관우</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>F</td>\n",
       "      <td>관우</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>규</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>F</td>\n",
       "      <td>규</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>김준면</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>T</td>\n",
       "      <td>김준면</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>니노</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>F</td>\n",
       "      <td>니노</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3    4  5  6    7    8  9  10 11\n",
       "0   가온 NaN NaN NaN  NNP  *  T   가온  NNP  *  *  *\n",
       "1   관우 NaN NaN NaN  NNP  *  F   관우  NNP  *  *  *\n",
       "2    규 NaN NaN NaN  NNP  *  F    규  NNP  *  *  *\n",
       "3  김준면 NaN NaN NaN  NNP  *  T  김준면  NNP  *  *  *\n",
       "4   니노 NaN NaN NaN  NNP  *  F   니노  NNP  *  *  *"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
