{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 데이터에 대한 통계 분석\n",
    "* 데이터 \n",
    "> 2017년 12월 1일부터 2018년 1월 1일까지 32일간 [네이버](http://www.naver.com)와 [다음](http://www.daum.net)의 랭킹뉴스와 뉴스의 댓글을 크롤링함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import sys\n",
    "import time, re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import html\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import functools\n",
    "from konlpy.tag import Mecab\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site = 'Naver'\n",
    "site = 'daum'\n",
    "if site == 'daum':\n",
    "    binList = []\n",
    "    collection = 'newsDaum'\n",
    "elif site.lower() == 'naver':\n",
    "    binList = []\n",
    "    collection = 'newsNaver'\n",
    "\n",
    "mongodb = dh.ToMongoDB(*dh.AWS_MongoDB_Information())\n",
    "dbname = 'hy_db'\n",
    "useDb = dh.Use_Database(mongodb, dbname)\n",
    "useCollection = dh.Use_Collection(useDb, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "def tokenize_pos2(doc):\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주의 \n",
    "> gensim의 doc2vec 모델의 infer_vector 함수를 이용해 벡터를 생성하면, 같은 단어열(문서)에 대해서도 여러번 호출하면 각기 다른 벡터가 나온다  \n",
    "> 이는 문서 벡터 추론 과정에서 랜덤하게 초기화하는 과정이 있기 때문인데, 문제는 결과로 나오는 벡터의 차이가 꽤 크다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load('./model/doc2vec_size2000_epoch20_by_mecab.model')\n",
    "classifier = pickle.load(open('./data/pre_data/classifier_by_mecab_from_doc2vec_size2000_epoch20.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cls(text, model, num):\n",
    "    mecab = Mecab()\n",
    "    def tokenize_pos2(doc):\n",
    "        return ['/'.join(t) for t in mecab.pos(doc)]\n",
    "    TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "    x2 = TaggedDocument(tokenize_pos2(text), 0.5)\n",
    "    out = list()\n",
    "    for idx in range(num):\n",
    "        x = model.infer_vector(x2.words)\n",
    "        cl = classifier.predict(x.reshape(1,-1))\n",
    "        out.append(cl)\n",
    "    out = list(itertools.chain.from_iterable(out))\n",
    "    cls2 = int(round(sum(out)/len(out)))\n",
    "    return cls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsDict = pickle.load(open('./data/pre_data/keywords_daum.pickled','rb'))\n",
    "#keywordsDict = pickle.load(open('./data/pre_data/keywords_Naver.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "path = './outcome/outcome_'+site+'_by_mecab*'\n",
    "extracted_file = glob(path)\n",
    "extracted = dict()\n",
    "for file in extracted_file:\n",
    "    ex = pickle.load(open(file,'rb'))\n",
    "    extracted.update(ex)\n",
    "outlen = str(len(extracted_file))\n",
    "print (len(extracted))\n",
    "outname = './outcome/outcome_'+site+'_by_mecab.pickled'+outlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputDict  =dict()\n",
    "collection = useCollection.find({'site':site})\n",
    "count = 0\n",
    "for data in collection:\n",
    "    idis = data['_id']._ObjectId__id.hex()\n",
    "    if idis not in extracted:\n",
    "        text = data['title']+'. '+data['mainText']    \n",
    "        commentCollection = dh.Use_Collection(useDb, 'comments')\n",
    "        comments = commentCollection.find({'site':site, 'category':data['category'], 'date':data['date'], 'rank':data['rank'] })\n",
    "        commEstimation = list()\n",
    "        for comment in comments:\n",
    "            comm = comment['comments']\n",
    "            commEstimation.append(Cls(comm, model, 30))\n",
    "        cls = sum(commEstimation)/ len(commEstimation)    \n",
    "        outputDict[idis] = tmpdict = dict()\n",
    "        tmpdict['title'] = data['title']\n",
    "        tmpdict['maintextestimation'] = Cls(text, model, 50)\n",
    "        tmpdict['keywordsindaum'] = data['keywords']\n",
    "        tmpdict['analyzedkeywords'] = keywordsDict[idis]\n",
    "        tmpdict['number_crawledcomments'] = comments.count()\n",
    "        tmpdict['commentsestimation'] = cls\n",
    "        tmpdict['press'] = data['press']\n",
    "        tmpdict['number_of_comment'] = data['number_of_comment']\n",
    "        if count == 10:break\n",
    "        count +=1\n",
    "pickle.dump(outputDict, open(outname,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용자 사전을 만들고, 사전작업을 하는데 너무 많은 시간을 소비하였다. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
