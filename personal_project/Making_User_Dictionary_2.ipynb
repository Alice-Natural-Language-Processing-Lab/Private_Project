{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 ( Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 1289497, 5532]\n",
      "[('이', 'Noun'), ('것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테', 'Noun'), ('스트', 'Noun')]\n",
      "[('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n",
      "best: [('이것', 'Noun'), ('은', 'Josa'), ('테스트', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "import re, pickle, itertools\n",
    "import urllib3, json\n",
    "sys.path.append('~/Documents/GitHub/Private_Project/personal_project/')\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import normalize as sknorm\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import html\n",
    "\n",
    "from ckonlpy.tag import Twitter as ctwitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter, Mecab\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "import chat_bot as cb\n",
    "import Database_Handler as dh\n",
    "\n",
    "ct = ctwitter()\n",
    "ot = Twitter()\n",
    "mecab = Mecab()\n",
    "xxxx = ct._dictionary._pos2words\n",
    "print (list(map(lambda x: len(xxxx[x]), xxxx)))\n",
    "\n",
    "score_weights = {\n",
    "    'num_nouns': -0.1,\n",
    "    'num_words': -0.2,\n",
    "    'no_noun': -1\n",
    "}\n",
    "\n",
    "def my_score(candidate):\n",
    "    num_nouns = len([w for w,t in candidate if t == 'Noun'])\n",
    "    num_words = len(candidate)\n",
    "    no_noun = 1 if num_nouns == 0 else 0\n",
    "    \n",
    "    score = (num_nouns * score_weights['num_nouns'] \n",
    "             + num_words * score_weights['num_words']\n",
    "             + no_noun * score_weights['no_noun'])\n",
    "    return score\n",
    "\n",
    "ct.set_selector(score_weights, my_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 사전 추가 및 Stopwords 를 위한 전처리 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언론사 목록 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> stopwords를 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressList = pd.read_excel('./data/presslist.xlsx')\n",
    "\n",
    "with(open('./data/newspress.txt','w',encoding='utf-8')) as f:\n",
    "    f.write('\\n'.join(set(pressList['언론사'].tolist())))\n",
    "def Stopwords(file):\n",
    "    stopwords = open(file,'r',  encoding='utf-8').readlines()\n",
    "    stopwords = list(map(lambda x:x.strip(), stopwords))\n",
    "    return stopwords\n",
    "stopwordList = Stopwords('./data/newspress.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공공데이터 포털에서 에서 뉴스 빅데이터 분석 정보로부터 키워드와 명사등의 정보를 추출하기\n",
    "* 자료 출처 \n",
    "> * [공공데이터 포털](http://www.data.go.kr/)  \n",
    "> * [빅카인즈 공공데이터](https://www.kinds.or.kr/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './data/news/high_frequency_noun/'\n",
    "path3 = './data/news/newstopic/'\n",
    "path4 = './data/news/people/'\n",
    "path5 = './data/news/4th_industry/'\n",
    "path6 = './data/news/have_negative_positive/constitution/'\n",
    "path7 = './data/news/have_negative_positive/household_debt/'\n",
    "path8 = './data/news/have_negative_positive/olymphic/'\n",
    "path9 = './data/news/have_negative_positive/'\n",
    "path10 = './data/news/etc1/'\n",
    "path11 = './data/news/etc2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인물, 기관, 지역, 장소 등의 명사 추출 from news big data in kinds.or.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathList2 = [path11,path10,path4, path5, path6, path7, path8, path9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/news/etc2/김영란법 관련 뉴스메타데이터(20110616-20160731).csv\n",
      "./data/news/people/연도별 분야별 TOP5 뉴스메이커.csv\n",
      "./data/news/people/2017년 11월 경제 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 11월 정치 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 9월 정치 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 9월 경제 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 10월 정치 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/people/2017년 10월 경제 분야 상위 언급 뉴스 인물 분석.csv\n",
      "./data/news/4th_industry/4차 산업혁명 (20130101-20170630)_1.csv\n",
      "./data/news/4th_industry/4차 산업혁명 (20130101-20170630)_2.csv\n",
      "./data/news/4th_industry/4차 산업혁명 (20130101-20170630)_3.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_5.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_4.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_3.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_2.csv\n",
      "./data/news/have_negative_positive/constitution/csv)개헌 관련 뉴스메타데이터(19900101-20161026)_1.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_6.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_5.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_4.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_1.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_3.csv\n",
      "./data/news/have_negative_positive/household_debt/csv)가계부채 관련 뉴스메타데이터(19900101-2016.9.30)_2.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_5.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_4.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_6.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_7.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_3.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_2.csv\n",
      "./data/news/have_negative_positive/olymphic/csv)올림픽 관련 뉴스메타데이터(20160101-20161231)_1.csv\n",
      "./data/news/have_negative_positive/누진제 관련 뉴스메타데이터(19900101-20160731).csv\n",
      "./data/news/have_negative_positive/증강현실 관련 뉴스메타데이터(19900101-2016.9.30).csv\n"
     ]
    }
   ],
   "source": [
    "def Read_File(file):\n",
    "    df = pd.read_csv(file,encoding = 'utf-8', engine = 'python')\n",
    "    targetColumns = list(filter(lambda x: ('인물' in x) or ('기관' in x) or ('지역' in x) or ('장소' in x),df.columns.values))\n",
    "    if len(targetColumns) != 0:\n",
    "        return df[targetColumns]\n",
    "    else:\n",
    "        return df[targetColumns]\n",
    "outpath = './data/news/prenoun/'\n",
    "for idx1, path in enumerate(pathList2):\n",
    "    fileList = glob(path+'*.csv')\n",
    "    fileList.extend(glob(path+'CSV'))\n",
    "    for idx2, file in enumerate(fileList):\n",
    "        outfile = outpath+str(idx1)+'_'+str(idx2)+'.txt'\n",
    "        if not os.path.isfile(outfile):\n",
    "            print (file)\n",
    "            rawFile = Read_File(file)\n",
    "            rawFile.to_csv(outfile,index=False, header = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter1(isList):\n",
    "    tmp1 = set(isList)\n",
    "    #tmp3 = set(map(lambda x: x.strip(), set(itertools.chain.from_iterable(tmp2))))\n",
    "    tmp3 = list(filter(lambda x: len(x)>1, tmp1))\n",
    "    #tmp3 = list(filter(lambda x: x!='', tmp2))\n",
    "    #tmp3 = set(map(lambda x: x.strip(), set(itertools.chain.from_iterable(tmp3))))\n",
    "    tmp3 = list(map(lambda x: x.strip(), tmp3))\n",
    "    return tmp3\n",
    "def MakeCombination(li):\n",
    "    outlist = []\n",
    "    for ix in range(1, len(li)+1):\n",
    "        outlist += list(itertools.permutations(li,ix))\n",
    "    outlist = list(map(lambda x: ' '.join(x), outlist)) + list(map(lambda x: ''.join(x), outlist))\n",
    "    outlist = list(filter(lambda x: len(x)!=1, outlist))\n",
    "    outlist = outlist + list(map(lambda x: re.sub('[\\W]','', x), outlist))\n",
    "    return outlist\n",
    "def filter2(idx):\n",
    "    idx2 = set(filter(lambda x: x!='', set(MakeCombination(idx.split(' ')))))\n",
    "    idx2 = set(filter(lambda x: not '  'in x , idx2))\n",
    "    idx2 = set(map(lambda x: x.strip(), idx2))\n",
    "    return idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36409,), (188220,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/news/prenoun/'\n",
    "second_file_list = glob(path+'*.txt')\n",
    "outList = list()\n",
    "for file in second_file_list:\n",
    "    second_file = pd.read_csv(file, encoding = 'utf-8', engine = 'python', header=None)\n",
    "    for colorder in second_file.columns.values:\n",
    "        targetData = second_file[colorder]\n",
    "        targetData.dropna(inplace = True)\n",
    "        targetData.drop_duplicates(inplace = True)\n",
    "        targetData.reset_index(drop=True, inplace = True)\n",
    "        targetData = pd.DataFrame(list(set(list(itertools.chain.from_iterable(targetData.str.split(','))))))\n",
    "        targetData.reset_index(drop=True, inplace = True)\n",
    "        targetData.drop_duplicates(inplace = True)\n",
    "        targetData.reset_index(drop=True, inplace = True)\n",
    "        targetData = pd.DataFrame(list(set(list(itertools.chain.from_iterable( targetData[0].str.split('[가-힣 ]{3,3}기자'))))))\n",
    "        targetData = targetData[targetData[0].str.len() >1 ]\n",
    "        outList += targetData[0].tolist()\n",
    "outDf = pd.Series(outList)\n",
    "outDf.dropna(inplace = True)\n",
    "outDf.drop_duplicates(inplace=True)\n",
    "outDf.reset_index(drop = True, inplace = True)\n",
    "tmp1 = outDf[outDf.str.split(' ').str.len() == 1]\n",
    "#tmp1 = tmp1[tmp1.str.len() <= 15]\n",
    "tmp1.drop_duplicates(inplace=True)\n",
    "tmp1.dropna(inplace = True)\n",
    "tmp1.reset_index(drop = True, inplace = True)\n",
    "tmp2 = outDf[outDf.str.split(' ').str.len() > 1]\n",
    "tmp2 = pd.Series(list(set(list(itertools.chain.from_iterable( outDf.str.split(' '))))))\n",
    "#tmp2 = tmp2[tmp2.str.len() <15]\n",
    "tmp2.reset_index(drop = True, inplace = True)\n",
    "tmp2 = tmp2[tmp2.str.len() >1 ]\n",
    "tmp2.reset_index(drop = True, inplace = True)\n",
    "tmp2.drop_duplicates(inplace=True)\n",
    "tmp2.dropna(inplace = True)\n",
    "tmp1.shape, tmp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176640,)\n"
     ]
    }
   ],
   "source": [
    "outis = pd.concat([tmp1,tmp2])\n",
    "outis.drop_duplicates(inplace = True)\n",
    "outis.reset_index(drop = True, inplace = True)\n",
    "outis = outis[outis.str.match('[a-zA-Z0-9가-힣\\s]+$')]\n",
    "outis.drop_duplicates(inplace = True)\n",
    "outis.reset_index(drop = True, inplace = True)\n",
    "print (outis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword extract from news big data in kinds.or.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_File2(file):\n",
    "    df = pd.read_csv(file,encoding = 'utf-8', engine = 'python')\n",
    "    targetColumns = list(filter(lambda x: x in ['중요키워드','키워드','keword','토픽키워드','토픽 키워드'], df.columns.values))\n",
    "    if len(targetColumns) != 0:\n",
    "        return df[targetColumns]\n",
    "    else:\n",
    "        return df[targetColumns]\n",
    "def Extract_Keywords(path):\n",
    "    fileList = glob(path+'*.csv')\n",
    "    fileList.extend(glob(path+'*.CSV'))\n",
    "    outDf = pd.DataFrame()\n",
    "    for file in fileList:\n",
    "        df = Read_File2(file)\n",
    "        if len(outDf) == 0:\n",
    "            outDf = df\n",
    "        else:\n",
    "            outDf = pd.concat([outDf, df], axis = 0)\n",
    "    outDf.reset_index(drop=True, inplace = True)\n",
    "    outDf.astype(str)\n",
    "    outDf = outDf.to_dict('list')\n",
    "    outDict = dict()\n",
    "    for outKey in outDf:\n",
    "        outDict[outKey] = filter3(outDf[outKey])\n",
    "    return outDict\n",
    "def filter3(isList):\n",
    "    tmp1 = set(isList)\n",
    "    tmp1 = list(filter(lambda x: type(x)!=float, tmp1))\n",
    "    tmp2 = list(map(lambda x: x.split(','), tmp1))\n",
    "    tmp3 = set(map(lambda x: x.strip(), set(itertools.chain.from_iterable(tmp2))))\n",
    "    tmp3 = list(filter(lambda x: len(x)>1, tmp3))\n",
    "    tmp3 = list(filter(lambda x: x!='', tmp3))\n",
    "    tmp3 = list(map(lambda x: x.strip(), tmp3))\n",
    "    return tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathList1 = [path3, path4, path5, path6, path7, path8, path9, path10,path11]\n",
    "#pathList1 = [path1]\n",
    "#outpath = './data/news/'\n",
    "outpath = './data/news/keywords/'\n",
    "outSet2 =set()\n",
    "for path1 in pathList1:\n",
    "    ek = Extract_Keywords(path1)\n",
    "    for info2 in ek:\n",
    "        out2 = pd.DataFrame(ek[info2])\n",
    "        out2 = out2[out2[0].str.match('[a-zA-Z0-9가-힣\\s]+$')]\n",
    "        out2.reset_index(drop=True, inplace=True)\n",
    "        out2 = out2[0].apply(lambda x: filter2(x))\n",
    "        out2.apply(lambda x: outSet2.update(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436204,)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outis2 = pd.Series(list(outSet2))\n",
    "outis2.shape\n",
    "outis2.reset_index(inplace = True, drop=True)\n",
    "outis2.drop_duplicates(inplace = True)\n",
    "outis2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(515979,)\n"
     ]
    }
   ],
   "source": [
    "beforeFilter = pd.concat([outis,outis2])\n",
    "beforeFilter.drop_duplicates(inplace = True)\n",
    "beforeFilter.reset_index(drop=True, inplace = True)\n",
    "print (beforeFilter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464037,)\n"
     ]
    }
   ],
   "source": [
    "beforeFilter = beforeFilter[~beforeFilter.str.contains('[a-zA-Z0-9]+')]\n",
    "beforeFilter.drop_duplicates(inplace = True)\n",
    "beforeFilter.reset_index(drop=True, inplace = True)\n",
    "print (beforeFilter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(beforeFilter,open('./data/pre_data/keywords_rawdata.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeFilter = pickle.load(open('./data/pre_data/keywords_rawdata.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeFilter2_ct = beforeFilter.apply(ct.pos)\n",
    "beforeFilter2_mecab = beforeFilter.apply(mecab.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(beforeFilter2_ct,open('./data/pre_data/keywords_tagging_ct.pickled','wb'))\n",
    "pickle.dump(beforeFilter2_mecab,open('./data/pre_data/keywords_tagging_mecab.pickled','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeFilter2_ct = pickle.load(open('./data/pre_data/keywords_tagging_ct.pickled','rb'))\n",
    "beforeFilter2_mecab = pickle.load(open('./data/pre_data/keywords_tagging_mecab.pickled','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77788,)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = beforeFilter[((beforeFilter2_ct.str.len() == 1) & (beforeFilter2_mecab.str.len() == 1))]\n",
    "f1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75234,), (2554,))"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_1noun = f1[(beforeFilter2_mecab[f1.index].apply(lambda x: x[-1][-1] in ['NNP','NNG'])) & (beforeFilter2_ct[f1.index].apply(lambda x: x[-1][-1] == 'Noun'))]\n",
    "f1_mul_noun = f1[f1.index.difference(f1_1noun.index)]\n",
    "f1_1noun.shape, f1_mul_noun.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 품사 태깅이 하나만 되어 있고, mecab과 twitter 양쪽에서 명사로 판별함 : 75234 -> 사용할 필요 없음\n",
    "* 그렇지 않은 것 : 2554 -> 버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(386249,)\n",
      "(386084,)\n"
     ]
    }
   ],
   "source": [
    "f2 = beforeFilter[beforeFilter.index.difference(f1.index)]\n",
    "print (f2.shape)\n",
    "f2.drop(f2[f2.str.match('[써씨더서은만랑큼음는인에이게가으로기다을를들지측]+$')].index, inplace = True)\n",
    "f2.dropna(inplace = True)\n",
    "f2.drop_duplicates(inplace = True)\n",
    "print (f2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(386084,) (386084,)\n"
     ]
    }
   ],
   "source": [
    "f2_mecab = beforeFilter2_mecab[f2.index]\n",
    "f2_ct = beforeFilter2_ct[f2.index]\n",
    "print (f2_mecab.shape, f2_ct.shape)\n",
    "#f2.reset_index(drop=True, inplace = True)\n",
    "#f2_mecab.reset_index(drop=True, inplace = True)\n",
    "#f2_ct.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83769,) (17683,)\n"
     ]
    }
   ],
   "source": [
    "f2_1n_mecab = f2_mecab[(f2_mecab.str.len() == 1) & (f2_mecab.apply(lambda x: x[-1][-1] in ['NNP','NNG']))]\n",
    "f2_1n_ct = f2_ct[(f2_ct.str.len() == 1) & (f2_ct.apply(lambda x: x[-1][-1] =='Noun'))]\n",
    "print (f2_1n_mecab.shape, f2_1n_ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(302315,) (368401,)\n"
     ]
    }
   ],
   "source": [
    "f2_mul_mecab = f2_mecab[f2_mecab.index.difference(f2_1n_mecab.index)]\n",
    "f2_mul_ct = f2_ct[f2_ct.index.difference(f2_1n_ct.index)]\n",
    "print (f2_mul_mecab.shape, f2_mul_ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301453,) (367367,)\n"
     ]
    }
   ],
   "source": [
    "f2_mul_mecab = f2_mul_mecab[f2_mul_mecab.str.len() != 1]\n",
    "f2_mul_ct = f2_mul_ct[f2_mul_ct.str.len() != 1]\n",
    "print (f2_mul_mecab.shape, f2_mul_ct.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 명사가 아닌데 품사 태깅이 하나만 된 것 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83769, 301453, 17683, 367367)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2_1n_mecab.shape[0], f2_mul_mecab.shape[0], f2_1n_ct.shape[0], f2_mul_ct.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283770,) (283598,)\n"
     ]
    }
   ],
   "source": [
    "f2_mul_mecab.drop(f2_1n_ct.index, inplace = True)\n",
    "f2_mul_ct.drop(f2_1n_mecab.index, inplace = True)\n",
    "print (f2_mul_mecab.shape, f2_mul_ct.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mecab과 twitter 둘 중 하나의 품사 태깅에서 품사 태깅이 1개만 되고 명사로 태깅됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101452,)\n"
     ]
    }
   ],
   "source": [
    "data1is = pd.concat([f2[f2_1n_ct.index], f2[f2_1n_mecab.index]])\n",
    "data1is.dropna(inplace=True)\n",
    "data1is.drop_duplicates(inplace = True)\n",
    "data1is.reset_index(drop = True, inplace =True)\n",
    "print (data1is.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mecab과 twitter 둘 다에서 다수의 품사 태깅이 됨  ->> 제거"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
